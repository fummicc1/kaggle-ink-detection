{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Vesuvius Challenge - Ink Detection Training Notebook"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Note\n","\n","- 高い解像度でリサイズすることはprecisionの向上につながるため有効\n","- seresnextでチャネル間の相関を見れるので有効\n","- 文字の太さ・書き方に大きくバリアントがあるので、横の相関よりもdepthの相関の方が大事かもしれない\n","- 深さに関して、隣り合う深さ同士に大きな変化はない\n","- fpをfnよりも小さくしたい\n","- valid_scoreはCFG.thd依存\n","- valid_lossはCFG.loss1/loss2依存\n","- encoder内でdepthをクロップしてバッチで繋げた方が精度が良い\n","- maskに対しては有効ではないが、labelは的確なラベルを用いることで精度が向上\n","- depthは22~34 or 24 ~ 36\n","- 画像のサイズは大きい方が良いのか？（BUFFER / SHARED_HEIGHT）\n","    - 比率を同じにして試してみる\n","    - SHARED_HEIGHTをデカくするとデータがメモリに載らなさそうだった\n","- NetのweightsにBatchNormはない方が良い\n","- BUFFER:strideを160:96から160:80にしたら精度が落ちた\n","    - trainの精度は上がっていた\n","    - BUFFERに対してstrideが細かすぎると過学習に繋がっているのかもしれない\n","- 文字が見えなくなるよりも、高いthdを設定して、文字が大きく見えた方が良い（仮説）\n","- 学習時にはstrideは大きくし、過学習を防ぐ。識別時にはstrideは小さくし見落としを減らす\n","- バイリニアよりもバイキュービックが良い\n","- ノイズについて\n","    - 大きすぎると良くない(intensityが(0.0001, 0.0005)くらいが良かった)\n","        - 元画像が255 * 255の範囲であることを踏まえると、1/255の変化で255のずれがある\n","- lossのαとβについて\n","    - 理論的にはαは大きい方が良い（偽陽性fpはスコアを大きく下げるので）\n","    - しかし、thdを用いて偽陽性を防ぐ手法もあるため、学習時にはあえて偽陰性に強いペナルティを与えた方が良いのかもしれない\n","    - Stacked UnetのI層目では高いβで見落としをなくして、二層目で高いαにしてノイズを除去する作戦\n","- 文字サイズとリサイズサイズについて\n","- thresholdを変えるだけでpublic lbが大きく変わる\n","    - 0.5から0.7にしたら0.05上がった（0.58→0.63）\n","- 学習時・推論時に低いRESIZED_HEIGHTで進めると、提出時にcvとlbに差が出るかもしれない。これは最終的にリサイズする際に細かい部分の見落としが発生するからだと思う\n","    - 良い学習をするために、正しいラベル・正しい入力が適切→resizeすることでデータの質が損なわれているのかも\n","- [ ] 3dcnnにbatch化したdepthを横に繋げる実装が使えるか考える\n","- [ ] Stacked Unetを3dcnnに適用してみる\n","- 解像度（BUFFER）はある程度大きくないといけない（<112）\n","- DecoderBlockのattention=scseについて\n","- 3dresnetで見る場合、最初のconv1とmaxpoolで解像度が1/4に縮小されるので、元が128x128であれば32x32をUpScaleすることになる。なので、元々ある程度高い解像度が必要\n","- 3dresnetで見る場合、Z_LISTは2飛ばしで広く見る方が精度が高い\n","- ampを有効にする（precision=16）\n","- 3dresnetで学習時の精度が良くならない\n","    - 解像度が低い（仮説）\n","    - agumentationが難しすぎる（仮説）\n","        - augmentationの確率を下げても変化はなかった\n","        - augmentationをなくすとどうなる？"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-05-20T04:31:07.456867Z","iopub.status.busy":"2023-05-20T04:31:07.456553Z","iopub.status.idle":"2023-05-20T04:31:44.807975Z","shell.execute_reply":"2023-05-20T04:31:44.806511Z","shell.execute_reply.started":"2023-05-20T04:31:07.456835Z"},"trusted":true},"outputs":[],"source":["# # Pretrained weights\n","# # ref - https://github.com/kenshohara/3D-ResNets-PyTorch\n","# !pip install gdown\n","# !gdown 1Nb4abvIkkp_ydPFA9sNPT1WakoVKA8Fa\n","\n","# # Utility packages for reading and visualizing volumes\n","# !pip install zarr imageio-ffmpeg\n","\n","# # save model checkpoints\n","# !mkdir ./ckpts\n","\n","# !kaggle datasets download -d samfc10/vesuvius-zarr-files"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T04:31:44.811420Z","iopub.status.busy":"2023-05-20T04:31:44.811084Z","iopub.status.idle":"2023-05-20T04:31:48.016722Z","shell.execute_reply":"2023-05-20T04:31:48.015498Z","shell.execute_reply.started":"2023-05-20T04:31:44.811386Z"},"trusted":true},"outputs":[],"source":["\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import wandb\n","import torchvision\n","import datetime\n","import imageio\n","# import cupy\n","from sklearn.model_selection import KFold\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","import pytorch_lightning\n","import segmentation_models_pytorch as smp\n","import pytorch_lightning as pl\n","import pytorch_lightning.callbacks.model_checkpoint\n","import pytorch_lightning.plugins\n","from skimage.transform import resize as resize_ski\n","from pytorch_lightning.strategies.ddp import DDPStrategy\n","from pytorch_lightning.loggers import WandbLogger\n","from einops import rearrange, reduce, repeat\n","import torch.nn.functional as F\n","import segmentation_models_pytorch as smp\n","from segmentation_models_pytorch.decoders.unet.decoder import UnetDecoder, DecoderBlock\n","from timm.models.resnet import resnet10t, resnet34d, resnet50d, resnet14t, seresnext26d_32x4d, seresnext50_32x4d\n","import os\n","import torch.utils.data\n","from dataclasses import dataclass\n","\n","from scipy.ndimage import distance_transform_edt\n","\n","import ssl\n","ssl._create_default_https_context = ssl._create_unverified_context\n","\n","import glob\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import cv2\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data\n","import os,cv2\n","import gc\n","import sys\n","import matplotlib.patches as patches\n","import random\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import pytorch_lightning as pl\n","\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.cuda import amp\n","from torch.utils.data import Dataset, DataLoader\n","import segmentation_models_pytorch as smp\n","import albumentations as A\n","from IPython.display import Video\n","\n","# sys.path.append(\"/home/fummicc1/codes/competitions/kaggle-ink-detection\")\n","# sys.path.append(\"/kaggle/input/resnet3d\")\n","from resnet3d import generate_model\n","\n","pytorch_lightning.seed_everything(seed=42)\n","torch.set_float32_matmul_precision('high')\n","\n","\n","@dataclass\n","class CFG():\n","    # Data config\n","    # DATA_DIR = '/kaggle/input/vesuvius-challenge-ink-detection/'\n","    DATA_DIR = '/home/fummicc1/codes/competitions/kaggle-ink-detection'\n","    # DATA_DIR = '/home/fummicc1/codes/Kaggle/kaggle-ink-detection'\n","    BUFFER = 160 # Half-size of papyrus patches we'll use as model inputs\n","    CROP_SIZE = BUFFER * 2\n","    STRIDE = 96\n","    # Z_LIST = list(range(0, 20, 5)) + list(range(22, 34))  # Offset of slices in the z direction\n","    # Z_LIST = [20, 22, 24, 26] + list(range(28, 36)) + [36, 38]\n","    # Z_LIST = list(range(16, 48, 2))\n","    Z_LIST = list(range(20, 40, 2))\n","    # Z_LIST = list(range(0, 24, 8)) + list(range(24, 36, 2)) + list(range(36, 64, 10))\n","    Z_DIM = len(Z_LIST)  # Number of slices in the z direction. Max value is 64 - Z_START\n","    # BATCH_Z_DIFF = 4\n","    SHARED_HEIGHT = 5120  # Max height to resize all papyrii\n","\n","    # Model config\n","    BATCH_SIZE = 96\n","\n","    device = torch.device(\"cuda\")\n","    threshold = 0.5\n","    num_workers = 8\n","    exp = 1e-7\n","    mask_padding = BUFFER\n","\n","    num_epochs = 20\n","    lr = 1e-3\n","    eta_min_lr = 5e-5\n","    WANDB_NOTE = \"augmentationをさらに減らした\"\n","    loss1_alpha = 0.5\n","    loss1_beta = 0.5\n","    loss2_alpha = 0.5\n","    loss2_beta = 0.5\n","    \n","    loss1_weight = 0.5\n","    loss2_weight = 0.5\n","    \n","    lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts\n","    loss1 = smp.losses.TverskyLoss(\n","        smp.losses.BINARY_MODE,\n","        log_loss=False,\n","        from_logits=True, \n","        smooth=1e-7,\n","        alpha=loss1_alpha,\n","        beta=loss1_beta,\n","    )\n","    loss2 = smp.losses.TverskyLoss(\n","        smp.losses.BINARY_MODE,\n","        log_loss=False,\n","        from_logits=True, \n","        smooth=1e-7,\n","        alpha=loss2_alpha,\n","        beta=loss2_beta,\n","    )\n","    noise_intensity = (0.0001, 0.0005)\n","    use_new_label_mask = True\n","    pretrained = True\n","    MODEL_DEPTH = 34\n","    \n","    exp_name = \"006-gpu17\"\n","    prev_exp_name = \"006-gpu17\"\n","    \n","def class2dict(c):\n","    return {attr: getattr(c, attr) for attr in dir(c) if not callable(getattr(c, attr)) and not attr.startswith(\"__\")}"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Config"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Load data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def resize(img):\n","    current_height, current_width = img.shape    \n","    aspect_ratio = current_width / current_height\n","    if CFG.SHARED_HEIGHT is None:\n","        return img\n","    # new_height = CFG.SHARED_HEIGHT\n","    # pad_y = new_height - current_height\n","    # if pad_y > 0:\n","    #     # 元画像が小さい場合は解像度を大きくしないでpaddingをつける\n","    #     img = np.pad(img, [(0, pad_y), (0, 0)], constant_values=0)\n","    # else:\n","    # 既に十分でかい場合はリサイズする\n","    # 本当はpaddingしたいけど、メモリサイズが大きくなる\n","    new_height = CFG.SHARED_HEIGHT\n","    new_width = int(CFG.SHARED_HEIGHT * aspect_ratio)\n","    new_size = (new_width, new_height)\n","    # (W, H)の順で渡すが結果は(H, W)になっている\n","    img = cv2.resize(img, new_size)\n","    return img\n","\n","def load_mask(split, index): \n","    if index == \"2a\" or index == \"2b\":\n","        mode = index[1]\n","        index = \"2\"\n","    img = cv2.imread(f\"{CFG.DATA_DIR}/{split}/{index}/mask.png\", 0) // 255\n","    if index == \"2\":\n","        h = 9456\n","        if mode == \"a\":\n","            img = img[h:, :]\n","        elif mode == \"b\":   \n","            img = img[:h, :]\n","    img = resize(img)\n","    img = np.pad(img, 1, constant_values=0)\n","    dist = distance_transform_edt(img)\n","    img[dist <= CFG.mask_padding] = 0\n","    img = img[1:-1, 1:-1]    \n","    return img\n","\n","\n","def load_labels(split, index):\n","    if index == \"2a\" or index == \"2b\":\n","        mode = index[1]\n","        index = \"2\"\n","    suffix = \"_new\" if CFG.use_new_label_mask else \"\"\n","    img = cv2.imread(f\"{CFG.DATA_DIR}/{split}/{index}/inklabels{suffix}.png\", 0) // 255    \n","    if index == \"2\":\n","        h = 9456\n","        if mode == \"a\":\n","            img = img[h:, :]\n","        elif mode == \"b\":   \n","            img = img[:h, :]\n","    img = resize(img)\n","    return img\n","\n","\n","\n","\n","def load_ir(split, index):\n","    if index == \"2a\" or index == \"2b\":\n","        mode = index[1]\n","        index = \"2\"\n","    img = cv2.imread(f\"{CFG.DATA_DIR}/{split}/{index}/ir.png\", 0)\n","    if index == \"2\":\n","        h = 9456\n","        if mode == \"a\":\n","            img = img[h:, :]\n","        elif mode == \"b\":   \n","            img = img[:h, :]\n","    img = resize(img)\n","    return img"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def load_volume(split, index):\n","    if index == \"2a\" or index == \"2b\":\n","        mode = index[1]\n","        index = \"2\"\n","    # Load the 3d x-ray scan, one slice at a time\n","    all = sorted(glob.glob(f\"{CFG.DATA_DIR}/{split}/{index}/surface_volume/*.tif\"))\n","    z_slices_fnames = [all[i] for i in range(len(all)) if i in CFG.Z_LIST]\n","    assert len(z_slices_fnames) == CFG.Z_DIM\n","    z_slices = []\n","    for z, filename in  tqdm(enumerate(z_slices_fnames)):\n","        img = cv2.imread(filename, -1)\n","        if index == \"2\":\n","            h = 9456\n","            if mode == \"a\":\n","                img = img[h:, :]\n","            elif mode == \"b\":\n","                img = img[:h, :]\n","        img = resize(img)\n","        # img = (img / (2 ** 8)).astype(np.uint8)\n","        img = img.astype(np.float32) // 255\n","        z_slices.append(img)\n","    return np.stack(z_slices, axis=-1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def is_in_masked_zone(location, mask):\n","    return mask[location[0], location[1]] > 0"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def generate_locations_ds(volume, mask, label=None, skip_zero=False):\n","    is_in_mask_train = lambda x: is_in_masked_zone(x, mask)\n","\n","    # Create a list to store train locations\n","    locations = []\n","\n","    # Generate train locations\n","    volume_height, volume_width = volume.shape[:-1]\n","\n","    for y in range(CFG.BUFFER, volume_height - CFG.BUFFER, CFG.STRIDE):\n","        for x in range(CFG.BUFFER, volume_width - CFG.BUFFER, CFG.STRIDE):\n","            if skip_zero and label is not None and np.all(label[y - CFG.BUFFER // 2 : y + CFG.BUFFER // 2, x - CFG.BUFFER // 2 : x + CFG.BUFFER // 2] == 0):\n","                # print(f\"skip location at (y: {y}, x: {x})\")\n","                continue\n","            if is_in_mask_train((y, x)):\n","                locations.append((y, x))\n","\n","    # Convert the list of train locations to a PyTorch tensor\n","    locations_ds = np.stack(locations, axis=0)\n","    return locations_ds"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["FRAGMENTS_ZARR = {\n","    \"1\" : {},\n","    \"2a\" : {},\n","    \"2b\" : {},\n","    \"3\" : {},\n","}\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for key in [\"1\", \"2a\", \"2b\", \"3\"]:\n","    FRAGMENTS_ZARR[key][\"surface_volume\"] = load_volume(\"train\", key)\n","    FRAGMENTS_ZARR[key][\"mask\"] = load_mask(\"train\", key)\n","    FRAGMENTS_ZARR[key][\"truth\"] = load_labels(\"train\", key)\n","    FRAGMENTS_ZARR[key][\"infrared\"] = load_ir(\"train\", key)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T04:31:48.029208Z","iopub.status.busy":"2023-05-20T04:31:48.027973Z","iopub.status.idle":"2023-05-20T04:31:48.122129Z","shell.execute_reply":"2023-05-20T04:31:48.121088Z","shell.execute_reply.started":"2023-05-20T04:31:48.029167Z"},"trusted":true},"outputs":[],"source":["FRAGMENTS_SHAPE = {k : v[\"surface_volume\"].shape[:-1] for k, v in FRAGMENTS_ZARR.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["list(FRAGMENTS_ZARR[\"1\"].keys())\n","# np.unique(np.array(FRAGMENTS_ZARR[\"1\"][\"surface_volume\"]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.imshow(FRAGMENTS_ZARR[\"2a\"][\"truth\"])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.imshow(FRAGMENTS_ZARR[\"1\"][\"truth\"][:,:])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Visualise input"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T04:31:48.128130Z","iopub.status.busy":"2023-05-20T04:31:48.127294Z","iopub.status.idle":"2023-05-20T04:31:50.740007Z","shell.execute_reply":"2023-05-20T04:31:50.739022Z","shell.execute_reply.started":"2023-05-20T04:31:48.128088Z"},"trusted":true},"outputs":[],"source":["fragment_id = FRAGMENTS_ZARR[\"1\"]\n","x, y = 1000, 1000\n","\n","# np.unique(FRAGMENTS_ZARR[\"1\"][\"surface_volume\"][y:y+CROP_SIZE, x:x+CROP_SIZE, :])\n","fragment_cropped = fragment_id[\"surface_volume\"][ x - CFG.BUFFER : x + CFG.BUFFER, y - CFG.BUFFER : y + CFG.BUFFER]\n","imageio.mimwrite(\"fragment_crop.mp4\", (fragment_cropped.transpose(2, 0, 1)), \"ffmpeg\")\n","Video(\"fragment_crop.mp4\", height=256, width=256)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T04:31:50.742410Z","iopub.status.busy":"2023-05-20T04:31:50.741764Z","iopub.status.idle":"2023-05-20T04:31:51.014241Z","shell.execute_reply":"2023-05-20T04:31:51.012587Z","shell.execute_reply.started":"2023-05-20T04:31:50.742366Z"},"trusted":true},"outputs":[],"source":["mask_cropped = fragment_id[\"truth\"][y-CFG.BUFFER:y+CFG.BUFFER, x - CFG.BUFFER : x + CFG.BUFFER]\n","ir_cropped = fragment_id[\"infrared\"][y-CFG.BUFFER:y+CFG.BUFFER, x - CFG.BUFFER : x + CFG.BUFFER]\n","\n","plt.figure(figsize=(6, 3))\n","plt.subplot(1, 2, 1)\n","plt.imshow(mask_cropped, cmap=\"gray\")\n","plt.axis(\"off\")\n","\n","plt.subplot(1, 2, 2)\n","plt.imshow(ir_cropped, cmap=\"gray\")\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T04:31:51.022260Z","iopub.status.busy":"2023-05-20T04:31:51.016326Z","iopub.status.idle":"2023-05-20T04:31:51.180463Z","shell.execute_reply":"2023-05-20T04:31:51.179062Z","shell.execute_reply.started":"2023-05-20T04:31:51.022203Z"},"trusted":true},"outputs":[],"source":["# del fragment_id, fragment_cropped, mask_cropped, ir_cropped\n","# gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def extract_subvolume(location, volume):\n","    global printed\n","    y = location[0]\n","    x = location[1]\n","    subvolume = volume[y-CFG.BUFFER:y+CFG.BUFFER, x-CFG.BUFFER:x+CFG.BUFFER, :].astype(np.float32)\n","    \n","    return subvolume"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def visualize_dataset_patches(locations_ds, labels, mode: str, fold = 0):\n","    fig, ax = plt.subplots()\n","    ax.imshow(labels)\n","\n","    for y, x in locations_ds:\n","        patch = patches.Rectangle([x - CFG.BUFFER, y - CFG.BUFFER], 2 * CFG.BUFFER, 2 * CFG.BUFFER, linewidth=2, edgecolor='g', facecolor='none')\n","        ax.add_patch(patch)\n","    plt.savefig(f\"fold-{fold}-{mode}.png\")\n","    plt.show()    "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Dataloaders"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","from albumentations.core.transforms_interface import ImageOnlyTransform\n","\n","class MultichannelNoise(ImageOnlyTransform):\n","\n","    def __init__(self, intensity=CFG.noise_intensity, always_apply=False, p=0.5):\n","        super().__init__(always_apply, p)\n","        self.intensity = intensity\n","\n","    def apply(self, img, **params):\n","        intensity = np.random.uniform(*self.intensity)\n","        noise = np.random.normal(loc=0, scale=intensity*255, size=img.shape)\n","        img = img + noise\n","        return np.clip(img, 0, 255).astype(np.float32) # クリップして0から255の範囲に保つ\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T04:31:51.183662Z","iopub.status.busy":"2023-05-20T04:31:51.182694Z","iopub.status.idle":"2023-05-20T04:31:51.199714Z","shell.execute_reply":"2023-05-20T04:31:51.198664Z","shell.execute_reply.started":"2023-05-20T04:31:51.183616Z"},"trusted":true},"outputs":[],"source":["\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.preprocessing import OneHotEncoder\n","\n","from albumentations.core.transforms_interface import ImageOnlyTransform\n","\n","class SubvolumeDataset(Dataset):\n","    def __init__(self, locations, volume, labels, buffer, is_train: bool, return_location: bool = False):\n","        self.locations = locations\n","        self.volume = volume\n","        self.labels = labels        \n","        self.buffer = buffer\n","        self.is_train = is_train\n","        self.return_location = return_location\n","\n","    def __len__(self):\n","        return len(self.locations)\n","\n","    def __getitem__(self, idx):\n","        label = None\n","        location = np.array(self.locations[idx])\n","        y, x = location[0], location[1]\n","\n","        subvolume = extract_subvolume(location, self.volume)\n","        \n","        if self.labels is not None:\n","            label = self.labels[y - self.buffer:y + self.buffer, x - self.buffer:x + self.buffer]            \n","            label = np.stack([label], axis=-1)            \n","            \n","        # 段々meanは小さくなる\n","        mean = np.array([0.45 - i / 200 for i in range(0, CFG.Z_DIM)]).reshape(-1, 1, 1)\n","        # 段々stdは小さくなる\n","        std = np.array([0.25 for i in range(0, CFG.Z_DIM)]).reshape(-1, 1, 1)\n","        \n","        if self.is_train and label is not None:          \n","            transformed = A.Compose([\n","                A.HorizontalFlip(p=0.2),\n","                A.VerticalFlip(p=0.2),                \n","                A.Transpose(p=0.2),\n","                # A.RandomScale(p=0.2, scale_limit=0.2),\n","                # A.RandomRotate90(p=0.2),                \n","                A.ShiftScaleRotate(p=0.4, scale_limit=0.2,),\n","                # A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n","                # A.CoarseDropout(\n","                #     max_holes=1, \n","                #     max_width=int(CFG.BUFFER * 2 * 0.3),\n","                #     max_height=int(CFG.BUFFER * 2 * 0.3), \n","                #     mask_fill_value=0, \n","                #     p=0.5\n","                # ),\n","                # A.GridDistortion(p=0.2),                \n","                # MultichannelNoise(\n","                #     p=0.1,\n","                # ),                \n","                A.PadIfNeeded(min_height=self.buffer * 2, min_width=self.buffer * 2),\n","                A.Resize(height=self.buffer * 2, width=self.buffer * 2),\n","            ])(image=subvolume, mask=label)\n","            subvolume = transformed[\"image\"]\n","            label = transformed[\"mask\"]\n","            subvolume = np.transpose(subvolume, (2, 0, 1))\n","            label = np.transpose(label, (2, 0, 1))\n","            subvolume /= 255.\n","            subvolume = (subvolume - mean) / std       \n","        else:\n","            if label is None:\n","                subvolume = np.transpose(subvolume, (2, 0, 1))\n","                subvolume /= 255.\n","                subvolume = (subvolume - mean) / std\n","            else:\n","                # print(\"subvolume in val dataset (before aug)\", subvolume, file=open(\"before-val-aug.log\", \"w\")) \n","                subvolume = np.transpose(subvolume, (2, 0, 1))\n","                label = np.transpose(label, (2, 0, 1))\n","                subvolume /= 255.\n","                subvolume = (subvolume - mean) / std\n","        # print(\"subvolume\", subvolume)\n","        if self.return_location:\n","            return subvolume, location\n","        return subvolume, label        "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Model\n","* Encoder is a 3D ResNet model. The architecture has been modified to remove temporal downsampling between blocks.\n","* A 2D decoder is used for predicting the segmentation map.\n","* The encoder feature maps are average pooled over the Z dimension before passing it to the decoder."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class SmpUnetDecoder(nn.Module):\n","\tdef __init__(\n","\t\tself,\n","\t\tin_channel,\n","\t\tskip_channel,\n","\t\tout_channel,\n","\t):\n","\t\tsuper().__init__()\n","\t\tself.center = nn.Identity()\n","\n","\t\ti_channel = [\n","\t\t\tin_channel,\n","\t\t] + out_channel[:-1]\n","\t\ts_channel = skip_channel\n","\t\to_channel = out_channel\n","\t\tblock = [\n","\t\t\tDecoderBlock(i, s, o, use_batchnorm=True, attention_type=None)\n","\t\t\tfor i, s, o in zip(i_channel, s_channel, o_channel)\n","\t\t]\n","\t\tself.block = nn.ModuleList(block)\n","\n","\tdef forward(self, feature, skip):\n","\t\td = self.center(feature)\n","\t\tdecode = []\n","\t\tfor i, block in enumerate(self.block):\n","\t\t\ts = skip[i]\n","\t\t\td = block(d, s)\n","\t\t\tdecode.append(d)\n","\n","\t\tlast = d\n","\t\treturn last, decode"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, encoder_dims, upscale):\n","        super().__init__()\n","        self.convs = nn.ModuleList([\n","            nn.Sequential(\n","                nn.Conv2d(encoder_dims[i]+encoder_dims[i-1], encoder_dims[i-1], 3, 1, 1, bias=False),\n","                nn.BatchNorm2d(encoder_dims[i-1]),\n","                nn.ReLU(inplace=True)\n","            ) for i in range(1, len(encoder_dims))])\n","\n","        self.logit = nn.Conv2d(encoder_dims[0], 1, 1, 1, 0)\n","        self.up = nn.Upsample(scale_factor=upscale, mode=\"bicubic\")\n","\n","    def forward(self, feature_maps):\n","        for i in range(len(feature_maps)-1, 0, -1):\n","            f_up = F.interpolate(feature_maps[i], scale_factor=2, mode=\"bicubic\")\n","            f = torch.cat([feature_maps[i-1], f_up], dim=1)\n","            f_down = self.convs[i-1](f)\n","            feature_maps[i-1] = f_down\n","\n","        x = self.logit(feature_maps[0])\n","        # print(\"feature_map in decoder\", list(map(lambda x: x.shape, feature_maps)))\n","        mask = self.up(x)\n","        return mask, feature_maps[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class SegModel(nn.Module):\n","    def __init__(self,model_depth=CFG.MODEL_DEPTH):\n","        super().__init__()\n","        self.encoder = generate_model(model_depth=CFG.MODEL_DEPTH, n_input_channels=1)\n","        encoder_dims = [64, 128, 256, 512]\n","        self.decoder = Decoder(encoder_dims=encoder_dims, upscale=4)\n","  \n","        # self.logit1 = nn.Conv2d(encoder_dims[0], 1, 1, 1, 0)\n","        # self.up1 = nn.Upsample(scale_factor=4, mode=\"bicubic\")\n","        \n","        self.encoder2_dim = [64, 128, 256, 512]  #\n","        self.decoder2_dim = [\n","            128,\n","            64,\n","            32,\n","        ]\n","        self.encoder2 = resnet10t(pretrained=CFG.pretrained, in_chans=64)\n","\n","        self.decoder2 = SmpUnetDecoder(\n","            in_channel=self.encoder2_dim[-1],\n","            skip_channel=self.encoder2_dim[:-1][::-1],\n","            out_channel=self.decoder2_dim,\n","        )\n","        self.logit2 = nn.Conv2d(self.decoder2_dim[-1], 1, kernel_size=1)\n","\n","    def forward(self, x: torch.Tensor):\n","        B, C, D, H, W = x.shape\n","        assert x.dim() == 5\n","        # print(\"initial x.shape\", x.shape)\n","        feat_maps = self.encoder(x)\n","        feat_maps_pooled = [torch.mean(f, dim=2) for f in feat_maps]\n","        # print(\"feature map pooled\", list(map(lambda a: a.shape, feat_maps_pooled)))\n","        logit1, last = self.decoder(feat_maps_pooled)\n","  \n","        # logit1 = self.logit1(last)\n","        # print(\"logit1.shape\", logit1.shape)\n","        # logit1 = self.up1(logit1)\n","  \n","        x = last  # .detach()\n","        # x = F.avg_pool2d(x,kernel_size=2,stride=2)\n","        encoder = []\n","        e = self.encoder2\n","        x = e.layer1(x)\n","        encoder.append(x)\n","        x = e.layer2(x)\n","        encoder.append(x)\n","        x = e.layer3(x)\n","        encoder.append(x)\n","        x = e.layer4(x)\n","        encoder.append(x)\n","\n","        feature = encoder[-1]\n","        skip = encoder[:-1][::-1]\n","        last, decoder = self.decoder2(feature, skip)\n","        logit2 = self.logit2(last)\n","        # print(\"logit2.shape\", logit2.shape)\n","        logit2 = F.interpolate(\n","            logit2, size=(H, W), mode=\"bicubic\", align_corners=False, antialias=True\n","        )\n","  \n","        return logit1, logit2\n","    \n","    def load_pretrained_weights(self, state_dict):\n","        # Convert 3 channel weights to single channel\n","        # ref - https://timm.fast.ai/models#Case-1:-When-the-number-of-input-channels-is-1\n","        conv1_weight = state_dict['conv1.weight']\n","        state_dict['conv1.weight'] = conv1_weight.sum(dim=1, keepdim=True)\n","        print(self.encoder.load_state_dict(state_dict, strict=False))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class CustomModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.encoder=SegModel()\n","        weight_path = f\"r3d{CFG.MODEL_DEPTH}_K_200ep.pt\"\n","        if CFG.pretrained:\n","            self.encoder.load_pretrained_weights(torch.load(weight_path)[\"state_dict\"])\n","\n","    def forward(self, images:torch.Tensor):\n","        #image.shape=(b,C,H,W)\n","        if images.ndim==4:\n","            images=images[:,None]\n","        # images=normalization(images)\n","        output = self.encoder(images)\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T04:31:51.237798Z","iopub.status.busy":"2023-05-20T04:31:51.237305Z","iopub.status.idle":"2023-05-20T04:31:51.255734Z","shell.execute_reply":"2023-05-20T04:31:51.253840Z","shell.execute_reply.started":"2023-05-20T04:31:51.237647Z"},"trusted":true},"outputs":[],"source":["# class Decoder(nn.Module):\n","# \tdef __init__(self, encoder_dims, upscale):\n","# \t\tsuper().__init__()\n","# \t\tself.convs = nn.ModuleList([\n","# \t\t\tnn.Sequential(\n","# \t\t\t\tnn.Conv2d(encoder_dims[i]+encoder_dims[i-1], encoder_dims[i-1], 3, 1, 1, bias=False),\n","# \t\t\t\tnn.BatchNorm2d(encoder_dims[i-1]),\n","# \t\t\t\tnn.ReLU(inplace=True)\n","# \t\t\t) for i in range(1, len(encoder_dims))])\n","\n","# \t\tself.logit = nn.Conv2d(encoder_dims[0], 1, 1, 1, 0)\n","# \t\tself.up = nn.Upsample(scale_factor=upscale, mode=\"bicubic\")\n","\n","# \tdef forward(self, feature_maps):\n","# \t\tfor i in range(len(feature_maps)-1, 0, -1):\n","# \t\t\tf_up = F.interpolate(feature_maps[i], scale_factor=2, mode=\"bicubic\")\n","# \t\t\tf = torch.cat([feature_maps[i-1], f_up], dim=1)\n","# \t\t\tf_down = self.convs[i](f)\n","# \t\t\tfeature_maps[i-1] = f_down\n","\n","# \t\tx = self.logit(feature_maps[0])\n","# \t\tmask = self.up(x)\n","# \t\treturn mask, feature_maps[0]\n","\t\n","# class SegModel(nn.Module):\n","# \tdef __init__(self,model_depth=CFG.MODEL_DEPTH):\n","# \t\tsuper().__init__()\n","# \t\tself.encoder = generate_model(model_depth=CFG.MODEL_DEPTH, n_input_channels=1)\n","# \t\tencoder_dims = [64, 64, 128, 256, 512]\n","# \t\tself.decoder = Decoder(encoder_dims=encoder_dims, upscale=4)\n","  \t\t\n","# \t\tself.weight1 = nn.ModuleList(\n","# \t\t\t[\n","# \t\t\t\tnn.Sequential(\n","# \t\t\t\t\tnn.Conv3d(dim, dim, kernel_size=3, padding=1),\n","# \t \t\t\t\tnn.BatchNorm3d(dim),\n","# \t\t\t\t\tnn.ReLU(inplace=True),\n","# \t\t\t\t)\n","# \t\t\t\tfor dim in encoder_dims\n","# \t\t\t]\n","# \t\t)\n","  \n","# \t\tself.encoder2_dim = [64, 128, 256, 512]  #\n","# \t\tself.decoder2_dim = [\n","# \t\t\t128,\n","# \t\t\t64,\n","# \t\t\t32,\n","# \t\t]\n","# \t\tself.encoder2 = resnet10t(pretrained=CFG.pretrained, in_chans=64)\n","\n","# \t\tself.decoder2 = SmpUnetDecoder(\n","# \t\t\tin_channel=self.encoder2_dim[-1],\n","# \t\t\tskip_channel=self.encoder2_dim[:-1][::-1],\n","# \t\t\tout_channel=self.decoder2_dim,\n","# \t\t)\n","# \t\tself.logit2 = nn.Conv2d(self.decoder2_dim[-1], 1, kernel_size=1)\n","\n","\t\t\n","# \tdef forward(self, x: torch.Tensor):\n","# \t\tB, C, _, H, W = x.shape\n","# \t\tD = CFG.Z_DIM - CFG.BATCH_Z_DIFF\n","# \t\tassert x.dim() == 5\n","# \t\t# print(x.shape)\n","# \t\txx = [\n","# \t\t\tx[:, :, i: i + CFG.Z_DIM - CFG.BATCH_Z_DIFF]\n","# \t\t\tfor i in range(0, CFG.BATCH_Z_DIFF + 1, 2)\n","# \t\t]\n","# \t\tK = len(xx)\n","# \t\tx = torch.cat(xx, 0)\n","  \n","# \t\t# print(\"K:\", K)\n","# \t\t# print(\"x.shape\", x.shape)\n","\t\t\n","# \t\tencoder = []\n","# \t\te = self.encoder\n","# \t\tx = e.conv1(x)\n","# \t\tx = e.bn1(x)\n","# \t\tx = e.relu(x)\t\t\n","# \t\t# print(\"first encoder: preprocess conv layer ok\", x.shape)\t\n","# \t\tencoder.append(x)\n","# \t\tx = e.maxpool(x)\n","# \t\tx = e.layer1(x)\n","# \t\t# print(\"first encoder: first layer ok\", x.shape)\t\n","# \t\tencoder.append(x)\n","# \t\tx = e.layer2(x)\n","# \t\t# print(\"first encoder: second layer ok\", x.shape)\t\n","# \t\tencoder.append(x)\n","# \t\tx = e.layer3(x)\n","# \t\t# print(\"first encoder: third layer ok\", x.shape)\t\n","# \t\tencoder.append(x)\n","# \t\tx = e.layer4(x)\n","# \t\t# print(\"first encoder: fourth layer ok\", x.shape)\t\n","# \t\tencoder.append(x)\n","# \t\t# print('encoder', [f.shape for f in encoder])\n","\n","# \t\tfor i in range(len(encoder)):\n","# \t\t\te = encoder[i]\n","# \t\t\tf = self.weight1[i](e)\n","# \t\t\t_, _, _, h, w = e.shape\n","# \t\t\tf = rearrange(f, \"(K B) c d h w -> B K c d h w\", K=K, B=B, d=D, h=h, w=w)\n","# \t\t\te = rearrange(e, \"(K B) c d h w -> B K c d h w\", K=K, B=B, d=D, h=h, w=w)\n","# \t\t\tw = F.softmax(f, 1)\n","# \t\t\te = (w * e).sum(1)\n","# \t\t\tencoder[i] = e\t\t\n","# \t\t# print(\"first encoder ok\", list(map(lambda a: a.shape, encoder)))\n","# \t\tfeat_maps_pooled = [torch.mean(f, dim=2) for f in encoder]\n","# \t\t# print(\"feature map pooled\", list(map(lambda a: a.shape, feat_maps_pooled)))\n","# \t\tlogit1, last = self.decoder(feat_maps_pooled[1:])\n","  \n","# \t\t# print(\"logit1.shape\", logit1.shape)\n","# \t\t# print(\"last.shape\", last.shape)\n","  \n","# \t\t# --- second Unet ---\n","# \t\tx = last  # .detach()\n","# \t\t# x = F.avg_pool2d(x,kernel_size=2,stride=2)\n","# \t\tencoder = []\n","# \t\te = self.encoder2\n","# \t\tx = e.layer1(x)\n","# \t\tencoder.append(x)\n","# \t\tx = e.layer2(x)\n","# \t\tencoder.append(x)\n","# \t\tx = e.layer3(x)\n","# \t\tencoder.append(x)\n","# \t\tx = e.layer4(x)\n","# \t\tencoder.append(x)\n","\n","# \t\tfeature = encoder[-1]\n","# \t\tskip = encoder[:-1][::-1]\n","# \t\tlast, decoder = self.decoder2(feature, skip)\n","# \t\tlogit2 = self.logit2(last)\n","# \t\tlogit2 = F.interpolate(\n","# \t\t\tlogit2, size=(H, W), mode=\"bicubic\", align_corners=False, antialias=True\n","# \t\t)\n","\n","# \t\t# print(\"logit2.shape\", logit2.shape)\n","  \n","# \t\treturn logit1, logit2\n","\t\n","# \tdef load_pretrained_weights(self, state_dict):\n","# \t\t# Convert 3 channel weights to single channel\n","# \t\t# ref - https://timm.fast.ai/models#Case-1:-When-the-number-of-input-channels-is-1\n","# \t\tconv1_weight = state_dict['conv1.weight']\n","# \t\tstate_dict['conv1.weight'] = conv1_weight.sum(dim=1, keepdim=True)\n","# \t\tprint(self.encoder.load_state_dict(state_dict, strict=False))\n","\t\t\n","   \n","# class CustomModel(nn.Module):\n","# \tdef __init__(self):\n","# \t\tsuper().__init__()\n","\n","# \t\tself.encoder=SegModel()\n","# \t\tweight_path = f\"r3d{CFG.MODEL_DEPTH}_KM_200ep.pt\"\n","# \t\tif CFG.pretrained:\n","# \t\t\tself.encoder.load_pretrained_weights(torch.load(weight_path)[\"state_dict\"])\n","\n","# \tdef forward(self, images:torch.Tensor):\n","# \t\t#image.shape=(b,C,H,W)\n","# \t\tif images.ndim==4:\n","# \t\t\timages=images.unsqueeze(dim=1)\n","# \t\t# images=normalization(images)\n","# \t\toutput = self.encoder(images)\n","# \t\treturn output"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["net_check_input = torch.from_numpy(np.random.randn(12, 12, 128, 128).astype(np.float32))\n","out = CustomModel()(net_check_input)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Competition metric (F0.5 Score)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T04:31:55.157226Z","iopub.status.busy":"2023-05-20T04:31:55.156163Z","iopub.status.idle":"2023-05-20T04:31:55.166659Z","shell.execute_reply":"2023-05-20T04:31:55.165357Z","shell.execute_reply.started":"2023-05-20T04:31:55.157174Z"},"trusted":true},"outputs":[],"source":["# ref - https://www.kaggle.com/competitions/vesuvius-challenge-ink-detection/discussion/397288\n","def fbeta_score(preds, targets, threshold, beta=0.5, smooth=1e-5):\n","    preds_t = torch.where(preds > threshold, 1.0, 0.0).float()\n","    y_true_count = targets.sum()\n","    \n","    ctp = preds_t[targets==1].sum()\n","    cfp = preds_t[targets==0].sum()\n","    beta_squared = beta * beta\n","\n","    c_precision = ctp / (ctp + cfp + smooth)\n","    c_recall = ctp / (y_true_count + smooth)\n","    dice = (1 + beta_squared) * (c_precision * c_recall) / (beta_squared * c_precision + c_recall + smooth)\n","\n","    return dice"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tc = torch\n","def TTA(x:tc.Tensor,model:nn.Module):\n","    #x.shape=(batch,c,h,w)\n","    shape=x.shape\n","    x=[x,*[tc.rot90(x,k=i,dims=(-2,-1)) for i in range(1,4)]]\n","    x=tc.cat(x,dim=0)\n","    _, x = model(x)\n","    x=x.reshape(4,shape[0], 1 ,*shape[2:])\n","    x=[tc.rot90(x[i],k=-i,dims=(-2,-1)) for i in range(4)]\n","    x=tc.stack(x,dim=0)\n","    return x.mean(0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","class Model(pl.LightningModule):\n","    training_step_outputs = []\n","    validation_step_outputs = []\n","    test_step_outputs = [[], []]\n","\n","    def __init__(self, **kwargs):\n","        super().__init__()\n","\n","        self.model = CustomModel()        \n","\n","        self.loss1 = CFG.loss1\n","        self.loss2 = CFG.loss2\n","\n","    def forward(self, image, stage):\n","        if stage != \"train\":\n","            mask = TTA(image, self.model)\n","        else:\n","            mask = self.model(image)\n","        return mask\n","\n","    def shared_step(self, batch, stage):\n","        subvolumes, labels = batch\n","\n","        image, labels = subvolumes.float(), labels.float()        \n","        assert image.ndim == 4\n","        \n","        h, w = image.shape[2:]\n","        assert h % 32 == 0 and w % 32 == 0\n","        \n","        # print(\"labels\", labels.max(), labels.min())\n","\n","        assert labels.max() <= 1.0 and labels.min() >= 0\n","\n","        if stage == \"train\":\n","            logit1, logit2 = self.forward(image, stage)\n","            loss = self.loss1(logit1, labels) * CFG.loss1_weight + CFG.loss2_weight * self.loss2(logit2, labels)\n","            logit = logit2\n","        else:\n","            logit = self.forward(image, stage)\n","            loss = self.loss1(logit, labels) * CFG.loss1_weight + CFG.loss2_weight * self.loss2(logit, labels)\n","        \n","        prob2 = torch.sigmoid(logit)\n","\n","        pred_mask = (prob2 > CFG.threshold).float()\n","        \n","        # print(\"pred_mask\", pred_mask)\n","        \n","        score = fbeta_score(pred_mask, labels, threshold=CFG.threshold)\n","\n","        tp, fp, fn, tn = smp.metrics.get_stats(\n","            pred_mask.long(), labels.long(), mode=\"binary\"\n","        )\n","\n","        return {\n","            \"loss\": loss,\n","            \"tp\": tp,\n","            \"fp\": fp,\n","            \"fn\": fn,\n","            \"tn\": tn,\n","            \"score\": score,\n","        }\n","\n","    def shared_epoch_end(self, outputs, stage):\n","        # aggregate step metics\n","        tp = torch.cat([x[\"tp\"] for x in outputs])\n","        fp = torch.cat([x[\"fp\"] for x in outputs])\n","        fn = torch.cat([x[\"fn\"] for x in outputs])\n","        tn = torch.cat([x[\"tn\"] for x in outputs])\n","        loss = torch.mean(torch.Tensor([x[\"loss\"] for x in outputs]))\n","        fbeta_score = torch.mean(torch.Tensor([x[\"score\"] for x in outputs]))\n","\n","        per_image_iou = smp.metrics.iou_score(\n","            tp, fp, fn, tn, reduction=\"micro-imagewise\"\n","        )\n","\n","        dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n","\n","        metrics = {\n","            f\"{stage}_per_image_iou\": per_image_iou,\n","            f\"{stage}_dataset_iou\": dataset_iou,\n","            f\"{stage}_loss\": 10000 if loss.item() == 0 else loss.item(),\n","            f\"{stage}_tp\": tp.sum().int().item(),\n","            f\"{stage}_fp\": fp.sum().int().item(),\n","            f\"{stage}_fn\": fn.sum().int().item(),\n","            f\"{stage}_tn\": tn.sum().int().item(),\n","            f\"{stage}_score\": fbeta_score.item(),\n","        }\n","\n","        self.log_dict(metrics, prog_bar=True, sync_dist=True)\n","\n","    def training_step(self, batch, batch_idx):\n","        out = self.shared_step(batch, \"train\")\n","        self.training_step_outputs.append(out)\n","        return out\n","\n","    def on_train_epoch_end(self):\n","        out = self.shared_epoch_end(self.training_step_outputs, \"train\")\n","        self.training_step_outputs.clear()\n","        return out\n","\n","    def validation_step(self, batch, batch_idx):\n","        out = self.shared_step(batch, \"valid\")\n","        self.validation_step_outputs.append(out)\n","        return out\n","\n","    def on_validation_epoch_end(self):\n","        out = self.shared_epoch_end(self.validation_step_outputs, \"valid\")\n","        self.validation_step_outputs.clear()\n","        return out\n","\n","    def test_step(self, batch, batch_idx):\n","        global predictions_map, predictions_map_counts\n","\n","        patch_batch, loc_batch = batch\n","\n","        loc_batch = loc_batch.long()\n","        patch_batch = patch_batch.float()\n","        predictions = self.forward(patch_batch, \"test\")\n","        predictions = predictions.sigmoid()\n"," \n","        predictions = torch.permute(predictions, (0, 2, 3, 1)).squeeze(dim=-1)\n","        predictions = (\n","            predictions.cpu().numpy()\n","        )\n","        loc_batch = loc_batch.cpu().numpy()\n","        \n","        self.test_step_outputs[0].extend(loc_batch)\n","        self.test_step_outputs[1].extend(predictions)        \n","        return loc_batch, predictions\n","\n","    def on_test_epoch_end(self):\n","        global predictions_map, predictions_map_counts\n","\n","        locs = np.array(self.test_step_outputs[0])\n","        preds = np.array(self.test_step_outputs[1])\n","        print(\"locs\", locs.shape)\n","        print(\"preds\", preds.shape)\n","\n","\n","        for (y, x), pred in zip(locs, preds):\n","            predictions_map[\n","                y - CFG.BUFFER : y + CFG.BUFFER, x - CFG.BUFFER : x + CFG.BUFFER\n","            ] += pred\n","            predictions_map_counts[\n","                y - CFG.BUFFER : y + CFG.BUFFER, x - CFG.BUFFER : x + CFG.BUFFER\n","            ] += 1\n","        \n","        predictions_map /= predictions_map_counts + CFG.exp\n","\n","    def configure_optimizers(self):\n","        optimizer = optim.AdamW(self.parameters(), lr=CFG.lr)\n","\n","        scheduler = CFG.lr_scheduler(\n","            optimizer, T_0=CFG.num_epochs, T_mult=2, eta_min=CFG.eta_min_lr,\n","        )\n","        return {\n","            \"optimizer\": optimizer,\n","            \"lr_scheduler\": scheduler,\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sample_data_name = \"2a\"\n","sample_volume = FRAGMENTS_ZARR[sample_data_name][\"surface_volume\"]\n","sample_mask = FRAGMENTS_ZARR[sample_data_name][\"mask\"]\n","sample_label = FRAGMENTS_ZARR[sample_data_name][\"truth\"]\n","sample_locations = generate_locations_ds(sample_volume, sample_mask, sample_label, skip_zero=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sample_ds = SubvolumeDataset(\n","    sample_locations,\n","    sample_volume,\n","    sample_label,\n","    CFG.BUFFER,\n","    is_train=True,\n",")\n","\n","id = 240\n","sample_depth = CFG.Z_DIM // 2\n","\n","fig, ax = plt.subplots(1, 2, figsize=(8, 6))\n","\n","img = sample_ds[id][0][sample_depth, :, :]\n","label = sample_ds[id][1][0, :, :]\n","ax[0].imshow(img)\n","ax[1].imshow(label)\n","\n","fig, ax = plt.subplots(figsize=(8, 6))\n","\n","ax.imshow(sample_label)\n","\n","y, x = sample_locations[id]\n","patch = patches.Rectangle([x - CFG.BUFFER, y - CFG.BUFFER], 2 * CFG.BUFFER, 2 * CFG.BUFFER, linewidth=2, edgecolor='g', facecolor='none')\n","ax.add_patch(patch)\n","plt.show()    \n","\n","fig, ax = plt.subplots(CFG.Z_DIM, 1, figsize=(12, 24))\n","\n","for i in range(CFG.Z_DIM):\n","    img, _ = sample_ds[id]\n","    img = img[i, :, :]\n","    ax[i].hist(img.flatten(), bins=1000)  # Plot histogram of the flattened data\n","    ax[i].set_title(f\"Histogram of Channel {i}\")  # Add title to the plot    \n","fig.tight_layout()\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T04:31:55.640175Z","iopub.status.busy":"2023-05-20T04:31:55.639774Z","iopub.status.idle":"2023-05-20T05:40:02.825430Z","shell.execute_reply":"2023-05-20T05:40:02.824131Z","shell.execute_reply.started":"2023-05-20T04:31:55.640135Z"},"trusted":true},"outputs":[],"source":["\n","\n","k_folds = 4\n","kfold = KFold(\n","    n_splits=k_folds,\n","    shuffle=True\n",")\n","\n","# 0: 1, 1: 2a（下側）, 2: 2b（上側）, 3: 3\n","data_list = [\n","    (\n","        FRAGMENTS_ZARR[key][\"surface_volume\"], \n","        FRAGMENTS_ZARR[key][\"truth\"],\n","        FRAGMENTS_ZARR[key][\"mask\"],         \n","    ) for key in [\"1\", \"2a\", \"2b\", \"3\"]\n","]\n","\n","predictions_map = None\n","predictions_map_counts = None\n","\n","for fold, (train_data, val_data) in enumerate(kfold.split(data_list)):\n","    print(f'FOLD {fold}')\n","    print('--------------------------------')\n","    print(\"train_data\", train_data)\n","    print(\"val_data\", val_data)\n","    one = data_list[train_data[0]]\n","    two = data_list[train_data[1]]\n","    three = data_list[train_data[2]]\n","    train_volume = np.concatenate([one[0], two[0], three[0]], axis=1)\n","    train_label = np.concatenate([one[1], two[1], three[1]], axis=1)\n","    train_mask = np.concatenate([one[2], two[2], three[2]], axis=1)\n","    val_volume, val_label, val_mask = data_list[val_data[0]]    \n","\n","    train_locations_ds = generate_locations_ds(train_volume, train_mask, train_label, skip_zero=True)\n","    val_location_ds = generate_locations_ds(val_volume, val_mask, skip_zero=False)\n","\n","    visualize_dataset_patches(train_locations_ds, train_label, \"train\", fold)\n","    visualize_dataset_patches(val_location_ds, val_label, \"val\", fold)\n","    \n","    # Init the neural network\n","    model = Model()\n","\n","    wandb.finish()\n","    # Initialize a trainer\n","    now = datetime.datetime.now()\n","    now = f\"{now}\".replace(\" \", \"\")\n","    \n","    checkpoint_callback = pytorch_lightning.callbacks.ModelCheckpoint(\n","        monitor='valid_score',\n","        dirpath=os.path.join(CFG.DATA_DIR, f\"best-{now}-fold-{fold}-val-{val_data[0]}\"),\n","        mode=\"max\",\n","        filename='3dcnn-train-fold-' + str(fold) + '-{epoch:02d}-{valid_score:.3f}',\n","        save_last=True,\n","    )\n","    \n","    trainer = pl.Trainer(\n","        max_epochs=CFG.num_epochs,\n","        precision=16,\n","        devices=\"0,1,2,3\",\n","        accelerator=\"gpu\",\n","        # strategy=\"ddp_find_unused_parameters_false\",\n","        # strategy=\"ddp_fork\",\n","        logger=WandbLogger(\n","            name=f\"3dcnn-{now}-fold-{fold}-val-{val_data[0]}\",\n","            notes=CFG.WANDB_NOTE,\n","            config=class2dict(CFG()),\n","        ),\n","        callbacks=[checkpoint_callback],\n","        default_root_dir=os.path.join(CFG.DATA_DIR, f\"{now}-fold-{fold}-val-{val_data[0]}\"),\n","    )\n","    \n","    # Sample elements randomly from a given list of ids, no replacement.\n","    train_ds = SubvolumeDataset(\n","        train_locations_ds,\n","        train_volume,\n","        train_label,\n","        CFG.BUFFER,\n","        is_train=True\n","    )\n","    val_ds = SubvolumeDataset(\n","        val_location_ds,\n","        val_volume,\n","        val_label,\n","        CFG.BUFFER,\n","        is_train=False,\n","    )\n","\n","    # Define data loaders for training and testing data in this fold\n","    train_loader = torch.utils.data.DataLoader(\n","        train_ds,\n","        batch_size=CFG.BATCH_SIZE,\n","        num_workers=CFG.num_workers,\n","        shuffle=True,\n","        # persistent_workers=True,\n","        # pin_memory=True,\n","    )\n","    val_loader = torch.utils.data.DataLoader(\n","        val_ds, \n","        batch_size=CFG.BATCH_SIZE,\n","        num_workers=CFG.num_workers,\n","        shuffle=False,\n","        # pin_memory=True,\n","        # persistent_workers=True,\n","    )\n","\n","    # Train the model\n","    trainer.fit(model, train_loader, val_loader)   \n","    \n","wandb.finish()\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat":4,"nbformat_minor":4}
