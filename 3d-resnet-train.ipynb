{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Vesuvius Challenge - Ink Detection Training Notebook"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Setup"]},{"cell_type":"code","execution_count":73,"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-05-20T04:31:07.456867Z","iopub.status.busy":"2023-05-20T04:31:07.456553Z","iopub.status.idle":"2023-05-20T04:31:44.807975Z","shell.execute_reply":"2023-05-20T04:31:44.806511Z","shell.execute_reply.started":"2023-05-20T04:31:07.456835Z"},"trusted":true},"outputs":[],"source":["# # Pretrained weights\n","# # ref - https://github.com/kenshohara/3D-ResNets-PyTorch\n","# !pip install gdown\n","# !gdown 1Nb4abvIkkp_ydPFA9sNPT1WakoVKA8Fa\n","\n","# # Utility packages for reading and visualizing volumes\n","# !pip install zarr imageio-ffmpeg\n","\n","# # save model checkpoints\n","# !mkdir ./ckpts\n","\n","# !kaggle datasets download -d samfc10/vesuvius-zarr-files"]},{"cell_type":"code","execution_count":74,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T04:31:44.811420Z","iopub.status.busy":"2023-05-20T04:31:44.811084Z","iopub.status.idle":"2023-05-20T04:31:48.016722Z","shell.execute_reply":"2023-05-20T04:31:48.015498Z","shell.execute_reply.started":"2023-05-20T04:31:44.811386Z"},"trusted":true},"outputs":[],"source":["import os\n","import gc\n","import sys\n","import zarr\n","import random\n","import imageio\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import os,cv2\n","import gc\n","import sys\n","import random\n","from glob import glob\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import pytorch_lightning as pl\n","from pytorch_lightning.loggers import WandbLogger\n","\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.cuda import amp\n","from torch.utils.data import Dataset, DataLoader\n","import segmentation_models_pytorch as smp\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from IPython.display import Video\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.cuda import amp\n","from torch.utils.data import Dataset, DataLoader\n","\n","# sys.path.append(\"/kaggle/input/resnet3d\")\n","ROOT_DIR = '/home/fummicc1/codes/competitions/kaggle-ink-detection/'\n","sys.path.append(ROOT_DIR)\n","from resnet3d import generate_model"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Config"]},{"cell_type":"code","execution_count":75,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T04:31:48.019510Z","iopub.status.busy":"2023-05-20T04:31:48.018523Z","iopub.status.idle":"2023-05-20T04:31:48.026182Z","shell.execute_reply":"2023-05-20T04:31:48.025018Z","shell.execute_reply.started":"2023-05-20T04:31:48.019466Z"},"trusted":true},"outputs":[],"source":["class CFG:\n","    DATA_DIR = ROOT_DIR\n","    # DATA_DIR = \"/kaggle/input/vesuvius-zarr-files/\"\n","    LR = 1e-3\n","    EPOCHS = 30\n","    BATCH_SIZE = 64\n","    NUM_WORKERS = 8\n","    CROP_SIZE = 224\n","    Z_LIST = list(range(24, 36, 1))\n","    TRAIN_FRAGMENTS = [\"1\", \"3\"]\n","    TEST_FRAGMENT = \"2\"\n","    MODEL_DEPTH = 34\n","    MAX_PIXEL_VALUE = 2 ** 16 - 1\n","    RESIZE_HEIGHT = 4800\n","    COMP_DATASET_PATH = \"/home/fummicc1/codes/competitions/kaggle-ink-detection/\"\n","    \n","    \n","    Z_START = Z_LIST[0]    \n","    Z_DIM = len(Z_LIST)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Load data"]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[],"source":["def resize(img):\n","    return img\n","    current_height, current_width = img.shape[:2]\n","    aspect_ratio = current_width / current_height\n","    new_height = RESIZE_HEIGHT\n","    new_width = int(new_height * aspect_ratio)\n","    new_size = (new_width, new_height)\n","    # (W, H)の順で渡すが結果は(H, W)になっている\n","    img = cv2.resize(img, new_size)\n","    return img"]},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[],"source":["def load_mask(split, index):\n","    img = cv2.imread(f\"{CFG.DATA_DIR}/{split}/{index}/mask.png\", 0) // 255    \n","    img = resize(img)    \n","    return img\n","\n","def load_labels(split, index):\n","    img = cv2.imread(f\"{CFG.DATA_DIR}/{split}/{index}/inklabels.png\", 0) // 255\n","    img = resize(img)\n","    return img"]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[],"source":["def calculate_mean_std(volume):\n","    # mean_0 = np.mean(volume / 255, axis=0)\n","    # mean_1 = np.mean(mean_0, axis=0)\n","    # std_0 = np.std(volume / 255, axis=0)\n","    # std_1 = np.std(std_0, axis=0)\n","    mean_1 = np.array([0.45 for i in range(CFG.Z_DIM)])\n","    std_1 = np.array([0.225 for i in range(CFG.Z_DIM)])\n","    return mean_1, std_1\n"]},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[],"source":["def read_image(mode, fragment_id):\n","    images = []\n","\n","    # idxs = range(65)\n","    start = 0\n","    end = 65\n","    idxs = range(start, end)\n","\n","    for i in tqdm(idxs):\n","        image = cv2.imread(CFG.COMP_DATASET_PATH + f\"{mode}/{fragment_id}/surface_volume/{i:02}.tif\", -1)\n","        # image = cv2.imread(CFG.comp_dataset_path + f\"{mode}/{fragment_id}/surface_volume/{i:02}.tif\", 0)\n","        \n","        # print(image.max())\n","        image = resize(image).astype(np.float32)     \n","        # print(image.max())\n","\n","        # pad0 = (CROP_SIZE - image.shape[0] % CROP_SIZE)\n","        # pad1 = (CROP_SIZE - image.shape[1] % CROP_SIZE)\n","\n","        # image = np.pad(image, [(0, pad0), (0, pad1)], constant_values=0)\n","        # print(image.max())\n","        images.append(image)\n","    images = np.stack(images, axis=2, dtype=np.float32)\n","    # print(images.max())\n","    return images"]},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[],"source":["FRAGMENTS_ZARR = {\n","    \"1\" : {},\n","    \"2\" : {},\n","    \"3\" : {},\n","}\n"]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 65/65 [00:05<00:00, 11.44it/s]\n","100%|██████████| 65/65 [00:14<00:00,  4.46it/s]\n"]}],"source":["\n","for key in [\"1\", \"2\", \"3\"]:\n","    FRAGMENTS_ZARR[key][\"surface_volume\"] = read_image(\"train\", key)\n","    FRAGMENTS_ZARR[key][\"mask\"] = load_mask(\"train\", key)\n","    FRAGMENTS_ZARR[key][\"truth\"] = load_labels(\"train\", key)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T04:31:48.029208Z","iopub.status.busy":"2023-05-20T04:31:48.027973Z","iopub.status.idle":"2023-05-20T04:31:48.122129Z","shell.execute_reply":"2023-05-20T04:31:48.121088Z","shell.execute_reply.started":"2023-05-20T04:31:48.029167Z"},"trusted":true},"outputs":[],"source":["FRAGMENTS_SHAPE = {k : v[\"surface_volume\"].shape[:-1] for k, v in FRAGMENTS_ZARR.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","ALL_MEAN_STD = {}\n","img = FRAGMENTS_ZARR[\"1\"][\"surface_volume\"][:, :, CFG.Z_START:CFG.Z_START+CFG.Z_DIM]\n","mean, std = calculate_mean_std(img)\n","for key in [\"1\", \"2\", \"3\"]:    \n","    ALL_MEAN_STD[key] = mean, std\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mean, std = ALL_MEAN_STD[\"2\"]\n","mean, std"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["list(FRAGMENTS_ZARR[\"1\"].keys())\n","# np.unique(np.array(FRAGMENTS_ZARR[\"1\"][\"surface_volume\"]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.imshow(FRAGMENTS_ZARR[\"1\"][\"truth\"])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def fetch_fragment_crop(id: str, y1, y2, x1, x2, z1=None, z2=None, key: str = \"surface_volume\"):\n","    if key == \"surface_volume\":\n","        img = FRAGMENTS_ZARR[id][key][y1:y2, x1:x2, z1:z2]\n","        return img.astype(np.float32)\n","    elif key == \"truth\":\n","        img = FRAGMENTS_ZARR[id][key][y1:y2, x1:x2]\n","        return img.astype(np.uint8)\n","    print(\"Error\", key)\n","\n","def fetch_fragment(id: str, z1=None, z2=None, key: str = \"surface_volume\"):\n","    if key == \"surface_volume\":\n","        img = FRAGMENTS_ZARR[id][key][:, :, z1:z2]\n","        return img.astype(np.float32)\n","    elif key == \"truth\":\n","        img = FRAGMENTS_ZARR[id][key]\n","        return img.astype(np.uint8)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.imshow(FRAGMENTS_ZARR[\"1\"][\"truth\"][:,:])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Visualise input"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T04:31:48.128130Z","iopub.status.busy":"2023-05-20T04:31:48.127294Z","iopub.status.idle":"2023-05-20T04:31:50.740007Z","shell.execute_reply":"2023-05-20T04:31:50.739022Z","shell.execute_reply.started":"2023-05-20T04:31:48.128088Z"},"trusted":true},"outputs":[],"source":["fragment_id = FRAGMENTS_ZARR[\"1\"]\n","x, y = 3000, 4000\n","\n","# np.unique(FRAGMENTS_ZARR[\"1\"][\"surface_volume\"][y:y+CROP_SIZE, x:x+CROP_SIZE, :])\n","fragment_cropped = fetch_fragment_crop(\"1\", y, y+CFG.CROP_SIZE, x, x+CFG.CROP_SIZE, CFG.Z_START, CFG.Z_START+CFG.Z_DIM)\n","imageio.mimwrite(\"fragment_crop.mp4\", (fragment_cropped.transpose(2, 0, 1) / 255), \"ffmpeg\")\n","Video(\"fragment_crop.mp4\", height=256, width=256)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T04:31:50.742410Z","iopub.status.busy":"2023-05-20T04:31:50.741764Z","iopub.status.idle":"2023-05-20T04:31:51.014241Z","shell.execute_reply":"2023-05-20T04:31:51.012587Z","shell.execute_reply.started":"2023-05-20T04:31:50.742366Z"},"trusted":true},"outputs":[],"source":["mask_cropped = fragment_id[\"truth\"][y:y+CFG.CROP_SIZE, x:x+CFG.CROP_SIZE]\n","# ir_cropped = fragment_id[\"infrared\"][y:y+CROP_SIZE, x:x+CROP_SIZE]\n","\n","plt.figure(figsize=(6, 3))\n","plt.subplot(1, 2, 1)\n","plt.imshow(mask_cropped, cmap=\"gray\")\n","plt.axis(\"off\")\n","\n","# plt.subplot(1, 2, 2)\n","# plt.imshow(ir_cropped, cmap=\"gray\")\n","# plt.axis(\"off\")\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T04:31:51.022260Z","iopub.status.busy":"2023-05-20T04:31:51.016326Z","iopub.status.idle":"2023-05-20T04:31:51.180463Z","shell.execute_reply":"2023-05-20T04:31:51.179062Z","shell.execute_reply.started":"2023-05-20T04:31:51.022203Z"},"trusted":true},"outputs":[],"source":["# del fragment_id, fragment_cropped, mask_cropped, ir_cropped\n","# gc.collect()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Dataloaders"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T04:31:51.183662Z","iopub.status.busy":"2023-05-20T04:31:51.182694Z","iopub.status.idle":"2023-05-20T04:31:51.199714Z","shell.execute_reply":"2023-05-20T04:31:51.198664Z","shell.execute_reply.started":"2023-05-20T04:31:51.183616Z"},"trusted":true},"outputs":[],"source":["\n","class VesuviusTrain(Dataset):\n","    def __init__(self, fragments):\n","        self.fragments = fragments\n","        self.xys = []\n","        \n","        for fragment_id in fragments:\n","            H, W = FRAGMENTS_SHAPE[fragment_id]\n","            for y in range(0, H-CFG.CROP_SIZE+1, CFG.CROP_SIZE):\n","                for x in range(0, W-CFG.CROP_SIZE+1, CFG.CROP_SIZE):\n","                    self.xys.append((fragment_id, x, y, W, H))\n","        \n","    def __getitem__(self, i):\n","        fragment_id, x1, y1, W, H = self.xys[i]\n","        z1, z2 = CFG.Z_START, CFG.Z_START + CFG.Z_DIM\n","        \n","        x2 = x1 + CFG.CROP_SIZE\n","        y2 = y1 + CFG.CROP_SIZE\n","            \n","        # print(\"x1: \", x1, \" x2: \", x2, \" y1: \", y1, \" y2: \", y2)\n","        \n","        frag_crop = fetch_fragment_crop(fragment_id, y1, y2, x1, x2, z1, z2)\n","        label_crop = fetch_fragment_crop(fragment_id, y1, y2, x1, x2, key=\"truth\")[:, :, np.newaxis]\n","        \n","        frag_crop /= (2 ** 16 - 1)\n","                                                        \n","        performed = A.Compose([\n","            A.ToFloat(max_value=1),\n","            A.HorizontalFlip(p=0.2), # 水平方向に反転\n","            A.VerticalFlip(p=0.2), # 水平方向に反転\n","            A.RandomScale(p=0.2),\n","            A.RandomRotate90(p=0.2),\n","            # A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.3),\n","            # A.CoarseDropout(\n","            #     max_holes=1,\n","            #     max_width=int(CROP_SIZE * 0.1), \n","            #     max_height=int(CROP_SIZE * 0.1), \n","            #     mask_fill_value=0,\n","            #     p=0.3\n","            # ),\n","            A.ShiftScaleRotate(p=0.2),\n","            A.Resize(height=CFG.CROP_SIZE, width=CFG.CROP_SIZE),\n","            A.FromFloat(max_value=1),\n","        ])(image=frag_crop, mask=label_crop)\n","        frag_crop = performed[\"image\"]        \n","        label_crop = performed[\"mask\"]\n","        \n","        frag_crop = frag_crop.astype(np.float16)\n","        \n","        # print(\"frag_crop\", frag_crop[100, 100, :])\n","        \n","        mean, std = ALL_MEAN_STD[fragment_id]\n","        frag_crop = (frag_crop - mean) / std        \n","        frag_crop = torch.from_numpy(frag_crop.astype(np.float32)).permute(2, 0, 1)        \n","        \n","        label_crop = torch.from_numpy(label_crop.astype(np.float32)).permute(2, 0, 1)\n","        \n","        # print(\"frag_crop after norm\", frag_crop[:, 100, 100])\n","        \n","        assert frag_crop.shape[0] == len(CFG.Z_LIST)\n","        \n","        return {\n","            \"images\": frag_crop,\n","            \"labels\": label_crop\n","        }\n","\n","    def __len__(self):\n","        return len(self.xys)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ALL_MEAN_STD[\"1\"][0].shape, ALL_MEAN_STD[\"1\"][1].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T04:31:51.202022Z","iopub.status.busy":"2023-05-20T04:31:51.201511Z","iopub.status.idle":"2023-05-20T04:31:51.217253Z","shell.execute_reply":"2023-05-20T04:31:51.215850Z","shell.execute_reply.started":"2023-05-20T04:31:51.201981Z"},"trusted":true},"outputs":[],"source":["class VesuviusVal(Dataset):\n","    def __init__(self, fragment_id):\n","        self.fragment_id = fragment_id\n","        self.xys = []\n","        \n","        H, W = FRAGMENTS_SHAPE[fragment_id]\n","        for y in range(0, H-CFG.CROP_SIZE+1, CFG.CROP_SIZE):\n","            for x in range(0, W-CFG.CROP_SIZE+1, CFG.CROP_SIZE):\n","                self.xys.append((x, y))\n","                \n","    def __getitem__(self, i):\n","        x1, y1 = self.xys[i]\n","        x2, y2 = x1+CFG.CROP_SIZE, y1+CFG.CROP_SIZE\n","        z1, z2 = CFG.Z_START, CFG.Z_START + CFG.Z_DIM\n","        \n","        frag_crop = fetch_fragment_crop(self.fragment_id, y1, y2, x1, x2, z1, z2)\n","        \n","        label_crop = fetch_fragment_crop(self.fragment_id, y1, y2, x1, x2, key=\"truth\")[:, :, np.newaxis]\n","        \n","        frag_crop = frag_crop / 255.0 / 255.0        \n","\n","        mean, std = ALL_MEAN_STD[self.fragment_id]\n","        \n","        frag_crop = (frag_crop - mean) / std\n","        frag_crop = torch.from_numpy(frag_crop.astype(np.float32)).permute(2, 0, 1)        \n","        \n","        label_crop = torch.from_numpy(label_crop.astype(np.float32)).permute(2, 0, 1)\n","        \n","        # print(\"frag_crop\", frag_crop)\n","        \n","        assert frag_crop.shape[0] == len(CFG.Z_LIST)\n","        \n","        return {\n","            \"images\": frag_crop,\n","            \"labels\": label_crop,\n","            \"locations\": torch.tensor([x1, y1, x2, y2], dtype=torch.int32)\n","        }\n","\n","    def __len__(self):\n","        return len(self.xys)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def collate_train_fn(batch):\n","  return {\n","      'images': torch.stack([x['images'] for x in batch]),\n","      'labels': torch.stack([x['labels'] for x in batch])\n","}\n","  \n","  \n","def collate_val_fn(batch):\n","  return {\n","      'images': torch.stack([x['images'] for x in batch]),\n","      'labels': torch.stack([x['labels'] for x in batch]),\n","      'locations': torch.stack([x['locations'] for x in batch])\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T04:31:51.221224Z","iopub.status.busy":"2023-05-20T04:31:51.220772Z","iopub.status.idle":"2023-05-20T04:31:51.235291Z","shell.execute_reply":"2023-05-20T04:31:51.234177Z","shell.execute_reply.started":"2023-05-20T04:31:51.221184Z"},"trusted":true},"outputs":[],"source":["dataset_train = VesuviusTrain(CFG.TRAIN_FRAGMENTS)\n","dataloader_train = DataLoader(\n","    dataset_train, \n","    batch_size=CFG.BATCH_SIZE, \n","    num_workers=CFG.NUM_WORKERS,\n","    collate_fn=collate_train_fn,\n","    shuffle=True\n",")\n","n_train = len(dataloader_train)\n","\n","dataset_valid = VesuviusVal(CFG.TEST_FRAGMENT)\n","dataloader_valid = DataLoader(\n","    dataset_valid, \n","    batch_size=CFG.BATCH_SIZE, \n","    num_workers=CFG.NUM_WORKERS,\n","    collate_fn=collate_val_fn,\n","    shuffle=False\n",")\n","n_valid = len(dataloader_valid)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Model\n","* Encoder is a 3D ResNet model. The architecture has been modified to remove temporal downsampling between blocks.\n","* A 2D decoder is used for predicting the segmentation map.\n","* The encoder feature maps are average pooled over the Z dimension before passing it to the decoder."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T04:31:51.237798Z","iopub.status.busy":"2023-05-20T04:31:51.237305Z","iopub.status.idle":"2023-05-20T04:31:51.255734Z","shell.execute_reply":"2023-05-20T04:31:51.253840Z","shell.execute_reply.started":"2023-05-20T04:31:51.237647Z"},"trusted":true},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, encoder_dims, upscale):\n","        super().__init__()\n","        self.convs = nn.ModuleList([\n","            nn.Sequential(\n","                nn.Conv2d(encoder_dims[i]+encoder_dims[i-1], encoder_dims[i-1], 3, 1, 1, bias=False),\n","                nn.BatchNorm2d(encoder_dims[i-1]),\n","                nn.ReLU(inplace=True)\n","            ) for i in range(1, len(encoder_dims))])\n","\n","        self.logit = nn.Conv2d(encoder_dims[0], 1, 1, 1, 0)\n","        self.up = nn.Upsample(scale_factor=upscale, mode=\"bilinear\")\n","\n","    def forward(self, feature_maps):\n","        for i in range(len(feature_maps)-1, 0, -1):\n","            f_up = F.interpolate(feature_maps[i], scale_factor=2, mode=\"bilinear\")\n","            f = torch.cat([feature_maps[i-1], f_up], dim=1)\n","            f_down = self.convs[i-1](f)\n","            feature_maps[i-1] = f_down\n","\n","        x = self.logit(feature_maps[0])\n","        mask = self.up(x)\n","        return mask\n","    \n","class SegModel(nn.Module):\n","    def __init__(self,model_depth=CFG.MODEL_DEPTH):\n","        super().__init__()\n","        self.encoder = generate_model(model_depth=CFG.MODEL_DEPTH, n_input_channels=1)\n","        self.decoder = Decoder(encoder_dims=[64, 128, 256, 512], upscale=4)\n","        \n","    def forward(self, x):\n","        if x.ndim==4:\n","            x=x[:,None] # チャネルを追加\n","        \n","        feat_maps = self.encoder(x)\n","        feat_maps_pooled = [torch.mean(f, dim=2) for f in feat_maps]\n","        pred_mask = self.decoder(feat_maps_pooled)\n","        return pred_mask\n","    \n","    def load_pretrained_weights(self, state_dict):\n","        # Convert 3 channel weights to single channel\n","        # ref - https://timm.fast.ai/models#Case-1:-When-the-number-of-input-channels-is-1\n","        conv1_weight = state_dict['conv1.weight']\n","        state_dict['conv1.weight'] = conv1_weight.sum(dim=1, keepdim=True)\n","        print(self.encoder.load_state_dict(state_dict, strict=False))\n","        \n","   \n","class CustomModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.encoder=SegModel()\n","        weight_path = \"r3d18_K_200ep.pth\"\n","        if os.path.exists(weight_path):\n","            self.encoder.load_pretrained_weights(torch.load(weight_path)[\"state_dict\"])\n","\n","    def forward(self, images:torch.Tensor):\n","        #image.shape=(b,C,H,W)\n","        if images.ndim==4:\n","            images=images[:,None]\n","        # images=normalization(images)\n","        output = self.encoder(images)\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T04:31:51.257977Z","iopub.status.busy":"2023-05-20T04:31:51.257691Z","iopub.status.idle":"2023-05-20T04:31:55.154544Z","shell.execute_reply":"2023-05-20T04:31:55.153397Z","shell.execute_reply.started":"2023-05-20T04:31:51.257929Z"},"trusted":true},"outputs":[],"source":["model = CustomModel()\n","model = nn.DataParallel(model, device_ids=[0, 1, 2, 3])\n","model = model.cuda()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Competition metric (F0.5 Score)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T04:31:55.157226Z","iopub.status.busy":"2023-05-20T04:31:55.156163Z","iopub.status.idle":"2023-05-20T04:31:55.166659Z","shell.execute_reply":"2023-05-20T04:31:55.165357Z","shell.execute_reply.started":"2023-05-20T04:31:55.157174Z"},"trusted":true},"outputs":[],"source":["# ref - https://www.kaggle.com/competitions/vesuvius-challenge-ink-detection/discussion/397288\n","def fbeta_score(preds, targets, threshold, beta=0.5, smooth=1e-5):\n","    preds_t = torch.where(preds > threshold, 1.0, 0.0).float()\n","    y_true_count = targets.sum()\n","    \n","    ctp = preds_t[targets==1].sum()\n","    cfp = preds_t[targets==0].sum()\n","    beta_squared = beta * beta\n","\n","    c_precision = ctp / (ctp + cfp + smooth)\n","    c_recall = ctp / (y_true_count + smooth)\n","    dice = (1 + beta_squared) * (c_precision * c_recall) / (beta_squared * c_precision + c_recall + smooth)\n","\n","    return dice"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T04:31:55.172471Z","iopub.status.busy":"2023-05-20T04:31:55.172081Z","iopub.status.idle":"2023-05-20T04:31:55.185506Z","shell.execute_reply":"2023-05-20T04:31:55.184362Z","shell.execute_reply.started":"2023-05-20T04:31:55.172439Z"},"trusted":true},"outputs":[],"source":["scaler = amp.GradScaler()\n","TverskyLoss = smp.losses.TverskyLoss(mode='binary', beta=0.7, smooth=1e-6)\n","BCELoss = smp.losses.SoftBCEWithLogitsLoss()\n","DiceLoss = smp.losses.DiceLoss(mode=\"binary\", smooth=1e-6)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=CFG.LR)\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=4)\n","\n","def criterion(pred,target):\n","    return TverskyLoss(pred, target)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T04:31:55.187800Z","iopub.status.busy":"2023-05-20T04:31:55.187425Z","iopub.status.idle":"2023-05-20T04:31:55.638302Z","shell.execute_reply":"2023-05-20T04:31:55.637126Z","shell.execute_reply.started":"2023-05-20T04:31:55.187763Z"},"trusted":true},"outputs":[],"source":["gt_mask = torch.from_numpy(np.asarray(FRAGMENTS_ZARR[CFG.TEST_FRAGMENT][\"truth\"])).float().cuda()\n","gt_shape = FRAGMENTS_SHAPE[CFG.TEST_FRAGMENT]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-20T04:31:55.640175Z","iopub.status.busy":"2023-05-20T04:31:55.639774Z","iopub.status.idle":"2023-05-20T05:40:02.825430Z","shell.execute_reply":"2023-05-20T05:40:02.824131Z","shell.execute_reply.started":"2023-05-20T04:31:55.640135Z"},"trusted":true},"outputs":[],"source":["for epoch in range(1, CFG.EPOCHS+1):\n","    model.train()\n","    cur_lr = optimizer.param_groups[0]['lr']\n","    pbar_train = enumerate(dataloader_train)\n","    pbar_train = tqdm(pbar_train, total=n_train, bar_format=\"{l_bar}{bar:10}{r_bar}{bar:-10b}\")\n","    mloss_train, mloss_val, val_metric = 0.0, 0.0, 0.0\n","\n","    for i, batches in pbar_train:\n","        fragments = batches[\"images\"]\n","        labels = batches[\"labels\"]\n","        fragments, labels = fragments.cuda(), labels.cuda()\n","        optimizer.zero_grad()\n","        with amp.autocast():\n","            pred_masks = model(fragments)\n","            loss = criterion(pred_masks, labels)\n","            scaler.scale(loss).backward()\n","            scaler.step(optimizer)\n","            scaler.update()\n","            mloss_train += loss.detach().item()\n","\n","        gpu_mem = f\"Mem : {torch.cuda.memory_reserved() / 1E9:.3g}GB\"\n","        pbar_train.set_description((\"%10s  \" * 3 + \"%10s\") % (f\"Epoch {epoch}/{CFG.EPOCHS}\", gpu_mem, cur_lr,\n","                                                              f\"Loss: {mloss_train / (i + 1):.4f}\"))    \n","    model.eval()\n","    pbar_val = enumerate(dataloader_valid)\n","    pbar_val = tqdm(pbar_val, total=n_valid, bar_format=\"{l_bar}{bar:10}{r_bar}{bar:-10b}\")\n","    final_pred_mask = torch.zeros(gt_shape, dtype=torch.float32, device='cuda')\n","    \n","    for i, batches in pbar_val:\n","        fragments = batches[\"images\"]\n","        labels = batches[\"labels\"]\n","        xys = batches[\"locations\"]\n","        fragments, labels = fragments.cuda(), labels.cuda()\n","        with torch.no_grad():\n","            pred_masks = model(fragments)\n","            mloss_val += criterion(pred_masks, labels).item()\n","            pred_masks = torch.sigmoid(pred_masks)\n","        \n","        for j, xy in enumerate(xys):\n","            final_pred_mask[xy[1]:xy[3], xy[0]:xy[2]] = pred_masks[j, 0]\n","\n","        pbar_val.set_description((\"%10s\") % (f\"Val Loss: {mloss_val / (i+1):.4f}\"))\n","    \n","    model.train()\n","    scheduler.step(metrics=mloss_val)\n","    \n","    for threshold in np.arange(0.2, 0.65, 0.05):\n","        fbeta = fbeta_score(final_pred_mask, gt_mask, threshold)\n","        print(f\"Threshold : {threshold:.2f}\\tFBeta : {fbeta:.6f}\")\n","    \n","    if epoch % 10 == 0:\n","        torch.save(model.module.state_dict(), f\"./ckpts/resnet{CFG.MODEL_DEPTH}_val_{CFG.TEST_FRAGMENT}_3d_seg_epoch_{epoch}.pt\")\n","\n","    if epoch == 30:\n","        break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat":4,"nbformat_minor":4}
