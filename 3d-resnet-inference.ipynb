{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Keras starter kit [full training set, UNet]"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# import sys\n","\n","# sys.path.append('/kaggle/input/pretrainedmodels/pretrainedmodels-0.7.4')\n","# sys.path.append('/kaggle/input/segmentation-models-pytorch/segmentation_models.pytorch-master')\n","# sys.path.append('/kaggle/input/efficientnet-pytorch/EfficientNet-PyTorch-master')\n","# sys.path.append('/kaggle/input/timm-pytorch-image-models/pytorch-image-models-master')\n","# sys.path.append(\"/kaggle/input/einops/einops-master\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Setup"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-05-10T15:03:39.741181Z","iopub.status.busy":"2023-05-10T15:03:39.740765Z","iopub.status.idle":"2023-05-10T15:03:39.750574Z","shell.execute_reply":"2023-05-10T15:03:39.749341Z","shell.execute_reply.started":"2023-05-10T15:03:39.741144Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Global seed set to 42\n"]}],"source":["\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import wandb\n","import torchvision\n","import datetime\n","import imageio\n","# import cupy\n","from sklearn.model_selection import KFold\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","import pytorch_lightning\n","import segmentation_models_pytorch as smp\n","import pytorch_lightning as pl\n","import pytorch_lightning.callbacks.model_checkpoint\n","import pytorch_lightning.plugins\n","from skimage.transform import resize as resize_ski\n","from pytorch_lightning.strategies.ddp import DDPStrategy\n","from pytorch_lightning.loggers import WandbLogger\n","from einops import rearrange, reduce, repeat\n","import torch.nn.functional as F\n","import segmentation_models_pytorch as smp\n","from segmentation_models_pytorch.decoders.unet.decoder import UnetDecoder, DecoderBlock\n","from timm.models.resnet import resnet10t, resnet34d, resnet50d, resnet14t, seresnext26d_32x4d, seresnext50_32x4d\n","from timm.models.mvitv2 import mvitv2_base\n","import os\n","import torch.utils.data\n","from dataclasses import dataclass\n","\n","from scipy.ndimage import distance_transform_edt\n","\n","import ssl\n","ssl._create_default_https_context = ssl._create_unverified_context\n","\n","import glob\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import cv2\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data\n","import os,cv2\n","import gc\n","import sys\n","import matplotlib.patches as patches\n","import random\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import pytorch_lightning as pl\n","\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.cuda import amp\n","from torch.utils.data import Dataset, DataLoader\n","import segmentation_models_pytorch as smp\n","import albumentations as A\n","from IPython.display import Video\n","\n","# sys.path.append(\"/home/fummicc1/codes/competitions/kaggle-ink-detection\")\n","# sys.path.append(\"/kaggle/input/resnet3d\")\n","from resnet import generate_model\n","# from resnext import generate_model\n","\n","pytorch_lightning.seed_everything(seed=42)\n","torch.set_float32_matmul_precision('high')\n","\n","\n","@dataclass\n","class CFG():\n","    # Data config\n","    # DATA_DIR = '/kaggle/input/vesuvius-challenge-ink-detection/'\n","    DATA_DIR = '/home/fummicc1/codes/competitions/kaggle-ink-detection'\n","    # DATA_DIR = '/home/fummicc1/codes/Kaggle/kaggle-ink-detection'\n","    BUFFER = 192 # Half-size of papyrus patches we'll use as model inputs\n","    CROP_SIZE = BUFFER * 2\n","    STRIDE = 192\n","    # Z_LIST = list(range(0, 20, 5)) + list(range(22, 34))  # Offset of slices in the z direction\n","    # Z_LIST = [20, 22, 24, 26] + list(range(28, 36)) + [36, 38]\n","    # Z_LIST = list(range(16, 48, 2))\n","    Z_LIST = list(range(16, 48, 2))\n","    # Z_LIST = list(range(0, 24, 8)) + list(range(24, 36, 2)) + list(range(36, 64, 10))\n","    Z_DIM = len(Z_LIST)  # Number of slices in the z direction. Max value is 64 - Z_START\n","    BATCH_Z_DIFF = None\n","    SHARED_HEIGHT = 4032  # Max height to resize all papyrii\n","\n","    # Model config\n","    BATCH_SIZE = 32\n","    \n","    GPU_COUNT = 4\n","\n","    device = torch.device(\"cuda\")\n","    threshold = 0.55\n","    num_workers = 8\n","    exp = 1e-7\n","    mask_padding = SHARED_HEIGHT // 40\n","\n","    num_epochs = 30\n","    lr = 1e-3\n","    lr_scheduler_name = \"CosineAnnealingLR\"\n","    eta_min_lr = 1e-5\n","    WANDB_NOTE = \"新ベースライン\"\n","    augmentation_names = [\n","        \"Horizontal\",\n","        \"Vertical\",\n","        \"RandomScale\",\n","        \"Transpose\",\n","        \"RandomRotate\",\n","        \"ShiftScaleRotate\",\n","        # \"Blur\",\n","        \"GridDistortion\"\n","        \"MultiChannelNoise\",\n","        \"PadIfNeeded\",\n","        \"Resize\",   \n","    ]\n","    loss1_alpha = 1.75\n","    loss1_beta = None\n","    \n","    loss2a_alpha = 1.5\n","    loss2a_beta = None\n","    \n","    loss2b_alpha = 1.5\n","    \n","    loss1_weight = 0.5\n","    loss2_weight = 0.5\n","    \n","    loss2a_weight = 0.5\n","    loss2b_weight = 0.5\n","    \n","    lr_scheduler = optim.lr_scheduler.CosineAnnealingLR\n","    optimizer = \"AdamW\"\n","    # loss1 = smp.losses.TverskyLoss(\n","    #     smp.losses.BINARY_MODE,\n","    #     log_loss=False,\n","    #     from_logits=True, \n","    #     smooth=1e-7,\n","    #     alpha=loss1_alpha,\n","    #     beta=loss1_beta,\n","    # )\n","    loss1 = nn.BCEWithLogitsLoss(\n","        pos_weight=torch.tensor([loss1_alpha]),\n","    )\n","    # loss2a = smp.losses.TverskyLoss(\n","    #     smp.losses.BINARY_MODE,\n","    #     log_loss=False,\n","    #     from_logits=True, \n","    #     smooth=1e-7,\n","    #     alpha=loss2a_alpha,\n","    #     beta=loss2a_beta,\n","    # )\n","    loss2a = nn.BCEWithLogitsLoss(\n","        pos_weight=torch.tensor([loss2a_alpha]),\n","    )\n","    loss2b = nn.BCEWithLogitsLoss(\n","        pos_weight=torch.tensor([loss2b_alpha]),\n","    )\n","    noise_intensity = (0.000025, 0.00005)\n","    use_new_label_mask = True\n","    pretrained = False\n","    # MODEL_DEPTH = 50\n","    # MODEL_KIND = \"KMS\"\n","    MODEL_DEPTH = 34\n","    MODEL_KIND = \"KM\"\n","    \n","    exp_name = \"070-gpu17\"\n","    prev_exp_name = \"\"\n","    \n","def class2dict(c):\n","    return {attr: getattr(c, attr) for attr in dir(c) if not callable(getattr(c, attr)) and not attr.startswith(\"__\")}"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import cupy as cp\n","xp = cp\n","\n","delta_lookup = {\n","    \"xx\": xp.array([[1, -2, 1]], dtype=float),\n","    \"yy\": xp.array([[1], [-2], [1]], dtype=float),\n","    \"xy\": xp.array([[1, -1], [-1, 1]], dtype=float),\n","}\n","\n","def operate_derivative(img_shape, pair):\n","    assert len(img_shape) == 2\n","    delta = delta_lookup[pair]\n","    fft = xp.fft.fftn(delta, img_shape)\n","    return fft * xp.conj(fft)\n","\n","def soft_threshold(vector, threshold):\n","    return xp.sign(vector) * xp.maximum(xp.abs(vector) - threshold, 0)\n","\n","def back_diff(input_image, dim):\n","    assert dim in (0, 1)\n","    r, n = xp.shape(input_image)\n","    size = xp.array((r, n))\n","    position = xp.zeros(2, dtype=int)\n","    temp1 = xp.zeros((r+1, n+1), dtype=float)\n","    temp2 = xp.zeros((r+1, n+1), dtype=float)\n","    \n","    temp1[position[0]:size[0], position[1]:size[1]] = input_image\n","    temp2[position[0]:size[0], position[1]:size[1]] = input_image\n","    \n","    size[dim] += 1\n","    position[dim] += 1\n","    temp2[position[0]:size[0], position[1]:size[1]] = input_image\n","    temp1 -= temp2\n","    size[dim] -= 1\n","    return temp1[0:size[0], 0:size[1]]\n","\n","\n","def forward_diff(input_image, dim):\n","    assert dim in (0, 1)\n","    r, n = xp.shape(input_image)\n","    size = xp.array((r, n))\n","    position = xp.zeros(2, dtype=int)\n","    temp1 = xp.zeros((r+1, n+1), dtype=float)\n","    temp2 = xp.zeros((r+1, n+1), dtype=float)\n","        \n","    size[dim] += 1\n","    position[dim] += 1\n","\n","    temp1[position[0]:size[0], position[1]:size[1]] = input_image\n","    temp2[position[0]:size[0], position[1]:size[1]] = input_image\n","    \n","    size[dim] -= 1\n","    temp2[0:size[0], 0:size[1]] = input_image\n","    temp1 -= temp2\n","    size[dim] += 1\n","    return -temp1[position[0]:size[0], position[1]:size[1]]\n","\n","def iter_deriv(input_image, b, scale, mu, dim1, dim2):\n","    g = back_diff(forward_diff(input_image, dim1), dim2)\n","    d = soft_threshold(g + b, 1 / mu)\n","    b = b + (g - d)\n","    L = scale * back_diff(forward_diff(d - b, dim2), dim1)\n","    return L, b\n","\n","def iter_xx(*args):\n","    return iter_deriv(*args, dim1=1, dim2=1)\n","\n","def iter_yy(*args):\n","    return iter_deriv(*args, dim1=0, dim2=0)\n","\n","def iter_xy(*args):\n","    return iter_deriv(*args, dim1=0, dim2=1)\n","\n","def iter_sparse(input_image, bsparse, scale, mu):\n","    d = soft_threshold(input_image + bsparse, 1 / mu)\n","    bsparse = bsparse + (input_image - d)\n","    Lsparse = scale * (d - bsparse)\n","    return Lsparse, bsparse\n","\n","def denoise_image(input_image, iter_num=100, fidelity=150, sparsity_scale=10, continuity_scale=0.5, mu=1):\n","    image_size = xp.shape(input_image)\n","    #print(\"Initialize denoising\")\n","    norm_array = (\n","        operate_derivative(image_size, \"xx\") + \n","        operate_derivative(image_size, \"yy\") + \n","        2 * operate_derivative(image_size, \"xy\")\n","    )\n","    norm_array += (fidelity / mu) + sparsity_scale ** 2\n","    b_arrays = {\n","        \"xx\": xp.zeros(image_size, dtype=float),\n","        \"yy\": xp.zeros(image_size, dtype=float),\n","        \"xy\": xp.zeros(image_size, dtype=float),\n","        \"L1\": xp.zeros(image_size, dtype=float),\n","    }\n","    g_update = xp.multiply(fidelity / mu, input_image)\n","    for i in tqdm(range(iter_num), total=iter_num):\n","        #print(f\"Starting iteration {i+1}\")\n","        g_update = xp.fft.fftn(g_update)\n","        if i == 0:\n","            g = xp.fft.ifftn(g_update / (fidelity / mu)).real\n","        else:\n","            g = xp.fft.ifftn(xp.divide(g_update, norm_array)).real\n","        g_update = xp.multiply((fidelity / mu), input_image)\n","        \n","        #print(\"XX update\")\n","        L, b_arrays[\"xx\"] = iter_xx(g, b_arrays[\"xx\"], continuity_scale, mu)\n","        g_update += L\n","        \n","        #print(\"YY update\")\n","        L, b_arrays[\"yy\"] = iter_yy(g, b_arrays[\"yy\"], continuity_scale, mu)\n","        g_update += L\n","        \n","        #print(\"XY update\")\n","        L, b_arrays[\"xy\"] = iter_xy(g, b_arrays[\"xy\"], 2 * continuity_scale, mu)\n","        g_update += L\n","        \n","        #print(\"L1 update\")\n","        L, b_arrays[\"L1\"] = iter_sparse(g, b_arrays[\"L1\"], sparsity_scale, mu)\n","        g_update += L\n","        \n","    g_update = xp.fft.fftn(g_update)\n","    g = xp.fft.ifftn(xp.divide(g_update, norm_array)).real\n","    \n","    g[g < 0] = 0\n","    g -= g.min()\n","    g /= g.max()\n","    return g"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def is_in_masked_zone(location, mask):\n","    return mask[location[0], location[1]] > 0\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def resize(img):\n","    current_height, current_width = img.shape    \n","    aspect_ratio = current_width / current_height\n","    if CFG.SHARED_HEIGHT is None:\n","        return img\n","    # new_height = CFG.SHARED_HEIGHT\n","    # pad_y = new_height - current_height\n","    # if pad_y > 0:\n","    #     # 元画像が小さい場合は解像度を大きくしないでpaddingをつける\n","    #     img = np.pad(img, [(0, pad_y), (0, 0)], constant_values=0)\n","    # else:\n","    # 既に十分でかい場合はリサイズする\n","    # 本当はpaddingしたいけど、メモリサイズが大きくなる\n","    if CFG.SHARED_HEIGHT > current_height:\n","        # 既に小さい場合はリサイズしない\n","        return img    \n","    new_height = CFG.SHARED_HEIGHT\n","    new_width = int(CFG.SHARED_HEIGHT * aspect_ratio)\n","    new_size = (new_width, new_height)\n","    # (W, H)の順で渡すが結果は(H, W)になっている\n","    img = cv2.resize(img, new_size)\n","    return img\n","\n","def load_mask(split, index):     \n","    img = cv2.imread(f\"{CFG.DATA_DIR}/{split}/{index}/mask.png\", 0) // 255\n","    img = resize(img)\n","    return img\n","\n","\n","def load_labels(split, index):    \n","    suffix = \"_new\" if CFG.use_new_label_mask else \"\"\n","    img = cv2.imread(f\"{CFG.DATA_DIR}/{split}/{index}/inklabels{suffix}.png\", 0) // 255    \n","    img = resize(img)\n","    return img"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def extract_subvolume(location, volume):\n","    global printed\n","    y = location[0]\n","    x = location[1]\n","    subvolume = volume[y-CFG.BUFFER:y+CFG.BUFFER, x-CFG.BUFFER:x+CFG.BUFFER, :].astype(np.float32)\n","    \n","    return subvolume"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def load_volume(split, index):    \n","    # Load the 3d x-ray scan, one slice at a time\n","    all = sorted(glob.glob(f\"{CFG.DATA_DIR}/{split}/{index}/surface_volume/*.tif\"))\n","    z_slices_fnames = [all[i] for i in range(len(all)) if i in CFG.Z_LIST]\n","    assert len(z_slices_fnames) == CFG.Z_DIM\n","    z_slices = []\n","    for z, filename in  tqdm(enumerate(z_slices_fnames)):\n","        img = cv2.imread(filename, -1)        \n","        img = resize(img)\n","        # img = (img / (2 ** 8)).astype(np.uint8)\n","        img = img.astype(np.float32) / 255\n","        z_slices.append(img)\n","    return np.stack(z_slices, axis=-1)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def generate_locations_ds(volume, mask, label=None, skip_zero=False):\n","    is_in_mask_train = lambda x: is_in_masked_zone(x, mask)\n","\n","    # Create a list to store train locations\n","    locations = []\n","\n","    # Generate train locations\n","    volume_height, volume_width = volume.shape[:-1]\n","\n","    for y in range(CFG.BUFFER, volume_height - CFG.BUFFER, CFG.STRIDE):\n","        for x in range(CFG.BUFFER, volume_width - CFG.BUFFER, CFG.STRIDE):\n","            if skip_zero and label is not None and np.all(label[y - CFG.BUFFER : y + CFG.BUFFER, x - CFG.BUFFER : x + CFG.BUFFER] == 0):\n","                # print(f\"skip location at (y: {y}, x: {x})\")\n","                continue\n","            if is_in_mask_train((y, x)):\n","                locations.append((y, x))\n","\n","    # Convert the list of train locations to a PyTorch tensor\n","    locations_ds = np.stack(locations, axis=0)\n","    return locations_ds"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["import numpy as np\n","from albumentations.core.transforms_interface import ImageOnlyTransform\n","\n","class MultichannelNoise(ImageOnlyTransform):\n","\n","    def __init__(self, intensity=CFG.noise_intensity, always_apply=False, p=0.5):\n","        super().__init__(always_apply, p)\n","        self.intensity = intensity\n","\n","    def apply(self, img, **params):\n","        intensity = np.random.uniform(*self.intensity)\n","        noise = np.random.normal(loc=0, scale=intensity, size=img.shape)\n","        # print(\"img\", img)\n","        img = img + noise        \n","        return np.clip(img, 0, 1).astype(np.float32) # クリップして0から255の範囲に保つ\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.preprocessing import OneHotEncoder\n","\n","from albumentations.core.transforms_interface import ImageOnlyTransform\n","\n","class SubvolumeDataset(Dataset):\n","    def __init__(self, locations, volume, labels, buffer, is_train: bool, return_location: bool = False):\n","        self.locations = locations\n","        self.volume = volume\n","        self.labels = labels        \n","        self.buffer = buffer\n","        self.is_train = is_train\n","        self.return_location = return_location\n","\n","    def __len__(self):\n","        return len(self.locations)\n","\n","    def __getitem__(self, idx):\n","        label = None\n","        location = np.array(self.locations[idx])\n","        y, x = location[0], location[1]\n","\n","        subvolume = extract_subvolume(location, self.volume)\n","        \n","        if self.labels is not None:\n","            label = self.labels[y - self.buffer:y + self.buffer, x - self.buffer:x + self.buffer]            \n","            label = np.stack([label], axis=-1)            \n","            \n","        # 段々meanは小さくなる\n","        mean = np.array([0.42 for i in range(0, CFG.Z_DIM)]).reshape(-1, 1, 1)\n","        # 段々stdは小さくなる\n","        std = np.array([0.28 for i in range(0, CFG.Z_DIM)]).reshape(-1, 1, 1)\n","        \n","        if self.is_train and label is not None:    \n","            # label = smooth_labels(label, alpha=0.1)                  \n","            transformed = A.Compose([\n","                A.ToFloat(max_value=255),\n","                A.HorizontalFlip(p=0.5) if \"Horizontal\" in CFG.augmentation_names else A.Compose([]),\n","                A.VerticalFlip(p=0.5) if \"Vertical\" in CFG.augmentation_names else A.Compose([]),  \n","                A.RandomScale(p=0.4, scale_limit=0.4) if \"RandomScale\" in CFG.augmentation_names else A.Compose([]),\n","                A.Transpose(p=0.5) if \"Transpose\" in CFG.augmentation_names else A.Compose([]), \n","                A.RandomRotate90(p=0.5,) if \"RandomRotate\" in CFG.augmentation_names else A.Compose([]), \n","                A.ShiftScaleRotate(p=0.7, scale_limit=0.4, rotate_limit=80) if \"ShiftScaleRotate\" in CFG.augmentation_names else A.Compose([]),\n","                A.MotionBlur(p=0.2) if \"Blur\" in CFG.augmentation_names else A.Compose([]),\n","                A.GridDistortion(num_steps=5, distort_limit=0.15, p=0.2) if \"GridDistortion\" in CFG.augmentation_names else A.Compose([]),                \n","                MultichannelNoise(\n","                    p=0.2,\n","                ) if \"MultichannelNoise\" in CFG.augmentation_names else A.Compose([]),\n","                A.PadIfNeeded(min_height=self.buffer * 2, min_width=self.buffer * 2) if \"PadIfNeeded\" in CFG.augmentation_names else A.Compose([]),\n","                A.Resize(height=self.buffer * 2, width=self.buffer * 2) if \"Resize\" in CFG.augmentation_names else A.Compose([]),\n","            ])(image=subvolume, mask=label)            \n","            subvolume = transformed[\"image\"]\n","            label = transformed[\"mask\"]\n","            subvolume = np.transpose(subvolume, (2, 0, 1))\n","            label = np.transpose(label, (2, 0, 1))\n","            subvolume = (subvolume - mean) / std       \n","        else:\n","            if label is None:\n","                subvolume = np.transpose(subvolume, (2, 0, 1))\n","                subvolume /= 255.\n","                subvolume = (subvolume - mean) / std\n","            else:\n","                # print(\"subvolume in val dataset (before aug)\", subvolume, file=open(\"before-val-aug.log\", \"w\")) \n","                subvolume = np.transpose(subvolume, (2, 0, 1))\n","                label = np.transpose(label, (2, 0, 1))\n","                subvolume /= 255.\n","                subvolume = (subvolume - mean) / std\n","        # print(\"subvolume\", subvolume)\n","        if self.return_location:\n","            return subvolume, location\n","        return subvolume, label        "]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["class SmpUnetDecoder(nn.Module):\n","\tdef __init__(\n","\t\tself,\n","\t\tin_channel,\n","\t\tskip_channel,\n","\t\tout_channel,\n","\t):\n","\t\tsuper().__init__()\n","\t\tself.center = nn.Identity()\n","\n","\t\ti_channel = [\n","\t\t\tin_channel,\n","\t\t] + out_channel[:-1]\n","\t\ts_channel = skip_channel\n","\t\to_channel = out_channel\n","\t\tblock = [\n","\t\t\tDecoderBlock(i, s, o, use_batchnorm=True, attention_type=None)\n","\t\t\tfor i, s, o in zip(i_channel, s_channel, o_channel)\n","\t\t]\n","\t\tself.block = nn.ModuleList(block)\n","\n","\tdef forward(self, feature, skip):\n","\t\td = self.center(feature)\n","\t\tdecode = []\n","\t\tfor i, block in enumerate(self.block):\n","\t\t\ts = skip[i]\n","\t\t\td = block(d, s)\n","\t\t\tdecode.append(d)\n","\n","\t\tlast = d\n","\t\treturn last, decode"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["class PoolingAttention(nn.Module):\n","    def __init__(self, in_channels):\n","        super().__init__()\n","        self.conv = nn.Conv3d(in_channels, 1, kernel_size=1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        batch, _, _, _, _ = x.size()\n","        attention_map = self.sigmoid(self.conv(x))\n","        return attention_map\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, encoder_dims, upscale):\n","        super().__init__()\n","        self.convs = nn.ModuleList([\n","            nn.Sequential(\n","                nn.Conv2d(encoder_dims[i]+encoder_dims[i-1], encoder_dims[i-1], 3, 1, 1, bias=False),\n","                nn.BatchNorm2d(encoder_dims[i-1]),\n","                nn.ReLU(inplace=True)\n","            ) for i in range(1, len(encoder_dims))])\n","\n","        self.logit = nn.Conv2d(encoder_dims[0], 1, 1, 1, 0)\n","        self.up = nn.Upsample(scale_factor=upscale, mode=\"bilinear\")\n","\n","    def forward(self, feature_maps):\n","        for i in range(len(feature_maps)-1, 0, -1):\n","            # print(\"index:\", i, feature_maps[i].shape)\n","            f_up = F.interpolate(feature_maps[i], scale_factor=2, mode=\"bilinear\")\n","            f = torch.cat([feature_maps[i-1], f_up], dim=1)\n","            f_down = self.convs[i-1](f)\n","            feature_maps[i-1] = f_down\n","\n","        x = self.logit(feature_maps[0])\n","        mask = self.up(x)\n","        return mask"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["class SegModel(nn.Module):\n","\tdef __init__(self,model_depth=CFG.MODEL_DEPTH):\n","\t\tsuper().__init__()\n","\t\tself.encoder = generate_model(model_depth=model_depth, n_input_channels=1)\n","\t\t# self.decoder = Decoder(encoder_dims=[128, 256, 512, 1024], upscale=4)\n","\t\tself.decoder = Decoder(encoder_dims=[64, 64, 128, 256, 512], upscale=2)\n","\t\t\n","\tdef forward(self, x):\n","\t\tif x.ndim==4:\n","\t\t\tx=x[:,None]\n","\t\t\t\n","\t\tencoder = []\n","\t\t# print(\"start encoder\")\n","\t\te = self.encoder\n","\t\tx = e.conv1(x)\n","\t\tx = e.bn1(x)\n","\t\tx = e.relu(x)\n","\t\tencoder.append(x)\n","\t\t# print(\"start encoder layet1\")\n","\t\tx = F.avg_pool3d(x, kernel_size=(1,2,2), stride=(1,2,2))\n","\t\tx = e.layer1(x)\n","\t\tencoder.append(x)\n","\t\t# print(\"start encoder layet2\")\n","\t\tx = e.layer2(x)\n","\t\tencoder.append(x)\n","\t\t# print(\"start encoder layet3\")\n","\t\tx = e.layer3(x)\n","\t\tencoder.append(x)\n","\t\t# print(\"start encoder layet4\")\n","\t\tx = e.layer4(x)\n","\t\tencoder.append(x)\n","\t\t# print(\"end encoder\")\n","\t\t\n","\t\tfeat_maps_pooled = [torch.mean(f, dim=2) for f in encoder]\n","\t\t# print(feat_maps_pooled[0].shape)\n","\t\tpred_mask = self.decoder(feat_maps_pooled)\n","\t\t# print(\"pred_mask\", pred_mask.shape)\n","\t\treturn pred_mask\n","\t\n","\tdef load_pretrained_weights(self, state_dict):\n","\t\t# Convert 3 channel weights to single channel\n","\t\t# ref - https://timm.fast.ai/models#Case-1:-When-the-number-of-input-channels-is-1\n","\t\tconv1_weight = state_dict['conv1.weight']\n","\t\tstate_dict['conv1.weight'] = conv1_weight.sum(dim=1, keepdim=True)\n","\t\tprint(self.encoder.load_state_dict(state_dict, strict=False))"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.encoder=SegModel()\n","        weight_path = f\"r3d{CFG.MODEL_DEPTH}_{CFG.MODEL_KIND}_200ep.pt\"\n","        if CFG.pretrained:\n","            self.encoder.load_pretrained_weights(torch.load(weight_path)[\"state_dict\"])\n","\n","    def forward(self, images:torch.Tensor):\n","        #image.shape=(b,C,H,W)\n","        if images.ndim==4:\n","            images=images[:,None]\n","        # images=normalization(images)\n","        output = self.encoder(images)\n","        return output"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["tc = torch\n","def TTA(x:tc.Tensor,model:nn.Module):\n","    #x.shape=(batch,c,h,w)\n","    shape=x.shape\n","    x=[x,*[tc.rot90(x,k=i,dims=(-2,-1)) for i in range(1,4)]]\n","    x=tc.cat(x,dim=0)\n","    _, x = model(x)\n","    x=x.reshape(4,shape[0], 1 ,*shape[2:])\n","    x=[tc.rot90(x[i],k=-i,dims=(-2,-1)) for i in range(4)]\n","    x=tc.stack(x,dim=0)\n","    return x.mean(0)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["# ref - https://www.kaggle.com/competitions/vesuvius-challenge-ink-detection/discussion/397288\n","def fbeta_score(preds, targets, threshold, beta=0.5, smooth=1e-5):\n","    preds_t = torch.where(preds > threshold, 1.0, 0.0).float()\n","    y_true_count = targets.sum()\n","    \n","    ctp = preds_t[targets==1].sum()\n","    cfp = preds_t[targets==0].sum()\n","    beta_squared = beta * beta\n","\n","    c_precision = ctp / (ctp + cfp + smooth)\n","    c_recall = ctp / (y_true_count + smooth)\n","    dice = (1 + beta_squared) * (c_precision * c_recall) / (beta_squared * c_precision + c_recall + smooth)\n","\n","    return dice"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["\n","\n","class Model(pl.LightningModule):\n","    training_step_outputs = []\n","    validation_step_outputs = []\n","    test_step_outputs = [[], []]\n","\n","    def __init__(self, **kwargs):\n","        super().__init__()\n","\n","        self.model = Net()        \n","\n","        self.loss1 = CFG.loss1\n","        self.loss2 = CFG.loss2a\n","        self.loss3 = CFG.loss2b\n","\n","    def forward(self, image, stage):\n","        if stage != \"train\":\n","            # mask = TTA(image, self.model)\n","            mask = self.model(image)\n","        else:\n","            mask = self.model(image)\n","        return mask\n","\n","    def shared_step(self, batch, stage):\n","        subvolumes, labels = batch\n","\n","        image, labels = subvolumes.float(), labels.float()        \n","        assert image.ndim == 4\n","        \n","        h, w = image.shape[2:]\n","        assert h % 32 == 0 and w % 32 == 0\n","        \n","        # print(\"labels\", labels.max(), labels.min())\n","\n","        assert labels.max() <= 1.0 and labels.min() >= 0\n","\n","        if stage == \"train\":\n","            logit2 = self.forward(image, stage)\n","            # loss = self.loss1(logit1, labels) * CFG.loss1_weight + CFG.loss2_weight * (self.loss2(logit2, labels) * CFG.loss2a_weight + self.loss3(logit2, labels) * CFG.loss2b_weight)\n","            loss = (self.loss2(logit2, labels) * CFG.loss2a_weight + self.loss3(logit2, labels) * CFG.loss2b_weight)\n","            logit = logit2\n","        else:\n","            logit = self.forward(image, stage)\n","            loss = self.loss2(logit, labels) * CFG.loss2a_weight + self.loss3(logit, labels) * CFG.loss2b_weight\n","        \n","        prob2 = torch.sigmoid(logit)\n","\n","        pred_mask = (prob2 > CFG.threshold).float()\n","        \n","        # print(\"pred_mask\", pred_mask)\n","        \n","        score = fbeta_score(pred_mask, labels, threshold=CFG.threshold)\n","\n","        tp, fp, fn, tn = smp.metrics.get_stats(\n","            pred_mask.long(), labels.long(), mode=\"binary\"\n","        )\n","        # cur_lr = self.lr_schedulers().get_last_lr()\n","\n","        return {\n","            \"loss\": loss,\n","            \"tp\": tp,\n","            \"fp\": fp,\n","            \"fn\": fn,\n","            \"tn\": tn,\n","            \"score\": score,\n","            # \"lr\": cur_lr,\n","        }\n","\n","    def shared_epoch_end(self, outputs, stage):\n","        # aggregate step metics\n","        tp = torch.cat([x[\"tp\"] for x in outputs])\n","        fp = torch.cat([x[\"fp\"] for x in outputs])\n","        fn = torch.cat([x[\"fn\"] for x in outputs])\n","        tn = torch.cat([x[\"tn\"] for x in outputs])\n","        # lr = outputs[0][\"lr\"]\n","        loss = torch.mean(torch.Tensor([x[\"loss\"] for x in outputs]))\n","        fbeta_score = torch.mean(torch.Tensor([x[\"score\"] for x in outputs]))\n","\n","        per_image_iou = smp.metrics.iou_score(\n","            tp, fp, fn, tn, reduction=\"micro-imagewise\"\n","        )\n","\n","        dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n","\n","        metrics = {\n","            f\"{stage}_per_image_iou\": per_image_iou,\n","            f\"{stage}_dataset_iou\": dataset_iou,\n","            f\"{stage}_loss\": 10000 if loss.item() == 0 else loss.item(),\n","            f\"{stage}_tp\": tp.sum().int().item(),\n","            f\"{stage}_fp\": fp.sum().int().item(),\n","            f\"{stage}_fn\": fn.sum().int().item(),\n","            f\"{stage}_tn\": tn.sum().int().item(),\n","            f\"{stage}_score\": fbeta_score.item(),\n","            # \"lr\": lr,\n","        }\n","\n","        self.log_dict(metrics, prog_bar=True, sync_dist=True)\n","\n","    def training_step(self, batch, batch_idx):\n","        out = self.shared_step(batch, \"train\")\n","        self.training_step_outputs.append(out)\n","        return out\n","\n","    def on_train_epoch_end(self):\n","        out = self.shared_epoch_end(self.training_step_outputs, \"train\")\n","        self.training_step_outputs.clear()\n","        return out\n","\n","    def validation_step(self, batch, batch_idx):\n","        out = self.shared_step(batch, \"valid\")\n","        self.validation_step_outputs.append(out)\n","        return out\n","\n","    def on_validation_epoch_end(self):\n","        out = self.shared_epoch_end(self.validation_step_outputs, \"valid\")\n","        self.validation_step_outputs.clear()\n","        return out\n","\n","\n","    def test_step(self, batch, batch_idx):\n","        global predictions_map, predictions_map_counts\n","\n","        patch_batch, loc_batch = batch\n","\n","        loc_batch = loc_batch.long()\n","        patch_batch = patch_batch.float()\n","        predictions = self.forward(patch_batch, \"test\")\n","        predictions = predictions.sigmoid()\n"," \n","        predictions = torch.permute(predictions, (0, 2, 3, 1)).squeeze(dim=-1)\n","        predictions = (\n","            predictions.cpu().numpy()\n","        )\n","        loc_batch = loc_batch.cpu().numpy()\n","        \n","        self.test_step_outputs[0].extend(loc_batch)\n","        self.test_step_outputs[1].extend(predictions)        \n","        return loc_batch, predictions\n","\n","    def on_test_epoch_end(self):\n","        global predictions_map, predictions_map_counts\n","\n","        locs = np.array(self.test_step_outputs[0])\n","        preds = np.array(self.test_step_outputs[1])\n","        print(\"locs\", locs.shape)\n","        print(\"preds\", preds.shape)\n","\n","        new_predictions_map = np.zeros_like(predictions_map[:, :, 0])\n","        new_predictions_map_counts = np.zeros_like(predictions_map_counts[:, :, 0])\n","\n","        for (y, x), pred in zip(locs, preds):\n","            new_predictions_map[\n","                y - CFG.BUFFER : y + CFG.BUFFER, x - CFG.BUFFER : x + CFG.BUFFER\n","            ] += pred\n","            new_predictions_map_counts[\n","                y - CFG.BUFFER : y + CFG.BUFFER, x - CFG.BUFFER : x + CFG.BUFFER\n","            ] += 1\n","        \n","        new_predictions_map /= new_predictions_map_counts + CFG.exp\n","        new_predictions_map = new_predictions_map[:, :, np.newaxis]\n","        predictions_map = np.concatenate(\n","            [predictions_map, new_predictions_map], axis=-1\n","        )\n","\n","\n","    def configure_optimizers(self):\n","        # optimizer = optim.SGD(self.parameters(), lr=CFG.lr)\n","        optimizer = optim.AdamW(self.parameters(), lr=CFG.lr)\n","        \n","        if CFG.lr_scheduler_name == \"ReduceLROnPlateau\":\n","            scheduler = CFG.lr_scheduler(\n","                optimizer, mode=\"min\", factor=0.75, patience=5,\n","            )\n","            return {\n","                \"optimizer\": optimizer,\n","                \"lr_scheduler\": {\"scheduler\": scheduler, \"monitor\": \"valid_loss\"},\n","            }\n","        elif CFG.lr_scheduler_name == \"CosineAnnealingLR\":\n","            scheduler = CFG.lr_scheduler(\n","                optimizer, T_max=10, eta_min=CFG.eta_min_lr,\n","            )\n","            return {\n","                \"optimizer\": optimizer,\n","                \"lr_scheduler\": scheduler,\n","            }"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["class EnsembleModel:\n","    def __init__(self, test_loader, test_volume):\n","        super().__init__()\n","        self.test_loader = test_loader\n","        self.test_volume = test_volume\n","        self.list = []\n","        for fold in [1]:\n","            _model = Model.load_from_checkpoint(\n","                f\"weights/weights_fold-{fold}.ckpt\",               \n","            )\n","            trainer = pl.Trainer(\n","                accelerator=\"gpu\",\n","                devices=\"1\",\n","                enable_checkpointing=False,\n","            )\n","\n","            self.list.append((_model, trainer))\n","    \n","    def forward(self):\n","        global predictions_map, predictions_map_counts\n","        predictions_map = np.empty_like(self.test_volume[:, :, 0])[:, :, np.newaxis].astype(np.float64)\n","        predictions_map_counts = np.empty_like(predictions_map).astype(np.uint8)\n","        for i, (model, trainer) in enumerate(self.list):\n","            model: Model = model\n","            model.test_step_outputs = [[], []]\n","            model.eval()\n","            trainer.test(\n","                model=model,\n","                dataloaders=self.test_loader,\n","                verbose=True,\n","            )\n","            self.list[i] = None\n","            gc.collect()\n","        predictions_map = predictions_map[:, :, 1:].mean(axis=-1, keepdims=True)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","from tqdm import tqdm\n","from skimage.transform import resize as resize_ski\n","import pathlib\n","import gc\n","\n","def compute_predictions_map(split, index):\n","    global predictions_map\n","    global predictions_map_counts\n","    \n","    print(f\"Load data for {split}/{index}\")\n","\n","    test_volume = load_volume(split=split, index=index)\n","    test_mask = load_mask(split=split, index=index)    \n","    \n","    #get coord\n","    stride = CFG.BUFFER\n","    H,W,D  = test_volume.shape\n","\n","    ##pad #assume H,W >size\n","    px, py = W % stride, H % stride\n","    if (px != 0) or (py != 0):\n","        px = stride - px\n","        py = stride - py\n","        test_volume = np.pad(test_volume, [(0, py), (0, px), (0, 0)], constant_values=0)\n","        test_mask = np.pad(test_mask, [(0, py), (0, px)], constant_values=0)  \n","        \n","    test_locations = generate_locations_ds(test_volume, test_mask)  \n","    \n","    print(f\"{len(test_locations)} test locations (before filtering by mask)\")\n","\n","    # filter locations inside the mask\n","    test_locations = [loc for loc in test_locations if is_in_masked_zone(loc, test_mask)]\n","    \n","    print(f\"{len(test_locations)} test locations (after filtering by mask)\")\n","\n","    test_ds = SubvolumeDataset(test_locations, test_volume, None, CFG.BUFFER, is_train=False, return_location=True)\n","    test_loader = DataLoader(test_ds, batch_size=CFG.BATCH_SIZE, num_workers=CFG.num_workers, pin_memory=True)        \n","\n","    # # shape: (Y, X, C)\n","    predictions_map = np.zeros_like(test_volume[:, :, 0])[:, :, np.newaxis].astype(np.float64)\n","    predictions_map_counts = np.zeros_like(predictions_map).astype(np.uint8)\n","\n","    print(\"test_volume.shape\", test_volume.shape)\n","    print(\"predictions_map.shape\", predictions_map.shape)\n","\n","    print(f\"Compute predictions\")\n","    \n","    model = EnsembleModel(test_loader, test_volume)\n","    model.forward()\n","    del model\n","    del test_locations\n","    del test_loader\n","    del test_ds\n","    del test_volume\n","    del test_mask\n","    gc.collect()\n","    \n","    # print(\"predictions_map\", predictions_map, file=open(\"predictions_map\", \"w\"))\n","    return predictions_map[:H, :W, :]\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["# Fast run length encoding, from https://www.kaggle.com/code/hackerpoet/even-faster-run-length-encoder/script\n","# (H, W)の形式\n","def rle (img: np.ndarray, threshold: float):\n","    flat_img = img.flatten()\n","    flat_img = np.where(flat_img > threshold, 1, 0).astype(np.uint8)\n","\n","    starts = np.array((flat_img[:-1] == 0) & (flat_img[1:] == 1))\n","    ends = np.array((flat_img[:-1] == 1) & (flat_img[1:] == 0))\n","    starts_ix = np.where(starts)[0] + 2\n","    ends_ix = np.where(ends)[0] + 2\n","    lengths = ends_ix - starts_ix\n","\n","    return \" \".join(map(str, sum(zip(starts_ix, lengths), ())))"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["def update_submission(predictions_map, index):\n","    rle_ = rle(predictions_map, threshold=CFG.threshold)\n","    print(f\"{index},\" + rle_, file=open('submission.csv', 'a'))\n","    # print(f\"{index},\" + rle_, file=open('/kaggle/working/submission.csv', 'a'))"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Load data for test/a\n"]},{"name":"stderr","output_type":"stream","text":["16it [00:00, 25.17it/s]\n"]},{"name":"stdout","output_type":"stream","text":["269 test locations (before filtering by mask)\n","269 test locations (after filtering by mask)\n","test_volume.shape (2880, 6336, 16)\n","predictions_map.shape (2880, 6336, 1)\n","Compute predictions\n"]},{"name":"stderr","output_type":"stream","text":["/home/fummicc1/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/lightning_fabric/plugins/environments/slurm.py:166: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/fummicc1/anaconda3/envs/ink-detection-3_8/lib/ ...\n","  rank_zero_warn(\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","/home/fummicc1/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n","  warning_cache.warn(\n","/home/fummicc1/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/lightning_fabric/plugins/environments/slurm.py:166: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/fummicc1/anaconda3/envs/ink-detection-3_8/lib/ ...\n","  rank_zero_warn(\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"181b64c82b0c4771aa1a3c19494077b9","version_major":2,"version_minor":0},"text/plain":["Testing: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["locs (269, 2)\n","preds (269, 384, 384)\n","original size (2727, 6330)\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 250/250 [00:31<00:00,  7.83it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Load data for test/b\n"]},{"name":"stderr","output_type":"stream","text":["16it [00:01, 13.25it/s]\n"]},{"name":"stdout","output_type":"stream","text":["261 test locations (before filtering by mask)\n","261 test locations (after filtering by mask)\n","test_volume.shape (4224, 4800, 16)\n","predictions_map.shape (4224, 4800, 1)\n","Compute predictions\n"]},{"name":"stderr","output_type":"stream","text":["GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4e944b2ef2214bf990b4792d4488204a","version_major":2,"version_minor":0},"text/plain":["Testing: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["locs (261, 2)\n","preds (261, 384, 384)\n","original size (5454, 6330)\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 250/250 [01:05<00:00,  3.83it/s]\n"]}],"source":["predictions_map = None\n","predictions_map_counts = None\n","print(\"Id,Predicted\", file=open('submission.csv', 'w'))\n","kind = \"test\"\n","folder = pathlib.Path(CFG.DATA_DIR) / kind\n","for p in list(folder.iterdir()):\n","    index = p.stem\n","    predictions_map = compute_predictions_map(split=kind, index=index)\n","    original_size = cv2.imread(CFG.DATA_DIR + f\"/{kind}/{index}/mask.png\", 0).shape[:2]\n","    resized_predictions_map = resize_ski(predictions_map, (original_size[0], original_size[1], 1), anti_aliasing=True).squeeze(axis=-1)\n","    print(\"original size\", original_size)\n","    resized_predictions_map=xp.array(resized_predictions_map)\n","    resized_predictions_map=denoise_image(resized_predictions_map, iter_num=250)\n","    resized_predictions_map=resized_predictions_map.get()\n","    update_submission(resized_predictions_map, index)\n","    resized_predictions_map = np.where(resized_predictions_map >= CFG.threshold, 255, 0)\n","    plt.imsave(f\"{index}_{str(CFG.threshold)}_{str(CFG.BUFFER)}.png\", resized_predictions_map, cmap=\"gray\")\n","    resized_predicitions_map = None\n","    gc.collect()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat":4,"nbformat_minor":4}
