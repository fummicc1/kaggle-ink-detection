{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Introduction\n","This notebook is based on \n","1. [2.5d segmentaion baseline [training]](https://www.kaggle.com/code/tanakar/2-5d-segmentaion-baseline-training)\n","2. [2.5d segmentaion baseline [inference]](https://www.kaggle.com/code/tanakar/2-5d-segmentaion-baseline-inference)\n","3. [Vesuvius Challenge - 3D ResNet Training](https://www.kaggle.com/code/samfc10/vesuvius-challenge-3d-resnet-training)\n","4. [Improving performance with L1/Hessian denoising](https://www.kaggle.com/code/brettolsen/improving-performance-with-l1-hessian-denoising)\n","\n","Thank them for letting us learn more.(^w^)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Base parameter:\n","1. ResNet34\n","2. 1fold only!! (Use 2,3 to train and 1 to val)\n","3. use 16 channels\n","4. loss = 0.5 * BCELoss + 0.5 * DiceLoss\n","\n","then get cv0.55 (^w^)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-18T05:05:37.802699Z","iopub.status.busy":"2023-05-18T05:05:37.802266Z","iopub.status.idle":"2023-05-18T05:05:37.811251Z","shell.execute_reply":"2023-05-18T05:05:37.809786Z","shell.execute_reply.started":"2023-05-18T05:05:37.802657Z"},"trusted":true},"outputs":[],"source":["import os,cv2\n","import gc\n","import sys\n","import random\n","from glob import glob\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import pytorch_lightning as pl\n","from pytorch_lightning.loggers import WandbLogger\n","\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.cuda import amp\n","from torch.utils.data import Dataset, DataLoader\n","import segmentation_models_pytorch as smp\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","# ROOT_DIR = '/home/fummicc1/codes/competitions/kaggle-ink-detection/'\n","ROOT_DIR = '/home/fummicc1/codes/Kaggle/kaggle-ink-detection/'\n","sys.path.append(ROOT_DIR)\n","\n","# sys.path.append(\"/kaggle/input/resnet3d\")\n","from resnet3d import generate_model\n","import torch as tc\n","\n","\n","def set_seed(seed=None, cudnn_deterministic=True):\n","    if seed is None:\n","        seed = 42\n","\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = cudnn_deterministic\n","    torch.backends.cudnn.benchmark = False"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-18T05:05:37.814777Z","iopub.status.busy":"2023-05-18T05:05:37.813627Z","iopub.status.idle":"2023-05-18T05:05:37.82481Z","shell.execute_reply":"2023-05-18T05:05:37.823645Z","shell.execute_reply.started":"2023-05-18T05:05:37.814737Z"},"trusted":true},"outputs":[],"source":["class CFG:\n","    # ============== comp exp name =============\n","    comp_name = 'vesuvius'\n","\n","    # comp_dir_path = '/home/fummicc1/codes/competitions/kaggle-ink-detection/'\n","    # comp_dir_path = \"/home/fummicc1/codes/Kaggle/kaggle-ink-detection/\"\n","    comp_dir_path = '/home/fummicc1/codes/Kaggle/kaggle-ink-detection/'    \n","    # comp_dir_path = '/kaggle/input/'\n","    comp_folder_name = 'compress_data'\n","    # comp_folder_name = ''\n","    # comp_folder_name = 'vesuvius-challenge-ink-detection'\n","    # comp_dataset_path = f'{comp_dir_path}datasets/{comp_folder_name}/'\n","    comp_dataset_path = f'{comp_dir_path}{comp_folder_name}/'\n","    \n","    exp_name = 'vesuvius_2d_slide_exp005'\n","\n","    # ============== pred target =============\n","    target_size = 1\n","\n","    # ============== model cfg =============\n","    model_name = 'Unet'\n","    #backbone = 'efficientnet-b5'\n","    #backbone = 'mit_b5'\n","    backbone = 'resnet3d'\n","    #backbone = 'resnext50_32x4d'\n","    pretrained = True\n","\n","    in_chans = 48 # 65\n","    # ============== training cfg =============\n","    prd_size=160\n","    stride = prd_size // 4\n","    reisze_height = 2400\n","\n","    batch_size = 32 # 32\n","    use_amp = True\n","\n","    seed = 42\n","    num_workers=8\n","    \n","    mode = \"train\"\n","\n","    device_ids=[0, 1, 2, 3]\n","\n","    lr = 1e-3\n","    epochs = 10\n","\n","    TH = 0.5\n","    exp = 1e-6\n","    model_depth = 34\n","    TTA = True"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["set_seed(CFG.seed)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#in_submission=get_folder_size(\"/kaggle/input/vesuvius-challenge-ink-detection/test\")!=6732244267\n","TH = CFG.TH\n","fragment_ids = [1, 2]\n","\n","predictions_map = None\n","predictions_map_counts = None\n","pad0 = None\n","pad1 = None\n","ori_h = None\n","ori_w = None\n","all_median = None"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## helper"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-18T05:05:37.827281Z","iopub.status.busy":"2023-05-18T05:05:37.826465Z","iopub.status.idle":"2023-05-18T05:05:37.841307Z","shell.execute_reply":"2023-05-18T05:05:37.840253Z","shell.execute_reply.started":"2023-05-18T05:05:37.827238Z"},"trusted":true},"outputs":[],"source":["# ref.: https://www.kaggle.com/stainsby/fast-tested-rle\n","def rle(img):\n","    '''\n","    img: numpy array, 1 - mask, 0 - background\n","    Returns run length as string formated\n","    '''\n","    pixels = img.flatten()\n","    # pixels = (pixels >= thr).astype(int)\n","    \n","    pixels = np.concatenate([[0], pixels, [0]])\n","    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n","    runs[1::2] -= runs[::2]\n","    return ' '.join(str(x) for x in runs)\n","def normalization(x:tc.Tensor)->tc.Tensor:\n","    \"\"\"input.shape=(batch,f1,f2,...)\"\"\"\n","    #[batch,f1,f2]->dim[1,2]\n","    dim=list(range(1,x.ndim))\n","    mean=x.mean(dim=dim,keepdim=True)\n","    std=x.std(dim=dim,keepdim=True)\n","    return (x-mean)/(std+1e-9)\n","\n","def get_folder_size(folder_path):\n","    total_size = 0\n","    for dirpath, dirnames, filenames in os.walk(folder_path):\n","        for f in filenames:\n","            fp = os.path.join(dirpath, f)\n","            total_size += os.path.getsize(fp)\n","    return total_size\n","\n","def resize(img):\n","    current_height, current_width = img.shape[:2]\n","    aspect_ratio = current_width / current_height\n","    new_height = CFG.reisze_height\n","    new_width = int(new_height * aspect_ratio)\n","    new_size = (new_width, new_height)\n","    # (W, H)の順で渡すが結果は(H, W)になっている\n","    img = cv2.resize(img, new_size)\n","    return img"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def calculate_all_median(volume):\n","    global all_median\n","    all_median = np.median(volume, axis=[0, 1])\n","    print(\"all_median\", all_median)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-18T05:05:37.961199Z","iopub.status.busy":"2023-05-18T05:05:37.96084Z","iopub.status.idle":"2023-05-18T05:05:37.969555Z","shell.execute_reply":"2023-05-18T05:05:37.96832Z","shell.execute_reply.started":"2023-05-18T05:05:37.961169Z"},"trusted":true},"outputs":[],"source":["def read_image(mode, fragment_id):\n","    images = []\n","\n","    # idxs = range(65)\n","    mid = 65 // 2\n","    start = mid - CFG.in_chans // 2\n","    end = mid + CFG.in_chans // 2\n","    idxs = range(start, end)\n","\n","    for i in tqdm(idxs):\n","        image = cv2.imread(CFG.comp_dataset_path + f\"{mode}/{fragment_id}/surface_volume/{i:02}.png\", 0)\n","        # image = cv2.imread(CFG.comp_dataset_path + f\"{mode}/{fragment_id}/surface_volume/{i:02}.tif\", 0)\n","        image = resize(image)        \n","\n","        pad0 = (CFG.prd_size - image.shape[0] % CFG.prd_size)\n","        pad1 = (CFG.prd_size - image.shape[1] % CFG.prd_size)\n","\n","        image = np.pad(image, [(0, pad0), (0, pad1)], constant_values=0)\n","\n","        images.append(image)\n","    images = np.stack(images, axis=2)\n","    \n","    return images"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-18T05:05:37.972599Z","iopub.status.busy":"2023-05-18T05:05:37.972104Z","iopub.status.idle":"2023-05-18T05:05:37.982419Z","shell.execute_reply":"2023-05-18T05:05:37.981368Z","shell.execute_reply.started":"2023-05-18T05:05:37.972562Z"},"trusted":true},"outputs":[],"source":["class NormalizeTransform(A.ImageOnlyTransform):\n","    def __init__(self, always_apply=False, p=1.0):\n","        super(NormalizeTransform, self).__init__(always_apply, p)\n","\n","    def apply(self, img, **params):\n","        # print(\"all_median\", all_medin)\n","        median = np.full_like(img, all_median).astype(np.float32)\n","        # print(\"median\", median)\n","        # mad = np.full_like(img, all_MAD).astype(np.float32)\n","        # img = (img - median) / mad\n","        img = img / median\n","        # img = np.log(img + CFG.exp)\n","        # img = img / 255\n","        # img = (img - 0.45) / 0.225\n","        # img[img < 0] = 0\n","        return img\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, images, cfg,xys, labels=None, transform: bool = False):\n","        self.images = images\n","        self.cfg = cfg\n","        self.labels = labels\n","        self.xys=xys\n","        self.transform = transform\n","\n","    def __len__(self):\n","        # return len(self.xyxys)\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        BUFFER = CFG.prd_size // 2\n","        location = np.array(self.xys[idx])\n","        image: np.ndarray = self.images[idx]\n","        label = None\n","        if self.labels is not None:\n","            label = self.labels[idx]\n","            label = np.stack([label], axis=-1)\n","        if self.transform and label is not None:\n","            size = int(CFG.prd_size)\n","            # print(\"label\", label.shape, \"image\", image.shape)\n","            performed = A.Compose([\n","                NormalizeTransform(always_apply=True),\n","                A.HorizontalFlip(p=0.5), # 水平方向に反転\n","                A.VerticalFlip(p=0.5), # 水平方向に反転\n","                A.RandomRotate90(p=0.5),\n","                A.ShiftScaleRotate(p=0.5, border_mode=0), # シフト、スケーリング、回転\n","                A.RandomCrop(height=int(size / 1.25), width=int(size / 1.25), p=0.5), # ランダムにクロップ, Moduleの中で計算する際に次元がバッチ内で揃っている必要があるので最後にサイズは揃える\n","                A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n","                A.CoarseDropout(max_holes=1, max_width=int(size * 0.3), max_height=int(size * 0.3), \n","                                mask_fill_value=0, p=0.2),\n","                A.OneOf([\n","                    A.GaussianBlur(blur_limit=(3, 5)),\n","                    A.MotionBlur(blur_limit=5, p=1),\n","                ], p=1),\n","                A.Resize(BUFFER * 2, BUFFER * 2, always_apply=True),                                          \n","                ToTensorV2(transpose_mask=True),                \n","            ])(image=image, mask=label)            \n","            image = performed[\"image\"]\n","            label = performed[\"mask\"]            \n","        else:\n","            if label is None:\n","                performed = A.Compose([                                \n","                    NormalizeTransform(always_apply=True),\n","                    ToTensorV2(transpose_mask=True),\n","                ])(image=image)\n","                image = performed[\"image\"]\n","                return image, location\n","            else:\n","                # print(\"image in val dataset (before aug)\", image, file=open(\"before-val-aug.log\", \"w\")) \n","                performed = A.Compose([\n","                    NormalizeTransform(always_apply=True),\n","                    ToTensorV2(transpose_mask=True),\n","                ])(image=image, mask=label)                \n","                label = performed[\"mask\"]                \n","                image = performed[\"image\"]\n","            \n","        return image, label\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-18T05:05:37.984974Z","iopub.status.busy":"2023-05-18T05:05:37.983869Z","iopub.status.idle":"2023-05-18T05:05:37.996389Z","shell.execute_reply":"2023-05-18T05:05:37.995129Z","shell.execute_reply.started":"2023-05-18T05:05:37.984894Z"},"trusted":true},"outputs":[],"source":["def make_dataset(fragment_id, is_train_data):\n","    global all_median\n","    images = read_image(CFG.mode, fragment_id)\n","    mask_path = os.path.join(CFG.comp_dataset_path, f\"{CFG.mode}/{fragment_id}/mask.png\")\n","    label_path = os.path.join(CFG.comp_dataset_path, f\"{CFG.mode}/{fragment_id}/inklabels.png\")\n","    if os.path.exists(label_path):\n","        labels = cv2.imread(label_path, 0)\n","        labels = resize(labels)\n","        labels //= 255\n","    else:\n","        labels = None\n","    mask = cv2.imread(mask_path, 0)\n","    mask = resize(mask)\n","    mask //= 255\n","    pad0 = (CFG.prd_size - mask.shape[0] % CFG.prd_size)\n","    pad1 = (CFG.prd_size - mask.shape[1] % CFG.prd_size)\n","\n","    if labels is not None:\n","        labels = np.pad(labels, [(0, pad0), (0, pad1)], constant_values=0)\n","    \n","    x1_list = list(range(0, images.shape[1]-CFG.prd_size+1, CFG.stride))\n","    y1_list = list(range(0, images.shape[0]-CFG.prd_size+1, CFG.stride))\n","    \n","    images_list = []\n","    labels_list = []\n","    xyxys = []\n","    for y1 in y1_list:\n","        for x1 in x1_list:\n","            y2 = y1 + CFG.prd_size\n","            x2 = x1 + CFG.prd_size\n","            if np.all(images[y1:y2, x1:x2]==0):\n","                continue\n","            if labels is not None:\n","                labels_list.append(labels[y1:y2, x1:x2])\n","            images_list.append(images[y1:y2, x1:x2])\n","            xyxys.append((x1, y1, x2, y2))\n","    print(\"hello\")\n","    xyxys = np.stack(xyxys)    \n","\n","    ds = CustomDataset(images_list, CFG,xys=xyxys, labels=labels_list if labels_list else None, transform=is_train_data)\n","    \n","    loader = DataLoader(\n","        ds,\n","        batch_size=CFG.batch_size,\n","        shuffle=is_train_data,\n","        num_workers=CFG.num_workers,\n","        pin_memory=True,\n","        drop_last=False,\n","        persistent_workers=True\n","    )\n","    \n","    return loader, xyxys, images"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 3D ResNet"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-18T05:05:37.99883Z","iopub.status.busy":"2023-05-18T05:05:37.998208Z","iopub.status.idle":"2023-05-18T05:05:38.197442Z","shell.execute_reply":"2023-05-18T05:05:38.196189Z","shell.execute_reply.started":"2023-05-18T05:05:37.998793Z"},"trusted":true},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, encoder_dims, upscale):\n","        super().__init__()\n","        self.convs = nn.ModuleList([\n","            nn.Sequential(\n","                nn.Conv2d(encoder_dims[i]+encoder_dims[i-1], encoder_dims[i-1], 3, 1, 1, bias=False),\n","                nn.BatchNorm2d(encoder_dims[i-1]),\n","                nn.ReLU(inplace=True)\n","            ) for i in range(1, len(encoder_dims))])\n","\n","        self.logit = nn.Conv2d(encoder_dims[0], 1, 1, 1, 0)\n","        self.up = nn.Upsample(scale_factor=upscale, mode=\"bilinear\")\n","\n","    def forward(self, feature_maps):\n","        for i in range(len(feature_maps)-1, 0, -1):\n","            f_up = F.interpolate(feature_maps[i], scale_factor=2, mode=\"bilinear\")\n","            f = torch.cat([feature_maps[i-1], f_up], dim=1)\n","            f_down = self.convs[i-1](f)\n","            feature_maps[i-1] = f_down\n","\n","        x = self.logit(feature_maps[0])\n","        mask = self.up(x)\n","        return mask\n","    \n","class SegModel(nn.Module):\n","    def __init__(self,model_depth=CFG.model_depth):\n","        super().__init__()\n","        self.encoder = generate_model(model_depth=model_depth, n_input_channels=1)\n","        self.decoder = Decoder(encoder_dims=[64, 128, 256, 512], upscale=4)\n","        \n","    def forward(self, x):\n","        if x.ndim==4:\n","            x=x[:,None] # チャネルを追加\n","        \n","        feat_maps = self.encoder(x)\n","        feat_maps_pooled = [torch.mean(f, dim=2) for f in feat_maps]\n","        pred_mask = self.decoder(feat_maps_pooled)\n","        return pred_mask\n","        \n","class CustomModel(nn.Module):\n","    def __init__(self, cfg=CFG, weight=None):\n","        super().__init__()\n","        self.cfg = cfg\n","\n","        if cfg.backbone==\"resnet3d\":\n","            self.encoder=SegModel()\n","        elif cfg.backbone[:3]!=\"mit\":\n","            self.encoder = smp.Unet(\n","                encoder_name=cfg.backbone, \n","                encoder_weights=weight,\n","                in_channels=cfg.in_chans,\n","                classes=cfg.target_size,\n","                activation=None,\n","            )\n","        else :\n","            self.encoder = smp.Unet(\n","                encoder_name=cfg.backbone, \n","                encoder_weights=weight,\n","                classes=cfg.target_size,\n","                activation=None,\n","            )\n","            print(\"self.encoder.encoder.patch_embed1.proj\", self.encoder.encoder.patch_embed1.proj)\n","            out_channels=self.encoder.encoder.patch_embed1.proj.out_channels\n","            self.encoder.encoder.patch_embed1.proj=nn.Conv2d(cfg.in_chans,out_channels,7,4,3)\n","\n","    def forward(self, images:torch.Tensor):\n","        #image.shape=(b,C,H,W)\n","        if images.ndim==4:\n","            images=images[:,None]\n","        # images=normalization(images)\n","        output = self.encoder(images)\n","        return output\n","\n","def build_model(cfg, weight=\"imagenet\"):\n","    print('model_name', cfg.model_name)\n","    print('backbone', cfg.backbone)\n","\n","    model = CustomModel(cfg, weight)\n","    return model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-18T05:05:38.202161Z","iopub.status.busy":"2023-05-18T05:05:38.20123Z","iopub.status.idle":"2023-05-18T05:05:38.213584Z","shell.execute_reply":"2023-05-18T05:05:38.212513Z","shell.execute_reply.started":"2023-05-18T05:05:38.202121Z"},"trusted":true},"outputs":[],"source":["tc = torch\n","def TTA(x:tc.Tensor,model:nn.Module):\n","    #x.shape=(batch,c,h,w)\n","    shape=x.shape\n","    x=[x,*[tc.rot90(x,k=i,dims=(-2,-1)) for i in range(1,4)]]\n","    x=tc.cat(x,dim=0)\n","    x=model(x)\n","    x=torch.sigmoid(x)\n","    x=x.reshape(4,shape[0], ,*shape[2:])\n","    x=[tc.rot90(x[i],k=4-i,dims=(-2,-1)) for i in range(4)]\n","    x=tc.stack(x,dim=0)\n","    return x.mean(0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lodaer, _, images = make_dataset(1, is_train_data=True)\n","calculate_all_median(images)\n","print(len(lodaer.dataset))\n","img, _ = lodaer.dataset[100]\n","plt.imshow(img[1, :, :])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Model(pl.LightningModule):\n","    \n","    training_step_outputs = []\n","    validation_step_outputs = []\n","    test_step_outputs = [[], []]\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","        model = build_model(CFG)        \n","        \n","        self.model = model\n","\n","        self.segmentation_loss_fn = smp.losses.DiceLoss(\n","            smp.losses.BINARY_MODE,\n","            log_loss=True,\n","            from_logits=True,\n","            smooth=1e-6,\n","        )\n","\n","    def forward(self, image):\n","        # normalize image here\n","        # image = (image - self.mean) / self.std\n","        if CFG.TTA:\n","            mask = TTA(image, self.model)\n","        else:\n","            mask = self.model(image)\n","            mask = torch.sigmoid(mask)\n","        return mask\n","\n","    def shared_step(self, batch, stage):        \n","        images, labels = batch\n","        images, labels = images.float(), labels.float()\n","        assert images.ndim == 4\n","        h, w = images.shape[2:]\n","        assert h % 32 == 0 and w % 32 == 0\n","\n","        assert labels.ndim == 4\n","\n","        assert labels.max() <= 1.0 and labels.min() >= 0\n","\n","        # print(\"labels.shape\", labels.shape[2:], \"images.shape\", images.shape[2:])\n","\n","        assert labels.shape[2] == images.shape[2] and labels.shape[3] == images.shape[3]\n","\n","        segmentation_out = self.forward(images)\n","        \n","        # Predicted mask contains logits, and loss_fn param `from_logits` is set to True\n","        loss = self.segmentation_loss_fn(segmentation_out, labels)\n","\n","        prob_mask = segmentation_out \n","        pred_mask = (prob_mask > TH).float()\n","\n","        tp, fp, fn, tn = smp.metrics.get_stats(pred_mask.long(), labels.long(), mode=\"binary\")\n","\n","        return {\n","            \"loss\": loss,\n","            \"tp\": tp,\n","            \"fp\": fp,\n","            \"fn\": fn,\n","            \"tn\": tn,\n","        }\n","\n","    def shared_epoch_end(self, outputs, stage):\n","        # aggregate step metics\n","        tp = torch.cat([x[\"tp\"] for x in outputs])\n","        fp = torch.cat([x[\"fp\"] for x in outputs])\n","        fn = torch.cat([x[\"fn\"] for x in outputs])\n","        tn = torch.cat([x[\"tn\"] for x in outputs])\n","        loss = torch.mean(torch.Tensor([x[\"loss\"] for x in outputs]))\n","\n","        # per image IoU means that we first calculate IoU score for each image \n","        # and then compute mean over these scores\n","        per_image_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro-imagewise\")        \n","        dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n","        \n","        metrics = {\n","            f\"{stage}_per_image_iou\": per_image_iou,\n","            f\"{stage}_dataset_iou\": dataset_iou,\n","            f\"{stage}_loss\": loss,\n","            f\"{stage}_tp\": tp.sum().item(),\n","            f\"{stage}_fp\": fp.sum().item(),\n","            f\"{stage}_fn\": fn.sum().item(),\n","            f\"{stage}_tn\": tn.sum().item(),\n","        }\n","        \n","        self.log_dict(metrics, prog_bar=True, sync_dist=True)\n","\n","    def training_step(self, batch, batch_idx):\n","        out = self.shared_step(batch, \"train\")\n","        self.training_step_outputs.append(out)\n","        return out\n","\n","    def on_train_epoch_end(self):\n","        out = self.shared_epoch_end(self.training_step_outputs, \"train\")\n","        self.training_step_outputs.clear()\n","        return out\n","\n","    def validation_step(self, batch, batch_idx):\n","        out = self.shared_step(batch, \"valid\")\n","        self.validation_step_outputs.append(out)        \n","        return out\n","\n","    def on_validation_epoch_end(self):\n","        out = self.shared_epoch_end(self.validation_step_outputs, \"valid\")\n","        self.validation_step_outputs.clear()\n","        return out\n","\n","    def test_step(self, batch, batch_idx):\n","        global predictions_map, predictions_map_counts\n","\n","        patch_batch, loc_batch = batch\n","        \n","        loc_batch = loc_batch.long()\n","        patch_batch = patch_batch.float()\n","        predictions: torch.Tensor = self.forward(patch_batch)\n","        # print(\"predictions.shape\", predictions.shape)\n","        # print(\"predictions\", predictions)        \n","        predictions = predictions.sigmoid()\n","        # print(\"Softmaxed predictions where conf is gt threshold\", predictions[predictions.gt(threshold)])\n","        # print(\"predictions.shape after sigmoid\", predictions.shape)\n","        # →(BATCH, W, H, C)\n","        predictions = torch.permute(predictions, (0, 3, 2, 1))\n","        predictions = predictions.squeeze(dim=-1)\n","        # print(\"prediction shape\", predictions.shape)\n","        predictions = predictions.cpu().numpy()  # move predictions to cpu and convert to numpy\n","        loc_batch = loc_batch.cpu().numpy()\n","        # print(\"predictions_map\", predictions_map)\n","        # print(\"predictions_map_count\", predictions_map_counts)\n","        self.test_step_outputs[0].extend(loc_batch)\n","        self.test_step_outputs[1].extend(predictions)\n","        return loc_batch, predictions\n","\n","    def on_test_epoch_end(self):\n","        global predictions_map, predictions_map_counts        \n","        BUFFER = CFG.prd_size // 2\n","        exp = CFG.exp\n","        \n","        locs = np.array(self.test_step_outputs[0])\n","        preds = np.array(self.test_step_outputs[1])\n","        print(\"locs\", locs.shape)\n","        print(\"preds\", preds.shape)\n","        \n","        new_predictions_map = np.zeros_like(predictions_map[:, :, 0])\n","        new_predictions_map_counts = np.zeros_like(predictions_map_counts[:, :, 0])\n","        \n","        for (x1, y1, x2, y2), pred in zip(locs, preds):            \n","            new_predictions_map[\n","                y1 : y2, x1 : x2\n","            ] += pred\n","            new_predictions_map_counts[y1 : y2, x1 : x2] += 1\n","        new_predictions_map /= (new_predictions_map_counts + exp)        \n","        new_predictions_map = new_predictions_map[:, :, np.newaxis]\n","        new_predictions_map_counts = new_predictions_map_counts[:, :, np.newaxis]\n","        predictions_map = np.concatenate([predictions_map, new_predictions_map], axis=-1)\n","        predictions_map_counts = np.concatenate([predictions_map_counts, new_predictions_map_counts], axis=-1)\n","        print(\"new_predictions_map\", new_predictions_map.shape)\n","        print(\"predictions_map\", predictions_map.shape)\n","\n","    def configure_optimizers(self):\n","        lr = CFG.lr\n","        optimizer = optim.Adam(self.parameters(), lr=lr)\n","        # Using a scheduler is optional but can be helpful.\n","        # The scheduler reduces the LR if the validation performance hasn't improved for the last N epochs\n","        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.05, patience=5, min_lr=5e-5)\n","        return {\"optimizer\": optimizer, \"lr_scheduler\": { \"scheduler\": scheduler, \"monitor\": \"valid_loss\" }}\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## main"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-18T05:05:42.041465Z","iopub.status.busy":"2023-05-18T05:05:42.040711Z","iopub.status.idle":"2023-05-18T05:06:06.128361Z","shell.execute_reply":"2023-05-18T05:06:06.126683Z","shell.execute_reply.started":"2023-05-18T05:05:42.041423Z"},"trusted":true},"outputs":[],"source":["results = []\n","\n","val_fragment_id = 3\n","\n","model = Model()\n","\n","def train(fragment_id, prev_fragment_id):\n","    global predictions_map, predictions_map_counts, pad0, pad1, ori_h, ori_w, model, trainer\n","    loader, xyxys, images = make_dataset(fragment_id, is_train_data=True)\n","    val_loader, _ ,_ = make_dataset(val_fragment_id, is_train_data=False)    \n","    calculate_all_median(images)\n","\n","    print(CFG.comp_dataset_path + f\"{CFG.mode}/{fragment_id}/mask.png\")\n","    binary_mask = cv2.imread(CFG.comp_dataset_path + f\"{CFG.mode}/{fragment_id}/mask.png\", 0)\n","    binary_mask = resize(binary_mask)\n","    binary_mask = (binary_mask // 255).astype(int)\n","    \n","    ori_h = binary_mask.shape[0]\n","    ori_w = binary_mask.shape[1]\n","\n","    pad0 = (CFG.prd_size - binary_mask.shape[0] % CFG.prd_size)\n","    pad1 = (CFG.prd_size - binary_mask.shape[1] % CFG.prd_size)\n","\n","    binary_mask = np.pad(binary_mask, [(0, pad0), (0, pad1)], constant_values=0)\n","\n","    # (W, H, 1)\n","    predictions_map = np.empty_like(binary_mask.transpose((1, 0)))[:, :, np.newaxis].astype(np.float64)\n","    predictions_map_counts = np.empty_like(predictions_map).astype(np.uint8)    \n","    trainer = pl.Trainer(\n","        max_epochs=CFG.epochs,\n","        devices=\"auto\",\n","        accelerator=\"auto\",\n","        # strategy=\"ddp_find_unused_parameters_false\",     \n","        logger=WandbLogger(name=CFG.exp_name)   \n","    )\n","    trainer.fit(\n","        model,\n","        loader,\n","        val_loader,\n","    )\n","    del loader, val_loader\n","    gc.collect()\n","    \n","\n","for i, fragment_id in enumerate(fragment_ids):    \n","    train(fragment_id, None)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat":4,"nbformat_minor":4}
