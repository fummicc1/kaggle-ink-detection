{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Keras starter kit [full training set, UNet]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:10:40.298967Z","iopub.status.busy":"2023-03-18T23:10:40.298008Z","iopub.status.idle":"2023-03-18T23:10:49.514216Z","shell.execute_reply":"2023-03-18T23:10:49.513058Z","shell.execute_reply.started":"2023-03-18T23:10:40.298921Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","import glob\n","import time\n","import PIL.Image as Image\n","import matplotlib.pyplot as plt\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import Rectangle\n","import matplotlib.patches as patches\n","from tqdm import tqdm\n","import cv2\n","\n","# Data config\n","# DATA_DIR = '/kaggle/input/vesuvius-challenge-ink-detection/'\n","DATA_DIR = '/home/fummicc1/codes/competitions/kaggle-ink-detection'\n","BUFFER = 128  # Half-size of papyrus patches we'll use as model inputs\n","Z_LIST = list(range(0, 65, 3))  # Offset of slices in the z direction\n","Z_DIM = len(Z_LIST)  # Number of slices in the z direction. Max value is 64 - Z_START\n","SHARED_HEIGHT = 4000  # Height to resize all papyrii\n","\n","# (y, x)\n","val_location = (600, 500)\n","val_zone_size = (1000, 2000)\n","\n","# Model config\n","BATCH_SIZE = 128\n","USE_MIXED_PRECISION = False\n","USE_JIT_COMPILE = False\n","\n","device = torch.device(\"cuda\")\n","threshold = 0.35\n","num_workers = 4\n","exp = 1e-7"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:10:49.517657Z","iopub.status.busy":"2023-03-18T23:10:49.516568Z","iopub.status.idle":"2023-03-18T23:10:52.651975Z","shell.execute_reply":"2023-03-18T23:10:52.650919Z","shell.execute_reply.started":"2023-03-18T23:10:49.517615Z"},"trusted":true},"outputs":[],"source":["plt.imshow(Image.open(DATA_DIR + \"/train/1/ir.png\"), cmap=\"gray\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Load up the training data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:10:52.653587Z","iopub.status.busy":"2023-03-18T23:10:52.653064Z","iopub.status.idle":"2023-03-18T23:10:57.915118Z","shell.execute_reply":"2023-03-18T23:10:57.914028Z","shell.execute_reply.started":"2023-03-18T23:10:52.653544Z"},"trusted":true},"outputs":[],"source":["def resize(img):\n","    current_height, current_width = img.shape    \n","    aspect_ratio = current_width / current_height\n","    new_width = int(SHARED_HEIGHT * aspect_ratio)\n","    new_size = (new_width, SHARED_HEIGHT)\n","    # (W, H)の順で渡すが結果は(H, W)になっている\n","    img = cv2.resize(img, new_size)\n","    return img\n","\n","def load_mask(split, index):\n","    img = cv2.imread(f\"{DATA_DIR}/{split}/{index}/mask.png\", 0) // 255\n","    img = resize(img)    \n","    return img\n","\n","\n","def load_labels(split, index):\n","    img = cv2.imread(f\"{DATA_DIR}/{split}/{index}/inklabels.png\", 0) // 255\n","    img = resize(img)\n","    return img\n","\n","\n","mask = load_mask(split=\"train\", index=1)\n","labels = load_labels(split=\"train\", index=1)\n","\n","fig, (ax1, ax2) = plt.subplots(1, 2)\n","ax1.set_title(\"mask.png\")\n","ax1.imshow(mask, cmap='gray')\n","ax2.set_title(\"inklabels.png\")\n","ax2.imshow(labels, cmap='gray')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:10:57.922052Z","iopub.status.busy":"2023-03-18T23:10:57.921638Z","iopub.status.idle":"2023-03-18T23:11:00.615559Z","shell.execute_reply":"2023-03-18T23:11:00.614397Z","shell.execute_reply.started":"2023-03-18T23:10:57.922012Z"},"trusted":true},"outputs":[],"source":["mask_test_a = load_mask(split=\"test\", index=\"a\")\n","mask_test_b = load_mask(split=\"test\", index=\"b\")\n","\n","mask_train_1 = load_mask(split=\"train\", index=1)\n","labels_train_1 = load_labels(split=\"train\", index=1)\n","\n","mask_train_2 = load_mask(split=\"train\", index=2)\n","labels_train_2 = load_labels(split=\"train\", index=2)\n","\n","mask_train_3 = load_mask(split=\"train\", index=3)\n","labels_train_3 = load_labels(split=\"train\", index=3)\n","\n","print(f\"mask_test_a: {mask_test_a.shape}\")\n","print(f\"mask_test_b: {mask_test_b.shape}\")\n","print(\"-\")\n","print(f\"mask_train_1: {mask_train_1.shape}\")\n","print(f\"labels_train_1: {labels_train_1.shape}\")\n","print(\"-\")\n","print(f\"mask_train_2: {mask_train_2.shape}\")\n","print(f\"labels_train_2: {labels_train_2.shape}\")\n","print(\"-\")\n","print(f\"mask_train_3: {mask_train_3.shape}\")\n","print(f\"labels_train_3: {labels_train_3.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:11:00.619816Z","iopub.status.busy":"2023-03-18T23:11:00.619103Z","iopub.status.idle":"2023-03-18T23:11:02.849127Z","shell.execute_reply":"2023-03-18T23:11:02.847843Z","shell.execute_reply.started":"2023-03-18T23:11:00.619773Z"},"trusted":true},"outputs":[],"source":["fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n","\n","ax1.set_title(\"labels_train_1\")\n","ax1.imshow(labels_train_1, cmap='gray')\n","\n","ax2.set_title(\"labels_train_2\")\n","ax2.imshow(labels_train_2, cmap='gray')\n","\n","ax3.set_title(\"labels_train_3\")\n","ax3.imshow(labels_train_3, cmap='gray')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:11:02.854397Z","iopub.status.busy":"2023-03-18T23:11:02.85384Z","iopub.status.idle":"2023-03-18T23:11:02.867203Z","shell.execute_reply":"2023-03-18T23:11:02.866021Z","shell.execute_reply.started":"2023-03-18T23:11:02.854351Z"},"trusted":true},"outputs":[],"source":["def load_volume(split, index):\n","    # Load the 3d x-ray scan, one slice at a time\n","    all = sorted(glob.glob(f\"{DATA_DIR}/{split}/{index}/surface_volume/*.tif\"))\n","    z_slices_fnames = [all[i] for i in range(len(all)) if i in Z_LIST]\n","    assert len(z_slices_fnames) == Z_DIM\n","    z_slices = []\n","    for z, filename in  tqdm(enumerate(z_slices_fnames)):\n","        img = cv2.imread(filename, -1)\n","        img = resize(img)\n","        z_slices.append(img)\n","    return np.stack(z_slices, axis=-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:11:02.874585Z","iopub.status.busy":"2023-03-18T23:11:02.872191Z","iopub.status.idle":"2023-03-18T23:14:11.220165Z","shell.execute_reply":"2023-03-18T23:14:11.218002Z","shell.execute_reply.started":"2023-03-18T23:11:02.874541Z"},"trusted":true},"outputs":[],"source":["volume_train_1 = load_volume(split=\"train\", index=1)\n","print(f\"volume_train_1: {volume_train_1.shape}, {volume_train_1.dtype}\")\n","\n","volume_train_2 = load_volume(split=\"train\", index=2)\n","print(f\"volume_train_2: {volume_train_2.shape}, {volume_train_2.dtype}\")\n","\n","volume_train_3 = load_volume(split=\"train\", index=3)\n","print(f\"volume_train_3: {volume_train_3.shape}, {volume_train_3.dtype}\")\n","\n","volume = np.concatenate([volume_train_1, volume_train_2, volume_train_3], axis=1)\n","print(f\"total volume: {volume.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["del volume_train_1\n","del volume_train_2\n","del volume_train_3"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:14:11.222178Z","iopub.status.busy":"2023-03-18T23:14:11.221817Z","iopub.status.idle":"2023-03-18T23:14:11.231554Z","shell.execute_reply":"2023-03-18T23:14:11.230344Z","shell.execute_reply.started":"2023-03-18T23:14:11.222141Z"},"trusted":true},"outputs":[],"source":["labels = np.concatenate([labels_train_1, labels_train_2, labels_train_3], axis=1)\n","print(f\"labels: {labels.shape}, {labels.dtype}\")\n","mask = np.concatenate([mask_train_1, mask_train_2, mask_train_3], axis=1)\n","print(f\"mask: {mask.shape}, {mask.dtype}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Free up memory\n","del labels_train_1\n","del labels_train_2\n","del labels_train_3\n","del mask_train_1\n","del mask_train_2\n","del mask_train_3"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Visualize the training data\n","\n","In this case, not very informative. But remember to always visualize what you're training on, as a sanity check!"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:14:11.233579Z","iopub.status.busy":"2023-03-18T23:14:11.233103Z","iopub.status.idle":"2023-03-18T23:14:16.475332Z","shell.execute_reply":"2023-03-18T23:14:16.474429Z","shell.execute_reply.started":"2023-03-18T23:14:11.233541Z"},"trusted":true},"outputs":[],"source":["fig, axes = plt.subplots(1, 2, figsize=(15, 3))\n","for z, ax in enumerate(axes):\n","    ax.imshow(volume[:, :, z], cmap='gray')\n","    ax.set_xticks([]); ax.set_yticks([])\n","fig.tight_layout()\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Selection a validation holdout area\n","\n","We set aside some fraction of the input to validate our model on."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:14:16.477954Z","iopub.status.busy":"2023-03-18T23:14:16.476814Z","iopub.status.idle":"2023-03-18T23:14:17.956818Z","shell.execute_reply":"2023-03-18T23:14:17.955681Z","shell.execute_reply.started":"2023-03-18T23:14:16.477911Z"},"trusted":true},"outputs":[],"source":["fig, ax = plt.subplots()\n","ax.imshow(labels)\n","patch = patches.Rectangle([val_location[1], val_location[0]], val_zone_size[1], val_zone_size[0], linewidth=2, edgecolor='g', facecolor='none')\n","ax.add_patch(patch)\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Create a dataset that samples random locations in the input volume\n","\n","Our training dataset will grab random patches within the masked area and outside of the validation area."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def sample_random_location(shape):\n","    random_train_x = np.random.randint(low=BUFFER, high=shape[1] - BUFFER - 1, size=())\n","    random_train_y = np.random.randint(low=BUFFER, high=shape[0] - BUFFER - 1, size=())\n","    random_train_location = np.stack([random_train_y, random_train_x], axis=-1)\n","    return random_train_location\n","\n","\n","def is_in_masked_zone(location, mask):\n","    return mask[location[0], location[1]] > 0\n","\n","def is_in_val_zone(location, val_location, val_zone_size):\n","    x = location[1]\n","    y = location[0]\n","    x_match = val_location[1] - BUFFER <= x <= val_location[1] + val_zone_size[1] + BUFFER\n","    y_match = val_location[0] - BUFFER <= y <= val_location[0] + val_zone_size[0] + BUFFER\n","    return x_match and y_match"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","def is_proper_train_location(location):\n","    return not is_in_val_zone(location, val_location, val_zone_size) and is_in_mask_train(location)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:14:17.958942Z","iopub.status.busy":"2023-03-18T23:14:17.958495Z","iopub.status.idle":"2023-03-18T23:14:18.45515Z","shell.execute_reply":"2023-03-18T23:14:18.454126Z","shell.execute_reply.started":"2023-03-18T23:14:17.958902Z"},"trusted":true},"outputs":[],"source":["sample_random_location_train = lambda x: sample_random_location(volume.shape[:-1])\n","is_in_mask_train = lambda x: is_in_masked_zone(x, mask)\n","\n","# Create a list to store train locations\n","train_locations = []\n","\n","# Define the number of train locations you want to generate\n","num_train_locations = 4000\n","\n","# Generate train locations\n","while len(train_locations) < num_train_locations:\n","    location = sample_random_location_train(0)\n","    if is_proper_train_location(location):\n","        train_locations.append(location)\n","\n","# Convert the list of train locations to a PyTorch tensor\n","train_locations_ds = np.stack(train_locations, axis=0)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Visualize some training patches\n","\n","Sanity check visually that our patches are where they should be."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:14:18.457083Z","iopub.status.busy":"2023-03-18T23:14:18.456727Z","iopub.status.idle":"2023-03-18T23:14:21.576187Z","shell.execute_reply":"2023-03-18T23:14:21.575222Z","shell.execute_reply.started":"2023-03-18T23:14:18.457045Z"},"trusted":true},"outputs":[],"source":["fig, ax = plt.subplots()\n","ax.imshow(labels)\n","\n","# Define the number of samples you want to take from train_locations_ds\n","num_samples = 4000\n","\n","# Iterate over the first 'num_samples' elements in train_locations_ds\n","for i in range(num_samples):\n","    y, x = train_locations_ds[i]\n","    patch = Rectangle([x - BUFFER, y - BUFFER], 2 * BUFFER, 2 * BUFFER, linewidth=2, edgecolor='r', facecolor='none')\n","    ax.add_patch(patch)\n","\n","val_patch = patches.Rectangle([val_location[1], val_location[0]], val_zone_size[1], val_zone_size[0], linewidth=2, edgecolor='g', facecolor='none')\n","ax.add_patch(val_patch)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from scipy.stats import median_abs_deviation\n","all_MAD = median_abs_deviation(volume, axis=[0, 1])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["all_median = np.median(volume, axis=[0, 1])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mean = np.mean(volume)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mean"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["std = np.std(volume)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["std"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["possible_max_input = ((2 ** 16 - 1) / all_median.min())\n","possible_max_input"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"all_median\", all_median)\n","\"all_median\", all_median"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"all_MAD\", all_MAD)\n","\"all_MAD\", all_MAD"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["all"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["printed = False\n","\n","def extract_subvolume(location, volume):\n","    global printed\n","    # print(np.unique(volume, return_counts=True, return_index=True))\n","    x = location[0]\n","    y = location[1]\n","    subvolume = volume[x-BUFFER:x+BUFFER, y-BUFFER:y+BUFFER, :].astype(np.float32)\n","    # print(\"subvolume[:, :, 0]\", subvolume[:, :, 0])\n","    median = np.full_like(subvolume, all_median).astype(np.float32)\n","    MAD = np.full_like(subvolume, all_MAD).astype(np.float32)\n","    # mean = np.mean(subvolume, axis=2)\n","    # mean = np.stack([mean for i in range(Z_DIM)], axis=2) + exp\n","    # MAD = median_abs_deviation(subvolume, axis=2)\n","    # print(\"MAD\", MAD[0, 0, :])\n","    # print(\"mean\", mean)\n","    # print(\"median\", median[0, 0, :])\n","    \n","    subvolume = (subvolume / median)\n","    \n","    if not printed:\n","        print(\"subvolume after taking care of median and MAD\", subvolume)\n","        printed = True\n","    \n","    return subvolume"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Create training dataset that yields random subvolumes and their labels"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.preprocessing import OneHotEncoder\n","\n","class SubvolumeDataset(Dataset):\n","    def __init__(self, locations, volume, labels, buffer, is_train: bool, return_location: bool = False):\n","        self.locations = locations\n","        self.volume = volume\n","        self.labels = labels        \n","        self.buffer = buffer\n","        self.is_train = is_train\n","        self.return_location = return_location\n","\n","    def __len__(self):\n","        return len(self.locations)\n","\n","    def __getitem__(self, idx):\n","        label = None\n","        location = np.array(self.locations[idx])\n","        y, x = location[0], location[1]\n","\n","        subvolume = extract_subvolume(location, self.volume)        \n","        # print(\"subvolume\", subvolume)\n","        # print(\"labels\", labels)\n","        # subvolume = subvolume.numpy()\n","        subvolume = subvolume\n","        \n","        if self.labels is not None:\n","            label = self.labels[y - self.buffer:y + self.buffer, x - self.buffer:x + self.buffer]\n","            # print(\"label\", label)\n","            # n_category = 2\n","            # label = np.eye(n_category)[label]\n","            label = np.stack([label], axis=-1)\n","            # label = label.numpy()\n","            # print(\"label.shape\", label.shape\n","        \n","        if self.is_train and label is not None:            \n","            \n","            # print(\"label\", label.dtype)\n","            # print(\"subvolume in dataset (before aug)\", subvolume)    \n","            size = int(BUFFER * 1.5)\n","            performed = A.Compose([            \n","                A.ToFloat(max_value=possible_max_input),\n","                A.HorizontalFlip(p=0.5), # 水平方向に反転\n","                A.VerticalFlip(p=0.5), # 水平方向に反転\n","                A.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0), # シフト、スケーリング、回転\n","                A.PadIfNeeded(min_height=size, min_width=size, always_apply=True, border_mode=0), # 必要に応じてパディングを追加\n","                A.RandomCrop(height=size, width=size, always_apply=True), # ランダムにクロップ, Moduleの中で計算する際に次元がバッチ内で揃っている必要があるので最後にサイズは揃える\n","                # A.GaussNoise(p=0.2), # ガウスノイズを追加　Note: これは背景とinkの境界を曖昧にしてしまうため不適切かもしれない\n","                A.Perspective(p=0.5), # パースペクティブ変換                   \n","                # A.OneOf(\n","                #     [\n","                #         A.Sharpen(p=1),\n","                #         A.Blur(blur_limit=3, p=1),\n","                #         A.MotionBlur(blur_limit=3, p=1),\n","                #     ],\n","                #     p=0.9,\n","                # ),\n","                A.Resize(BUFFER * 2, BUFFER * 2, always_apply=True),\n","                A.FromFloat(max_value=possible_max_input),\n","            ])(image=subvolume, mask=label)            \n","            subvolume = performed[\"image\"]            \n","            label = performed[\"mask\"]\n","            # print(\"subvolume in dataset (after aug)\", subvolume)\n","            # print(\"label\", label.dtype)\n","            # print(\"subvolume\", subvolume.dtype)\n","            # →C, H, W\n","            subvolume = torch.from_numpy(subvolume.transpose(2, 0, 1).astype(np.float64))\n","            # print(performed)\n","            # print(subvolume.shape, label.shape)\n","            # H, W, C → C, H, W\n","            label = torch.from_numpy(label.transpose(2, 0, 1).astype(np.uint8)) \n","        else:\n","            performed = A.Compose([  \n","                A.ToFloat(max_value=possible_max_input),                \n","                # A.Normalize(\n","                #     mean=[mean],\n","                #     std=[std],\n","                # ),\n","                A.FromFloat(max_value=possible_max_input),\n","            ])(image=subvolume)\n","            subvolume = performed[\"image\"]\n","            subvolume = torch.from_numpy(subvolume.transpose(2, 0, 1).astype(np.float64))\n","            if label is not None:\n","                label = torch.from_numpy(label.transpose(2, 0, 1).astype(np.uint8)) \n","        if self.return_location:\n","            return subvolume, location\n","        return subvolume, label        "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Convert train_locations_ds to a PyTorch tensor\n","train_locations_tensor = np.stack([x for x in train_locations_ds], axis=0)\n","\n","# Create an instance of the SubvolumeDataset\n","train_ds = SubvolumeDataset(train_locations_tensor, volume, labels, BUFFER, is_train=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create a DataLoader with the dataset\n","train_dataloader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:14:23.607564Z","iopub.status.busy":"2023-03-18T23:14:23.607154Z","iopub.status.idle":"2023-03-18T23:14:26.453345Z","shell.execute_reply":"2023-03-18T23:14:26.452345Z","shell.execute_reply.started":"2023-03-18T23:14:23.607531Z"},"trusted":true},"outputs":[],"source":["subvolume_batch, label_batch = train_ds[1]\n","print(f\"subvolume shape: {subvolume_batch.shape}\")\n","print(f\"label_batch shape: {label_batch.shape}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Check dataset throughput\n","\n","It's always a good idea to check that your data pipeline is efficient. You don't want to be CPU-bound at training time!"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:14:26.45912Z","iopub.status.busy":"2023-03-18T23:14:26.455968Z","iopub.status.idle":"2023-03-18T23:14:31.187401Z","shell.execute_reply":"2023-03-18T23:14:31.186334Z","shell.execute_reply.started":"2023-03-18T23:14:26.459076Z"},"trusted":true},"outputs":[],"source":["# t0 = time.time()\n","# n = 200\n","# for _ in train_ds:\n","#     pass\n","# print(f\"Time per batch: {(time.time() - t0) / n:.4f}s\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Create validation dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:14:31.189186Z","iopub.status.busy":"2023-03-18T23:14:31.188813Z","iopub.status.idle":"2023-03-18T23:14:31.263309Z","shell.execute_reply":"2023-03-18T23:14:31.262192Z","shell.execute_reply.started":"2023-03-18T23:14:31.189143Z"},"trusted":true},"outputs":[],"source":["val_locations_stride = BUFFER\n","val_locations = []\n","for x in range(val_location[0], val_location[0] + val_zone_size[0], val_locations_stride):\n","    for y in range(val_location[1], val_location[1] + val_zone_size[1], val_locations_stride):\n","        val_locations.append((x, y))\n","\n","# Convert the list of val locations to a PyTorch tensor\n","val_locations_ds = np.stack(val_locations, axis=0)\n","\n","# Create an instance of the SubvolumeDataset\n","val_ds = SubvolumeDataset(val_locations_ds, volume, labels, BUFFER, is_train=False)\n","\n","# Create a DataLoader with the dataset\n","val_dataloader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Visualize validation dataset patches\n","\n","Note that they are partially overlapping, since the stride is half the patch size."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:14:31.265448Z","iopub.status.busy":"2023-03-18T23:14:31.264916Z","iopub.status.idle":"2023-03-18T23:14:35.752995Z","shell.execute_reply":"2023-03-18T23:14:35.751869Z","shell.execute_reply.started":"2023-03-18T23:14:31.265399Z"},"trusted":true},"outputs":[],"source":["fig, ax = plt.subplots()\n","ax.imshow(labels)\n","\n","for y, x in val_locations_ds:\n","    patch = patches.Rectangle([x - BUFFER, y - BUFFER], 2 * BUFFER, 2 * BUFFER, linewidth=2, edgecolor='g', facecolor='none')\n","    ax.add_patch(patch)\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Compute a trivial baseline\n","\n","This is the highest validation score you can reach without looking at the inputs.\n","The model can be considered to have statistical power only if it can beat this baseline."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:14:35.755036Z","iopub.status.busy":"2023-03-18T23:14:35.754646Z","iopub.status.idle":"2023-03-18T23:14:38.733818Z","shell.execute_reply":"2023-03-18T23:14:38.73285Z","shell.execute_reply.started":"2023-03-18T23:14:35.754998Z"},"trusted":true},"outputs":[],"source":["def trivial_baseline(dataset):\n","    total = 0\n","    matches = 0.\n","    for _, batch_label in tqdm(dataset):\n","        batch_label = torch.tensor(batch_label)\n","        matches += torch.sum(batch_label.float())\n","        total += torch.numel(batch_label)\n","    return 1. - matches / total\n","\n","score = trivial_baseline(val_ds).item()\n","print(f\"Best validation score achievable trivially: {score * 100:.2f}% accuracy\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Augment the training data"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Train a Keras model\n","\n","This model is a U-Net taken from [this segmentation tutorial](https://keras.io/examples/vision/oxford_pets_image_segmentation/).\n","\n","`model.fit()` goes brrrrr\n","\n","Conceptually it looks like this (animation from [this tutorial](https://www.kaggle.com/code/jpposma/vesuvius-challenge-ink-detection-tutorial)):\n","\n","![animation](https://user-images.githubusercontent.com/22727759/224853385-ed190d89-f466-469c-82a9-499881759d57.gif)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchmetrics import Accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class DiceLoss(nn.Module):\n","    def __init__(self, smooth=1e-5):\n","        super(DiceLoss, self).__init__()\n","        self.smooth = smooth\n","\n","    def forward(self, preds, targets):\n","        preds = preds.view(-1)  # Flatten the tensor\n","        targets = targets.view(-1)  # Flatten the tensor\n","\n","        intersection = (preds * targets).sum()\n","        union = preds.sum() + targets.sum()\n","\n","        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n","\n","        return 1 - dice"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class CombinedLoss(nn.Module):\n","    def __init__(self, alpha=0.5, beta=0.5, smooth=1e-5):\n","        super(CombinedLoss, self).__init__()\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.dice_loss = DiceLoss(smooth)\n","        self.bce_loss = nn.BCELoss()\n","\n","    def forward(self, preds, targets):\n","        dice_loss = self.dice_loss(preds, targets)\n","        bce_loss = self.bce_loss(preds, targets)\n","        return self.alpha * dice_loss + self.beta * bce_loss\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","class UNet(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(UNet, self).__init__()\n","\n","        def conv_block(in_channels, out_channels):\n","            return nn.Sequential(                \n","                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(out_channels),\n","                nn.ReLU(),\n","            )\n","\n","        def transpose_conv_block(in_channels, out_channels):\n","            return nn.Sequential(                \n","                nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(out_channels),\n","                nn.ReLU(),\n","            )\n","            \n","        self.first_conv = nn.Sequential(\n","            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","        )\n","\n","        self.encoder = nn.ModuleList([\n","            nn.Sequential(\n","                nn.Conv2d(2**(i + 5), 2**(i + 6), kernel_size=3, stride=2, padding=1),\n","                nn.BatchNorm2d(2**(i + 6)),\n","                nn.ReLU(),\n","                nn.Conv2d(2**(i + 6), 2**(i + 6), kernel_size=3, padding=1),\n","                nn.BatchNorm2d(2**(i + 6)),\n","                nn.ReLU(),\n","            )\n","            for i in range(1, 4)\n","        ])\n","\n","\n","        self.middle = nn.Sequential(\n","            conv_block(512, 1024),\n","            conv_block(1024, 512),\n","        )\n","        \n","        self.decoder = nn.ModuleList([\n","            nn.Sequential(\n","                transpose_conv_block(2 ** (i + 7), 2 ** (i + 6)),\n","                transpose_conv_block(2 ** (i + 6), 2 ** (i + 5)),\n","                nn.Upsample(scale_factor=2, mode=\"nearest\"),\n","            )\n","            for i in range(3, 0, -1)\n","        ])\n","        self.final_decoder = nn.Sequential(\n","            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n","            # nn.BatchNorm2d(64),\n","            # nn.ReLU(),\n","            nn.Conv2d(64, out_channels, kernel_size=3, padding=1),\n","            # nn.BatchNorm2d(out_channels),\n","        )\n","        self.activation = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        # print(\"input:\", x)\n","        skip_connections = []\n","        x = self.first_conv(x)\n","        skip_connections.append(x)\n","        for layer in self.encoder:\n","            x = layer(x)\n","            skip_connections.append(x)\n","        # print(\"encoder ok\", x.shape)\n","\n","        x = self.middle(x)\n","        \n","        # print(\"middle ok\", x.shape)\n","        for i, layer in enumerate(self.decoder):                        \n","            skips = skip_connections.pop()\n","            # print(f\"decoder before skip connection: {i}: ok\", x.shape, skips.shape)\n","            x = torch.cat([x, skips], dim=1)  # Concatenate along channel dimension\n","            # print(f\"decoder with skip connection {i}: ok\", x.shape)            \n","            x = layer(x)\n","            # print(f\"decoder {i}: ok\", x.shape)\n","        # print(\"decoder ok\")\n","        x = torch.cat([x, skip_connections[0]], dim=1)\n","        x = self.final_decoder(x)\n","        x = self.activation(x)\n","        # print(\"final out\", x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:14:39.034124Z","iopub.status.busy":"2023-03-18T23:14:39.033748Z","iopub.status.idle":"2023-03-18T23:28:08.685364Z","shell.execute_reply":"2023-03-18T23:28:08.684324Z","shell.execute_reply.started":"2023-03-18T23:14:39.034086Z"},"trusted":true},"outputs":[],"source":["import os\n","import torch.optim.lr_scheduler\n","\n","# Define the model\n","model = UNet(Z_DIM, 1)\n","model = nn.DataParallel(model)\n","\n","if os.path.exists(f\"{DATA_DIR}/model.pt\"):\n","    # model.load_state_dict(torch.load(f\"{DATA_DIR}/model.pt\"))\n","    pass\n","\n","# Mixed precision training\n","# scaler = torch.cuda.amp.GradScaler(enabled=USE_MIXED_PRECISION)\n","\n","# Training loop\n","num_epochs = 30\n","\n","# Loss, optimizer, and metric\n","criterion = CombinedLoss()\n","optimizer = optim.Adam(model.parameters())\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","criterion = criterion.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for epoch in tqdm(range(1, num_epochs + 1)):\n","    model.train()\n","    running_loss = 0.0\n","    running_accuracy = 0.0\n","\n","    for idx, (subvolumes, labels) in tqdm(enumerate(train_dataloader)):\n","        subvolumes, labels = subvolumes.float().to(device), labels.float().to(device)        \n","        labels = labels.squeeze(dim=1)\n","        # print(\"torch.unique(subvolumes)\", torch.unique(subvolumes), file=open(\"subvolumes_unique\", \"w\"))\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(subvolumes).squeeze(dim=1)\n","        # print(\"outputs\", outputs)\n","        # print(\"outputs\", outputs.shape, \"labels\", labels.shape)\n","        loss = criterion(outputs, labels)  \n","        pred = torch.where(outputs.gt(threshold), 1, 0)            \n","        acc = (pred == labels).float().mean()\n","\n","        loss.backward()\n","        optimizer.step()    \n","\n","        running_loss += loss.item()\n","        running_accuracy += acc\n","        \n","    # print(\"train output\", outputs)\n","        \n","    scheduler.step()    \n","        \n","    running_loss /= len(train_dataloader)\n","    running_accuracy /= len(train_dataloader)\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0.0\n","    val_accuracy = 0.0\n","    with torch.no_grad():\n","        for subvolumes, labels in tqdm(val_dataloader):\n","            subvolumes, labels = subvolumes.float().to(device), labels.float().to(device)\n","            labels = labels.squeeze(dim=1)\n","            outputs = model(subvolumes).squeeze(dim=1)\n","            loss = criterion(outputs, labels)\n","            pred = torch.where(outputs.gt(threshold), 1, 0)            \n","            acc = (pred == labels).float().mean()\n","\n","            val_loss += loss.item()\n","            val_accuracy += acc\n","            \n","        # print(\"val outputs\", outputs)\n","\n","    val_loss /= len(val_dataloader)\n","    val_accuracy /= len(val_dataloader)\n","\n","    print(f\"Epoch [{epoch}/{num_epochs}] Loss: {running_loss:.4f} Accuracy: {running_accuracy:.4f} Val_Loss: {val_loss:.4f} Val_Accuracy: {val_accuracy:.4f}\")\n","\n","    if epoch % 10 == 0:\n","        torch.save(model.state_dict(), f\"model_{epoch}.pt\")\n","    if epoch == num_epochs:\n","        torch.save(model.state_dict(), f\"model.pt\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Clear up memory"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# del volume\n","# del mask\n","# del labels\n","# del train_ds\n","# del val_ds\n","\n","import gc\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:28:08.687464Z","iopub.status.busy":"2023-03-18T23:28:08.687065Z","iopub.status.idle":"2023-03-18T23:28:09.709853Z","shell.execute_reply":"2023-03-18T23:28:09.708782Z","shell.execute_reply.started":"2023-03-18T23:28:08.687423Z"},"trusted":true},"outputs":[],"source":["model = UNet(Z_DIM, 1)\n","model = nn.DataParallel(model)\n","model.load_state_dict(torch.load(\"model.pt\"))\n","model = model.to(device)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Compute predictions on test data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["a = np.arange(10)\n","a[:6][:]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:28:09.711867Z","iopub.status.busy":"2023-03-18T23:28:09.711493Z","iopub.status.idle":"2023-03-18T23:28:09.724755Z","shell.execute_reply":"2023-03-18T23:28:09.723509Z","shell.execute_reply.started":"2023-03-18T23:28:09.711827Z"},"trusted":true},"outputs":[],"source":["\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","from tqdm import tqdm\n","\n","def compute_predictions_map(split, index):\n","    \n","    print(f\"Load data for {split}/{index}\")\n","\n","    test_volume = load_volume(split=split, index=index)\n","    test_mask = load_mask(split=split, index=index)    \n","\n","    test_locations = []\n","    stride = BUFFER // 2\n","    for y in range(BUFFER, test_volume.shape[0] - BUFFER, stride):\n","        for x in range(BUFFER, test_volume.shape[1] - BUFFER, stride):\n","            test_locations.append((y, x))\n","\n","    print(f\"{len(test_locations)} test locations (before filtering by mask)\")\n","\n","    # filter locations inside the mask\n","    test_locations = [loc for loc in test_locations if is_in_masked_zone(loc, test_mask)]\n","    \n","    print(f\"{len(test_locations)} test locations (after filtering by mask)\")\n","\n","    test_ds = SubvolumeDataset(test_locations, test_volume, None, BUFFER, is_train=False, return_location=True)\n","    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, num_workers=num_workers)\n","\n","    # shape: (X, Y, C)\n","    predictions_map = np.zeros_like(test_volume[:, :, 0]).transpose((1, 0))[:, :, np.newaxis].astype(np.float64)\n","    \n","    print(\"test_volume.shape\", test_volume.shape)\n","    print(\"predictions_map.shape\", predictions_map.shape)\n","\n","    print(f\"Compute predictions\")\n","\n","    model.eval()  # set model to evaluation mode\n","    with torch.no_grad():    \n","        for patch_batch, loc_batch in tqdm(test_loader):\n","            loc_batch = loc_batch.to(device).long()\n","            patch_batch = patch_batch.to(device).float()\n","            predictions = model(patch_batch)\n","            # print(\"predictions\", predictions)\n","            print(\"predictions\", predictions[:, :, 0, 0])\n","            # print(\"Softmaxed predictions where conf is gt threshold\", predictions[predictions.gt(threshold)])\n","            # →(BATCH, W, H, C)\n","            predictions = torch.permute(predictions, (0, 3, 2, 1))\n","            predictions = predictions.cpu().numpy()  # move predictions to cpu and convert to numpy\n","            for (y, x), pred in zip(loc_batch, predictions):\n","                # print(\"index: \", index ,\"x, y, pred\", x.item(), y.item(), pred[BUFFER, BUFFER, :].item(), file=open('log.out', 'a'))\n","                predictions_map[\n","                    x - BUFFER : x + BUFFER, y - BUFFER : y + BUFFER, :\n","                ][pred > threshold] = 1\n","    print(\"predictions_map\", predictions_map, file=open(\"predictions_map\", \"w\"))\n","    return predictions_map\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from skimage.transform import resize as resize_ski\n","import pathlib"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:28:09.727518Z","iopub.status.busy":"2023-03-18T23:28:09.726481Z","iopub.status.idle":"2023-03-18T23:35:39.938224Z","shell.execute_reply":"2023-03-18T23:35:39.937169Z","shell.execute_reply.started":"2023-03-18T23:28:09.727465Z"},"trusted":true},"outputs":[],"source":["def rle(predictions_map, threshold):\n","    flat_img = (np.where(predictions_map.flatten() >= threshold, 1, 0)).astype(np.uint8)\n","    \n","    # Add padding at the beginning and end\n","    flat_img = np.pad(flat_img, pad_width=1, mode='constant', constant_values=0)\n","\n","    starts = np.where((flat_img[:-1] == 0) & (flat_img[1:] == 1))[0]\n","    ends = np.where((flat_img[:-1] == 1) & (flat_img[1:] == 0))[0]\n","\n","    lengths = ends - starts\n","\n","    return \" \".join(map(str, np.c_[starts, lengths].flatten()))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def update_submission(predictions_map, index):\n","    rle_ = rle(predictions_map, threshold=threshold)\n","    print(f\"{index},\" + rle_, file=open('submission.csv', 'a'))\n","    # print(f\"{index},\" + rle_, file=open('/kaggle/working/submission.csv', 'a'))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Resize prediction maps to their original size (for submission)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:35:44.578704Z","iopub.status.busy":"2023-03-18T23:35:44.578139Z","iopub.status.idle":"2023-03-18T23:35:49.82279Z","shell.execute_reply":"2023-03-18T23:35:49.821723Z","shell.execute_reply.started":"2023-03-18T23:35:44.578662Z"},"trusted":true},"outputs":[],"source":["print(\"Id,Predicted\", file=open('submission.csv', 'w'))\n","kind = \"train\"\n","folder = pathlib.Path(DATA_DIR) / kind\n","for p in list(folder.iterdir()):\n","    index = p.stem\n","    predictions_map = compute_predictions_map(split=kind, index=index)\n","    original_size = cv2.imread(DATA_DIR + f\"/{kind}/{index}/mask.png\", 0).shape[:2]\n","    # W, H, C → H, W, C\n","    predictions_map = predictions_map.transpose((1, 0, 2))    \n","    predictions_map = resize_ski(predictions_map, (original_size[0], original_size[1], 1)).squeeze(axis=-1)    \n","    print(\"original predictions_map size\", predictions_map.shape)    \n","    # H, W → W, H\n","    update_submission(predictions_map, index)\n","    plt.imsave(f\"{index}.png\", predictions_map, cmap=\"gray\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["predictions_map.shape, predictions_map.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["predictions_map"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":4}
