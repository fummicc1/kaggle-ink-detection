{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Keras starter kit [full training set, UNet]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Note\n","\n","- 高い解像度でリサイズすることはprecisionの向上につながるため有効\n","- seresnextでチャネル間の相関を見れるので有効\n","- 文字の太さ・書き方に大きくバリアントがあるので、横の相関よりもdepthの相関の方が大事かもしれない\n","- 深さに関して、隣り合う深さ同士に大きな変化はない\n","- fpをfnよりも小さくしたい\n","- valid_scoreはCFG.thd依存\n","- valid_lossはCFG.loss1/loss2依存\n","- encoder内でdepthをクロップしてバッチで繋げた方が精度が良い\n","- maskに対しては有効ではないが、labelは的確なラベルを用いることで精度が向上\n","- depthは22~34 or 24 ~ 36\n","    - 固めるより2skipで広げたほうが良い\n","- 画像のサイズは大きい方が良いのか？（BUFFER / SHARED_HEIGHT）\n","    - 比率を同じにして試してみる\n","    - SHARED_HEIGHTをデカくするとデータがメモリに載らなさそうだった\n","- NetのweightsにBatchNormはない方が良い\n","- BUFFER:strideを160:96から160:80にしたら精度が落ちた\n","    - trainの精度は上がっていた\n","    - BUFFERに対してstrideが細かすぎると過学習に繋がっているのかもしれない\n","    - P.S. strideを小さくすることで、学習時のバイアスが減るので良い学習ができる\n","- 文字が見えなくなるよりも、高いthdを設定して、文字が大きく見えた方が良い（仮説）\n","- バイリニアよりもバイキュービックが良い\n","    - 計算コストが気になるところ\n","- ノイズについて\n","    - 大きすぎると良くない(intensityが(0.0001, 0.0005)くらいが良かった)\n","        - 元画像が255 * 255の範囲であることを踏まえると、1/255の変化で255のずれがある\n","- lossのαとβについて\n","    - 理論的にはαは大きい方が良い（偽陽性fpはスコアを大きく下げるので）\n","    - しかし、thdを用いて偽陽性を防ぐ手法もあるため、学習時にはあえて偽陰性に強いペナルティを与えた方が良いのかもしれない\n","    - Stacked UnetのI層目では高いβで見落としをなくして、二層目で高いαにしてノイズを除去する作戦\n","- 文字サイズとリサイズサイズについて\n","- thresholdを変えるだけでpublic lbが大きく変わる\n","    - 0.5から0.7にしたら0.05上がった（0.58→0.63）\n","- 学習時・推論時に低いRESIZED_HEIGHTで進めると、提出時にcvとlbに差が出るかもしれない。これは最終的にリサイズする際に細かい部分の見落としが発生するからだと思う\n","    - 良い学習をするために、正しいラベル・正しい入力が適切→resizeすることでデータの質が損なわれているのかも\n","- [ ] 3dcnnにbatch化したdepthを横に繋げる実装が使えるか考える\n","- [ ] Stacked Unetを3dcnnに適用してみる\n","- 解像度（BUFFER）はある程度大きくないといけない（<112）\n","- DecoderBlockのattention=scseについて\n","- 3dresnetで見る場合、最初のconv1とmaxpoolで解像度が1/4に縮小されるので、元が128x128であれば32x32をUpScaleすることになる。なので、元々ある程度高い解像度が必要\n","    - これは2次元encoderでも同様のことが言えるはず\n","- 3dresnetで見る場合、Z_LISTは2飛ばしで広く見る方が精度が高い\n","- ampを有効にする（precision=16）\n","- 3dresnetで学習時の精度が良くならない\n","    - 解像度が低い（仮説）\n","    - agumentationが難しすぎる（仮説）\n","        - augmentationの確率を下げても変化はなかった\n","        - augmentationをなくすとどうなる？\n","            - 減らすとtrainの精度が上がるが、valはlossが下がらない現象が発生\n","                - それでもtrainの精度は60%程度で停滞\n","    - KMが有効に見える\n","- pretrainedのモデルの種類：K,KM\n","- 3次元においてもStacked UNetは有効（以下、比較）\n","    - https://wandb.ai/fummicc1/lightning_logs/runs/kylnuh3w?workspace=user-fummicc1\n","    - https://wandb.ai/fummicc1/lightning_logs/runs/jr49frri?workspace=user-fummicc1\n","\n","- augmentationを無くしたら学習時の精度は上がった\n","    - https://wandb.ai/fummicc1/lightning_logs/runs/rdmdfcm4?workspace=user-fummicc1\n","    - https://wandb.ai/fummicc1/lightning_logs/runs/ry6jtkkp?workspace=user-fummicc1\n","- fragment1もaとbに分割してみる（5fold）\n","- augmentation\n","    - transposeが微妙かも？\n","        - でも、正方形の入力画像でtransposeするのはHorizontal/VerticalのFlipと同値に感じる\n","    - Flip系はOK\n","    - なぜかPadIfNeededが検証データに対して効いている\n","- BCELossだと学習が不安定になった\n","    - https://wandb.ai/fummicc1/lightning_logs/runs/1byzvdec?workspace=user-fummicc1\n","    - https://wandb.ai/fummicc1/lightning_logs/runs/pmqpfwcg?workspace=user-fummicc1\n","- ラベルの値が0,1の場合はSoftBCEとBCEに違いはない\n","- BCELossが有効だった\n","    - 不均一性を考慮して（あとpositiveがほしいので）、pos_weightを1以上にする\n","- Rotate TTAは学習時にRotateのaugmentationをしているなら意味がなさそう\n","- Z_DIFFは4より6が良い\n","- Pooling Attentionのattention_typeはscseとNoneのどちらが良いのか\n","    - Noneの方が良さそう\n","        - https://wandb.ai/fummicc1/lightning_logs/runs/6dewm37h?workspace=user-fummicc1\n","        - https://wandb.ai/fummicc1/lightning_logs/runs/vyj9isbf?workspace=user-fummicc1\n","\n","- 解像度は高いから良いというわけではない\n","    - https://wandb.ai/fummicc1/lightning_logs/runs/6zchova0?workspace=user-fummicc1\n","    - https://wandb.ai/fummicc1/lightning_logs/runs/vyj9isbf?workspace=user-fummicc1\n","\n","- CROP_DEPTHは重ねた方が良いかも\n","- MultiChannelNoiseはない方が良い\n","\n","- Z_LISTについて\n","    - 学習した結果をvalで活かすには、同じ層あたりで同じ特徴量を持つ必要がある\n","        - しかし、fragmentによっては前半に集まっている・後半に集まっているがバラバラ\n","        - より普遍的な成果を出すには中央の層を集中して使った方が良いかも\n","\n","- lr_schedulerの違い\n","- 解像度は高い方が良い\n","    - Z_LISTは意味のある階層だけ見るでOK\n","\n","- エッジ部分の識別が甘い\n","    - mask_paddingしているため\n","\n","- submission時のdenoiseはskimageのresizeを先にかけた方が良い\n","- batchsize = 24 でbatchnorm必要？\n","    - batch正規化はあった方が良い\n","    - https://wandb.ai/fummicc1/lightning_logs/runs/l2f4pbad?workspace=user-fummicc1\n","    - https://wandb.ai/fummicc1/lightning_logs/runs/qukacva9?workspace=user-fummicc1\n","\n","- リサイズをしないでpadで高さを調整して学習させたらTest Fragmentに最適化されて精度が上がった"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:10:40.298967Z","iopub.status.busy":"2023-03-18T23:10:40.298008Z","iopub.status.idle":"2023-03-18T23:10:49.514216Z","shell.execute_reply":"2023-03-18T23:10:49.513058Z","shell.execute_reply.started":"2023-03-18T23:10:40.298921Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import wandb\n","import torchvision\n","import datetime\n","import imageio\n","\n","# import cupy\n","from sklearn.model_selection import KFold\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","import pytorch_lightning\n","import segmentation_models_pytorch as smp\n","import pytorch_lightning as pl\n","import pytorch_lightning.callbacks.model_checkpoint\n","import pytorch_lightning.plugins\n","from skimage.transform import resize as resize_ski\n","from pytorch_lightning.strategies.ddp import DDPStrategy\n","from pytorch_lightning.loggers import WandbLogger\n","from einops import rearrange, reduce, repeat\n","import torch.nn.functional as F\n","import segmentation_models_pytorch as smp\n","from segmentation_models_pytorch.decoders.unet.decoder import UnetDecoder, DecoderBlock\n","from timm.models.resnet import (\n","    resnet10t,\n","    resnet34d,\n","    resnet50d,\n","    resnet14t,\n","    seresnext26d_32x4d,\n","    seresnext50_32x4d,\n","    seresnext26tn_32x4d,\n",")\n","from timm.models.mvitv2 import mvitv2_base\n","import os\n","import torch.utils.data\n","from dataclasses import dataclass\n","\n","from scipy.ndimage import distance_transform_edt\n","\n","import ssl\n","\n","ssl._create_default_https_context = ssl._create_unverified_context\n","\n","import glob\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import cv2\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data\n","import os, cv2\n","import gc\n","import sys\n","import matplotlib.patches as patches\n","import random\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import pytorch_lightning as pl\n","\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.cuda import amp\n","from torch.utils.data import Dataset, DataLoader\n","import segmentation_models_pytorch as smp\n","import albumentations as A\n","from IPython.display import Video\n","\n","pytorch_lightning.seed_everything(seed=42)\n","torch.set_float32_matmul_precision(\"high\")\n","\n","\n","@dataclass\n","class CFG:\n","    # Data config\n","    # DATA_DIR = '/kaggle/input/vesuvius-challenge-ink-detection/'\n","    DATA_DIR = \"/home/fummicc1/codes/competitions/kaggle-ink-detection\"\n","    # DATA_DIR = '/home/fummicc1/codes/Kaggle/kaggle-ink-detection'\n","    BUFFER = 192  # Half-size of papyrus patches we'll use as model inputs\n","    CROP_SIZE = BUFFER * 2 # 384px\n","    STRIDE = 192\n","    Z_LIST = list(range(20, 40, 2)) # 10チャネル\n","    # Z_LIST = list(range(4, 16, 2)) + list(range(16, 36)) + list(range(36, 44, 2))\n","    Z_DIM = len(\n","        Z_LIST\n","    )  # Number of slices in the z direction. Max value is 64 - Z_START\n","    BATCH_Z_DIFF = 4\n","    BATCH_Z_INFO = [\n","        # (start, end)\n","        (0, 4),\n","        (2, 6),\n","        (4, 8),\n","        (6, 10),\n","    ]\n","    SHARED_HEIGHT = None  # Max height to resize all papyrii\n","\n","    # Model config\n","    BATCH_SIZE = 32\n","\n","    GPU_COUNT = 4\n","\n","    device = torch.device(\"cuda\")\n","    threshold = 0.5\n","    num_workers = 8\n","    exp = 1e-7\n","    mask_padding = (SHARED_HEIGHT // 80) if SHARED_HEIGHT is not None else 0\n","\n","    num_epochs = 40\n","    lr = 1e-3 # initial lr\n","    lr_scheduler_name = \"CosineAnnealingLR\"\n","    eta_min_lr = 1e-5\n","    WANDB_NOTE = \"mask_paddingを0に変更. PixelShuffleを除去\"\n","    augmentation_names = [\n","        \"Horizontal\",\n","        \"Vertical\",\n","        \"RandomScale\",\n","        \"Transpose\",\n","        \"RandomRotate\",\n","        \"ShiftScaleRotate\",\n","        # \"Blur\",\n","        \"GridDistortion\",\n","        # \"MultiChannelNoise\",\n","        \"PadIfNeeded\",\n","        \"Resize\",\n","        \"MixedUp\",\n","    ]\n","    \n","    init_weight_bias = True\n","    \n","    mixed_up_alpha = 0.8\n","    mixed_up_probability = 0.35\n","    start_mixedup_epochs = num_epochs - 15\n","    stop_mixedup_epochs = num_epochs - 7\n","    \n","    loss1_alpha = 0.6\n","    loss1_beta = None\n","\n","    loss2a_alpha = 1.5\n","    loss2a_beta = None\n","\n","    loss2b_alpha = 1.5\n","\n","    loss1_weight = 0.5\n","    loss2_weight = 0.5\n","\n","    loss2a_weight = 0.5\n","    loss2b_weight = 0.5\n","\n","    lr_scheduler = optim.lr_scheduler.CosineAnnealingLR\n","    optimizer = \"AdamW\"\n","    # loss1 = smp.losses.TverskyLoss(\n","    #     smp.losses.BINARY_MODE,\n","    #     log_loss=False,\n","    #     from_logits=True,\n","    #     smooth=1e-7,\n","    #     alpha=loss1_alpha,\n","    #     beta=loss1_beta,\n","    # )\n","    loss1 = smp.losses.SoftBCEWithLogitsLoss(\n","        pos_weight=torch.tensor([loss1_alpha]),\n","    )\n","    # loss2a = smp.losses.TverskyLoss(\n","    #     smp.losses.BINARY_MODE,\n","    #     log_loss=False,\n","    #     from_logits=True,\n","    #     smooth=1e-7,\n","    #     alpha=loss2a_alpha,\n","    #     beta=loss2a_beta,\n","    # )\n","    loss2a = smp.losses.SoftBCEWithLogitsLoss(\n","        pos_weight=torch.tensor([loss2a_alpha]),\n","    )\n","    loss2b = smp.losses.SoftBCEWithLogitsLoss(\n","        pos_weight=torch.tensor([loss2b_alpha]),\n","    )\n","    noise_intensity = (0.000025, 0.00005)\n","    \n","    mean = 0.42\n","    std = 0.3\n","    \n","    use_new_label_mask = True\n","    pretrained = True\n","    SKIP_ZERO_ON_TRAIN = False\n","    model_name = \"seresnext26d_32x4d\"\n","    # MODEL_DEPTH = 34\n","    # MODEL_KIND = \"KM\"\n","    \n","    precision = \"float32\" # float16\n","\n","    interpolation = \"bicubic\" # PixelShuffling\n","    use_pixel_shuffle = False\n","\n","    exp_name = \"114-gpu17\"\n","    prev_exp_name = \"113-gpu17\"\n","\n","\n","def class2dict(c):\n","    return {\n","        attr: getattr(c, attr)\n","        for attr in dir(c)\n","        if not callable(getattr(c, attr)) and not attr.startswith(\"__\")\n","    }"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Load up the training data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:10:52.653587Z","iopub.status.busy":"2023-03-18T23:10:52.653064Z","iopub.status.idle":"2023-03-18T23:10:57.915118Z","shell.execute_reply":"2023-03-18T23:10:57.914028Z","shell.execute_reply.started":"2023-03-18T23:10:52.653544Z"},"trusted":true},"outputs":[],"source":["def resize(img):\n","    current_height, current_width = img.shape    \n","    aspect_ratio = current_width / current_height\n","    if CFG.SHARED_HEIGHT is None:\n","        return img\n","    # new_height = CFG.SHARED_HEIGHT\n","    # pad_y = new_height - current_height\n","    # if pad_y > 0:\n","    #     # 元画像が小さい場合は解像度を大きくしないでpaddingをつける\n","    #     img = np.pad(img, [(0, pad_y), (0, 0)], constant_values=0)\n","    # else:\n","    # 既に十分でかい場合はリサイズする\n","    # 本当はpaddingしたいけど、メモリサイズが大きくなる\n","    new_height = CFG.SHARED_HEIGHT\n","    new_width = int(CFG.SHARED_HEIGHT * aspect_ratio)\n","    new_size = (new_width, new_height)\n","    # (W, H)の順で渡すが結果は(H, W)になっている\n","    img = cv2.resize(img, new_size)\n","    return img\n","\n","def load_mask(split, index): \n","    if index == \"2a\" or index == \"2b\":\n","        mode = index[1]\n","        index = \"2\"\n","    img = cv2.imread(f\"{CFG.DATA_DIR}/{split}/{index}/mask.png\", 0) // 255\n","    if index == \"2\":\n","        h = 9456\n","        if mode == \"a\":\n","            img = img[h:, :]\n","        elif mode == \"b\":   \n","            img = img[:h, :]\n","    img = resize(img)\n","    img = np.pad(img, 1, constant_values=0)\n","    dist = distance_transform_edt(img)\n","    img[dist <= CFG.mask_padding] = 0\n","    img = img[1:-1, 1:-1]    \n","    return img\n","\n","\n","def load_labels(split, index):\n","    if index == \"2a\" or index == \"2b\":\n","        mode = index[1]\n","        index = \"2\"\n","    suffix = \"_new\" if CFG.use_new_label_mask else \"\"\n","    img = cv2.imread(f\"{CFG.DATA_DIR}/{split}/{index}/inklabels{suffix}.png\", 0) // 255    \n","    if index == \"2\":\n","        h = 9456\n","        if mode == \"a\":\n","            img = img[h:, :]\n","        elif mode == \"b\":   \n","            img = img[:h, :]\n","    img = resize(img)\n","    return img"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# input shape: (H, W, C)\n","def rotate90(volume: np.ndarray, k=None, reverse=False):    \n","    if k:\n","        volume = np.rot90(volume, k)\n","    else:\n","        volume = np.rot90(volume, 1 if not reverse else 3)\n","    height = volume.shape[0]\n","    width = volume.shape[1]\n","    new_height = CFG.SHARED_HEIGHT\n","    new_width = int(new_height * width / height)\n","    if len(volume.shape) == 2:\n","        return cv2.resize(volume, (new_width, new_height))\n","    return resize_ski(volume, (new_height, new_width, volume.shape[2]))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:11:02.854397Z","iopub.status.busy":"2023-03-18T23:11:02.85384Z","iopub.status.idle":"2023-03-18T23:11:02.867203Z","shell.execute_reply":"2023-03-18T23:11:02.866021Z","shell.execute_reply.started":"2023-03-18T23:11:02.854351Z"},"trusted":true},"outputs":[],"source":["def load_volume(split, index):\n","    if index == \"2a\" or index == \"2b\":\n","        mode = index[1]\n","        index = \"2\"\n","    # Load the 3d x-ray scan, one slice at a time\n","    all = sorted(glob.glob(f\"{CFG.DATA_DIR}/{split}/{index}/surface_volume/*.tif\"))\n","    z_slices_fnames = [all[i] for i in range(len(all)) if i in CFG.Z_LIST]\n","    assert len(z_slices_fnames) == CFG.Z_DIM\n","    z_slices = []\n","    for z, filename in  tqdm(enumerate(z_slices_fnames)):\n","        img = cv2.imread(filename, -1)\n","        if index == \"2\":\n","            h = 9456\n","            if mode == \"a\":\n","                img = img[h:, :]\n","            elif mode == \"b\":\n","                img = img[:h, :]\n","        img = resize(img)\n","        # img = (img / (2 ** 8)).astype(np.uint8)\n","        if CFG.precision == \"float16\":\n","            img = img.astype(np.float32) // 255\n","        elif CFG.precision == \"float32\":\n","            img = img.astype(np.float32) / 255\n","        else:\n","            assert False\n","        z_slices.append(img)\n","    return np.stack(z_slices, axis=-1)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Create a dataset in the input volume\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def is_in_masked_zone(location, mask):\n","    return mask[location[0], location[1]] > 0\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:14:17.958942Z","iopub.status.busy":"2023-03-18T23:14:17.958495Z","iopub.status.idle":"2023-03-18T23:14:18.45515Z","shell.execute_reply":"2023-03-18T23:14:18.454126Z","shell.execute_reply.started":"2023-03-18T23:14:17.958902Z"},"trusted":true},"outputs":[],"source":["def generate_locations_ds(volume, mask, label=None, skip_zero=False):\n","    is_in_mask_train = lambda x: is_in_masked_zone(x, mask)\n","\n","    # Create a list to store train locations\n","    locations = []\n","\n","    # Generate train locations\n","    volume_height, volume_width = volume.shape[:-1]\n","\n","    for y in range(CFG.BUFFER, volume_height - CFG.BUFFER, CFG.STRIDE):\n","        for x in range(CFG.BUFFER, volume_width - CFG.BUFFER, CFG.STRIDE):\n","            if skip_zero and CFG.SKIP_ZERO_ON_TRAIN and label is not None and np.all(label[y - CFG.BUFFER : y + CFG.BUFFER, x - CFG.BUFFER : x + CFG.BUFFER] == 0):\n","                # print(f\"skip location at (y: {y}, x: {x})\")\n","                continue\n","            if is_in_mask_train((y, x)):\n","                locations.append((y, x))\n","\n","    # Convert the list of train locations to a PyTorch tensor\n","    locations_ds = np.stack(locations, axis=0)\n","    return locations_ds"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Visualize some training patches\n","\n","Sanity check visually that our patches are where they should be."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def extract_subvolume(location, volume):\n","    global printed\n","    y = location[0]\n","    x = location[1]\n","    subvolume = volume[y-CFG.BUFFER:y+CFG.BUFFER, x-CFG.BUFFER:x+CFG.BUFFER, :].astype(np.float32)\n","    \n","    return subvolume"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## SubvolumeDataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import numpy as np\n","\n","def mixup_data(x, y):\n","    alpha = CFG.mixed_up_alpha\n","    if alpha > 0.:\n","        lam = np.random.beta(alpha, alpha)\n","    else:\n","        lam = 1.\n","\n","    batch_size = x.size()[0]\n","    index = torch.randperm(batch_size)\n","\n","    mixed_x = lam * x + (1 - lam) * x[index]    \n","    y_a, y_b = y, y[index]\n","    \n","    mixed_x = mixed_x.float()\n","    y_a = y_a.float()\n","    y_b = y_b.float()\n","    \n","    y = y_a * lam + y_b * (1 - lam)\n","\n","    return mixed_x, y\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","from albumentations.core.transforms_interface import ImageOnlyTransform\n","\n","class MultichannelNoise(ImageOnlyTransform):\n","\n","    def __init__(self, intensity=CFG.noise_intensity, always_apply=False, p=0.5):\n","        super().__init__(always_apply, p)\n","        self.intensity = intensity\n","\n","    def apply(self, img, **params):\n","        intensity = np.random.uniform(*self.intensity)\n","        noise = np.random.normal(loc=0, scale=intensity, size=img.shape)\n","        # print(\"img\", img)\n","        img = img + noise        \n","        return np.clip(img, 0, 1).astype(np.float32) # クリップして0から255の範囲に保つ\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def smooth_labels(labels, alpha=0.1):\n","    labels = labels * (1 - alpha) + alpha * 0.5\n","    return labels"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.preprocessing import OneHotEncoder\n","\n","from albumentations.core.transforms_interface import ImageOnlyTransform\n","\n","class SubvolumeDataset(Dataset):\n","    def __init__(self, locations, volume, labels, buffer, is_train: bool, return_location: bool = False):\n","        self.locations = locations\n","        self.volume = volume\n","        self.labels = labels        \n","        self.buffer = buffer\n","        self.is_train = is_train\n","        self.return_location = return_location\n","\n","    def __len__(self):\n","        return len(self.locations)\n","\n","    def __getitem__(self, idx):\n","        label = None\n","        location = np.array(self.locations[idx])\n","        y, x = location[0], location[1]\n","\n","        subvolume = extract_subvolume(location, self.volume)\n","        \n","        if self.labels is not None:\n","            label = self.labels[y - self.buffer:y + self.buffer, x - self.buffer:x + self.buffer]            \n","            label = np.stack([label], axis=-1)            \n","            \n","        # 段々meanは小さくなる\n","        mean = np.array([CFG.mean for i in range(0, CFG.Z_DIM)]).reshape(-1, 1, 1)\n","        # 段々stdは小さくなる\n","        std = np.array([CFG.std for i in range(0, CFG.Z_DIM)]).reshape(-1, 1, 1)\n","        \n","        if self.is_train and label is not None:    \n","            # label = smooth_labels(label, alpha=0.1)                  \n","            transformed = A.Compose([\n","                A.ToFloat(max_value=255),\n","                A.HorizontalFlip(p=0.5) if \"Horizontal\" in CFG.augmentation_names else A.Compose([]),\n","                A.VerticalFlip(p=0.5) if \"Vertical\" in CFG.augmentation_names else A.Compose([]),  \n","                A.RandomScale(p=0.4, scale_limit=0.35) if \"RandomScale\" in CFG.augmentation_names else A.Compose([]),\n","                A.Transpose(p=0.5) if \"Transpose\" in CFG.augmentation_names else A.Compose([]), \n","                A.RandomRotate90(p=0.5,) if \"RandomRotate\" in CFG.augmentation_names else A.Compose([]), \n","                A.ShiftScaleRotate(p=0.7, scale_limit=0.35, rotate_limit=45) if \"ShiftScaleRotate\" in CFG.augmentation_names else A.Compose([]),\n","                A.MotionBlur(p=0.2) if \"Blur\" in CFG.augmentation_names else A.Compose([]),\n","                A.GridDistortion(num_steps=5, distort_limit=0.2, p=0.3) if \"GridDistortion\" in CFG.augmentation_names else A.Compose([]),\n","                MultichannelNoise(\n","                    p=0.1,\n","                ) if \"MultichannelNoise\" in CFG.augmentation_names else A.Compose([]),\n","                A.PadIfNeeded(min_height=self.buffer * 2, min_width=self.buffer * 2) if \"PadIfNeeded\" in CFG.augmentation_names else A.Compose([]),\n","                A.Resize(height=self.buffer * 2, width=self.buffer * 2) if \"Resize\" in CFG.augmentation_names else A.Compose([]),\n","            ])(image=subvolume, mask=label)            \n","            subvolume = transformed[\"image\"]\n","            label = transformed[\"mask\"]\n","            subvolume = np.transpose(subvolume, (2, 0, 1))\n","            label = np.transpose(label, (2, 0, 1))\n","            subvolume = (subvolume - mean) / std            \n","        else:\n","            if label is None:\n","                subvolume = np.transpose(subvolume, (2, 0, 1))\n","                subvolume /= 255.\n","                subvolume = (subvolume - mean) / std\n","            else:\n","                # print(\"subvolume in val dataset (before aug)\", subvolume, file=open(\"before-val-aug.log\", \"w\")) \n","                subvolume = np.transpose(subvolume, (2, 0, 1))\n","                label = np.transpose(label, (2, 0, 1))\n","                subvolume /= 255.\n","                subvolume = (subvolume - mean) / std\n","        # print(\"subvolume\", subvolume)\n","        if self.return_location:\n","            return subvolume, location\n","        return subvolume, label        "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Visualize validation dataset patches\n","\n","Note that they are partially overlapping, since the stride is half the patch size."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:14:31.265448Z","iopub.status.busy":"2023-03-18T23:14:31.264916Z","iopub.status.idle":"2023-03-18T23:14:35.752995Z","shell.execute_reply":"2023-03-18T23:14:35.751869Z","shell.execute_reply.started":"2023-03-18T23:14:31.265399Z"},"trusted":true},"outputs":[],"source":["def visualize_dataset_patches(locations_ds, labels, mode: str, fold = 0):\n","    fig, ax = plt.subplots()\n","    ax.imshow(labels)\n","\n","    for y, x in locations_ds:\n","        patch = patches.Rectangle([x - CFG.BUFFER, y - CFG.BUFFER], 2 * CFG.BUFFER, 2 * CFG.BUFFER, linewidth=2, edgecolor='g', facecolor='none')\n","        ax.add_patch(patch)\n","    plt.savefig(f\"fold-{fold}-{mode}.png\")\n","    plt.show()    "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Compute a trivial baseline\n","\n","This is the highest validation score you can reach without looking at the inputs.\n","The model can be considered to have statistical power only if it can beat this baseline."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Dataset check"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ref - https://www.kaggle.com/competitions/vesuvius-challenge-ink-detection/discussion/397288\n","def fbeta_score(preds, targets, threshold, beta=0.5, smooth=1e-5):\n","    preds_t = torch.where(preds > threshold, 1.0, 0.0).float()\n","    y_true_count = targets.sum()\n","    \n","    ctp = preds_t[targets==1].sum()\n","    cfp = preds_t[targets==0].sum()\n","    beta_squared = beta * beta\n","\n","    c_precision = ctp / (ctp + cfp + smooth)\n","    c_recall = ctp / (y_true_count + smooth)\n","    dice = (1 + beta_squared) * (c_precision * c_recall) / (beta_squared * c_precision + c_recall + smooth)\n","\n","    return dice"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def init_subpixel(weight):\n","    co, ci, h, w = weight.shape\n","    co2 = co // 4\n","    # initialize sub kernel\n","    k = torch.empty([co2, ci, h, w])\n","    nn.init.kaiming_uniform_(k)\n","    # repeat 4 times\n","    k = k.repeat_interleave(4, dim=0)\n","    weight.data.copy_(k)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def init_linear(m, relu=True):\n","    if relu: nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n","    else: nn.init.xavier_uniform_(m.weight)\n","    if m.bias is not None: nn.init.zeros_(m.bias)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def initialize_weights(model: nn.Module):\n","    for module in model.modules():\n","        if isinstance(module, (nn.Conv2d, nn.Linear)):\n","            nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n","            if module.bias is not None:\n","                nn.init.constant_(module.bias, 0)\n","        elif isinstance(module, nn.BatchNorm2d):\n","            nn.init.constant_(module.weight, 1)\n","            nn.init.constant_(module.bias, 0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class SmpUnetDecoder(nn.Module):\n","\tdef __init__(\n","\t\tself,\n","\t\tin_channel,\n","\t\tskip_channel,\n","\t\tout_channel,\n","\t):\n","\t\tsuper().__init__()\n","\t\tself.center = nn.Identity()\n","\n","\t\ti_channel = [\n","\t\t\tin_channel,\n","\t\t] + out_channel[:-1]\n","\t\ts_channel = skip_channel\n","\t\to_channel = out_channel\n","\t\t# print(\"i:\", i_channel)\n","\t\t# print(\"s:\", s_channel)\n","\t\t# print(\"o:\", o_channel)\n","\t\tblock = [\n","\t\t\tDecoderBlock(i, s, o, use_batchnorm=True, attention_type=None) # TODO: attention_typeをscseに変更してみる\n","\t\t\tfor i, s, o in zip(i_channel, s_channel, o_channel)\n","\t\t]\n","\t\tself.block = nn.ModuleList(block)\n","\t\tif CFG.init_weight_bias:\n","\t\t\tinitialize_weights(self)\n","\n","\tdef forward(self, feature, skip):\n","\t\t# for s in skip:\n","\t\t\t# print(\"feature\", feature.shape, \"skip\", s.shape)\n","\t\td = self.center(feature)\n","\t\tdecode = []\n","\t\tfor i, block in enumerate(self.block):\n","\t\t\ts = skip[i]\n","\t\t\td = block(d, s)\n","\t\t\tdecode.append(d)\n","\n","\t\tlast = d\n","\t\treturn last, decode\n","\n","class UpsampleShuffle(nn.Sequential):\n","\tdef __init__(self, in_channels, out_channels):\n","\t\tsuper().__init__(\n","\t\t\tnn.Conv2d(in_channels, out_channels * 4, kernel_size=1),\n","\t\t\tnn.ReLU(inplace=True),\n","\t\t\tnn.PixelShuffle(2)\n","\t\t)\n","\t\tif CFG.init_weight_bias:\n","\t\t\tinitialize_weights(self)\n","\n","class Decoder(nn.Module):\n","\tdef __init__(self, encoder_dims):\n","\t\tsuper().__init__()\n","\t\tself.encoder_dims = encoder_dims\n","\t\tif CFG.use_pixel_shuffle:\n","\t\t\tself.f_up_list = nn.ModuleList(\n","\t\t\t\t[UpsampleShuffle(d, d) for d in self.encoder_dims]\n","\t\t\t)\n","\t\t\tif CFG.init_weight_bias:\n","\t\t\t\tfor weight in self.f_up_list:        \n","\t\t\t\t\tinit_subpixel(weight)\n","\t\tself.convs = nn.ModuleList([\n","\t\t\tnn.Sequential(\n","\t\t\t\tnn.Conv2d(encoder_dims[i]+encoder_dims[i-1], encoder_dims[i-1], 3, 1, 1, bias=False),\n","\t\t\t\tnn.BatchNorm2d(encoder_dims[i-1]),\n","\t\t\t\tnn.ReLU(inplace=True),\t\t\t\t\n","\t\t\t) for i in range(1, len(encoder_dims))])\n","\t\tif CFG.init_weight_bias:\n","\t\t\tinitialize_weights(self)\n","\n","\tdef forward(self, feature_maps):\n","\t\tfor i in range(len(feature_maps)-1, 0, -1):\n","\t\t\t# print(\"index:\", i, feature_maps[i].shape)\n","\t\t\tif CFG.use_pixel_shuffle:\n","\t\t\t\tf_up = self.f_up_list[i](feature_maps[i])\n","\t\t\telse:\n","\t\t\t\tf_up = F.interpolate(feature_maps[i], scale_factor=2, mode=CFG.interpolation)\n","\t\t\tf = torch.cat([feature_maps[i-1], f_up], dim=1)\t\t\t\n","\t\t\tf_down = self.convs[i-1](f)\n","\t\t\tfeature_maps[i-1] = f_down\n","\n","\t\treturn feature_maps[0]\n","\n","\n","class Net(nn.Module):\n","\tdef __init__(\n","\t\tself,\n","\t):\n","\t\tsuper().__init__()\n","\t\tself.output_type = [\"inference\", \"loss\"]\n","\n","\t\tconv_dim = 64\n","\t\tencoder1_dim = [\n","\t\t\tconv_dim,\n","\t\t\t256,\n","\t\t\t512,\n","\t\t\t1024,\n","\t\t\t2048,\n","\t\t]\n","\t\tdecoder1_dim = [\n","\t\t\t1024,\n","\t\t\t512,\n","\t\t\t256,\n","\t\t\t64,\n","\t\t]\n","\t\tif CFG.model_name == \"seresnext50_32x4d\":\n","\t\t\tself.encoder1 = seresnext50_32x4d(pretrained=CFG.pretrained, in_chans=CFG.BATCH_Z_DIFF)\n","\t\telif CFG.model_name == \"seresnext26d_32x4d\":\n","\t\t\tself.encoder1 = seresnext26d_32x4d(pretrained=CFG.pretrained, in_chans=CFG.BATCH_Z_DIFF)\n","\t\telif CFG.model_name == \"seresnext26tn_32x4d\":\n","\t\t\tself.encoder1 = seresnext26tn_32x4d(pretrained=CFG.pretrained, in_chans=CFG.BATCH_Z_DIFF)\n","\n","\t\tself.decoder1 = Decoder(encoder_dims=encoder1_dim)\n","\t\t# -- pool attention weight  \n","\t\tself.weight1 = nn.ModuleList(\n","\t\t\t[\n","\t\t\t\tnn.Sequential(\n","\t\t\t\t\tnn.Conv2d(dim, dim, kernel_size=3, padding=1),\n","\t\t\t\t\tnn.BatchNorm2d(dim),\n","\t\t\t\t\tnn.ReLU(inplace=True),\n","\t\t\t\t)\n","\t\t\t\tfor dim in encoder1_dim\n","\t\t\t]\n","\t\t)\n","\t\tself.logit1 = nn.Conv2d(decoder1_dim[-1], 1, kernel_size=1)\n","\n","\t\t# --------------------------------\t\t\n","\t\tencoder2_dim = [64, 64, 128, 256, 512]\n","\t\tdecoder2_dim = [\n","\t\t\t256,\n","\t\t\t128,\n","\t\t\t64,\n","\t\t\t64,\n","\t\t]\n","\t\tself.encoder2 = resnet10t(pretrained=CFG.pretrained, in_chans=decoder1_dim[-1])\n","\n","\t\tself.decoder2 = SmpUnetDecoder(\n","\t\t\tin_channel=encoder2_dim[-1],\n","\t\t\tskip_channel=encoder2_dim[:-1][::-1],\n","\t\t\tout_channel=decoder2_dim,\n","\t\t)\n","\t\tself.logit2 = nn.Conv2d(decoder2_dim[-1], 1, kernel_size=1)\n","\t\tif CFG.init_weight_bias:\n","\t\t\tinitialize_weights(self.logit1)\n","\t\t\tinitialize_weights(self.logit2)\n","\t\t\tinitialize_weights(self.decoder1)\n","\t\t\tinitialize_weights(self.weight1)\n","\t\t\tinitialize_weights(self.decoder2)\n","\n","\tdef forward(self, batch):\n","\t\tv = batch\n","\t\tB, C, H, W = v.shape\n","\t\tvv = [\n","\t\t\tv[:, a: b]\n","\t\t\tfor a, b in CFG.BATCH_Z_INFO\n","\t\t]\n","\t\t# print(list(map(lambda a: a.shape, vv)))\n","\t\tK = len(vv)\n","\t\tx = torch.cat(vv, 0)\n","\t\t# x = v\n","\n","\t\t# ----------------------\n","\t\tencoder = []\n","\t\te = self.encoder1\n","\t\tx = e.conv1(x)\n","\t\tx = e.bn1(x)\n","\t\tx = e.act1(x)\n","\t\tencoder.append(x)\n","\t\tx = F.avg_pool2d(x, kernel_size=2, stride=2)\n","\t\tx = e.layer1(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer2(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer3(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer4(x)\n","\t\tencoder.append(x)\n","\t\t# print('encoder', [f.shape for f in encoder])\n","\n","\t\tfor i in range(len(encoder)):\n","\t\t\te = encoder[i]\n","\t\t\te = F.avg_pool2d(e, 2, 2)\n","\t\t\tf = self.weight1[i](e)\n","\t\t\t_, c, h, w = e.shape\n","\t\t\tf = rearrange(f, \"(K B) c h w -> B K c h w\", K=K, B=B, h=h, w=w)  #\n","\t\t\te = rearrange(e, \"(K B) c h w -> B K c h w\", K=K, B=B, h=h, w=w)  #\n","\t\t\tw = F.softmax(f, 1)\n","\t\t\te = (w * e).sum(1)\n","\t\t\tencoder[i] = e\n","\n","\t\t# print('encoder after weighted', [f.shape for f in encoder])\n","\t\t# print('skip after weighted', [f.shape for f in skip])\n","\t\tlast = self.decoder1(encoder)\n","\t\tlogit1 = self.logit1(last)\n","\t\t# print(\"last.shape\", last.shape)\n","\t\t# print(\"logit1.shape\", logit1.shape)\n","\t\t# print(\"encoder1 shapes\", list(map(lambda a: a.shape, encoder)))\n","\n","\t\tlogit1 = F.interpolate(\n","\t\t\tlogit1, size=(H, W), mode=CFG.interpolation, align_corners=False, antialias=True\n","\t\t)\n","\n","\t\t# ----------------------\n","\t\tx = last  # .detach()\n","\t\t# x = F.avg_pool2d(x,kernel_size=2,stride=2)\n","\t\tencoder = []\n","\t\te = self.encoder2\n","\t\tx = e.conv1(x)\n","\t\tx = e.bn1(x)\n","\t\tx = e.act1(x)\n","\t\tencoder.append(x)\n","\t\tx = F.avg_pool2d(x, kernel_size=2, stride=2)\n","\t\tx = e.layer1(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer2(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer3(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer4(x)\n","\t\tencoder.append(x)\n","\t\t\n","\t\tfeature = encoder[-1]\n","\t\tskip = encoder[:-1][::-1]\n","\n","\t\t# print(\"feature.shape\", list(map(lambda a: a.shape, feature)))\n","\t\t# print(\"skip.shape\", list(map(lambda a: a.shape, skip)))\n","\n","\t\tlast, decoder = self.decoder2(feature, skip)\n","\t\tlogit2 = self.logit2(last)\n","\t\t# print(\"logit2.shape\", logit2.shape)\n","\t\tlogit2 = F.interpolate(\n","\t\t\tlogit2, size=(H, W), mode=CFG.interpolation, align_corners=False, antialias=True\n","\t\t)\n","\t\treturn logit1, logit2\n","\t\t# return logit1\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["net_check_input = torch.from_numpy(np.random.randn(12, CFG.Z_DIM, CFG.CROP_SIZE, CFG.CROP_SIZE).astype(np.float32))\n","out = Net()(net_check_input)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tc = torch\n","def TTA(x:tc.Tensor,model:nn.Module):\n","    #x.shape=(batch,c,h,w)\n","    shape=x.shape\n","    x=[x,*[tc.rot90(x,k=i,dims=(-2,-1)) for i in range(1,4)]]\n","    x=tc.cat(x,dim=0)\n","    _, x = model(x)\n","    x=x.reshape(4,shape[0], 1 ,*shape[2:])\n","    x=[tc.rot90(x[i],k=-i,dims=(-2,-1)) for i in range(4)]\n","    x=tc.stack(x,dim=0)\n","    return x.mean(0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","class Model(pl.LightningModule):\n","\ttraining_step_outputs = []\n","\tvalidation_step_outputs = []\n","\ttest_step_outputs = [[], []]\n","\n","\tdef __init__(self, **kwargs):\n","\t\tsuper().__init__()\n","\n","\t\tself.model = Net()        \n","\n","\t\tself.loss1 = CFG.loss1\n","\t\tself.loss2 = CFG.loss2a\n","\t\tself.loss3 = CFG.loss2b\n","\n","\tdef forward(self, image, stage):\n","\t\tif stage != \"train\":\n","\t\t\t# mask = TTA(image, self.model)\n","\t\t\t_, mask = self.model(image)\n","\t\telse:\n","\t\t\tmask = self.model(image)\n","\t\treturn mask\n","\n","\tdef shared_step(self, batch, stage):\n","\t\tsubvolumes, labels = batch\n","\n","\t\timage, labels = subvolumes.float(), labels.float()        \n","\t\tassert image.ndim == 4\n","\t\t\n","\t\th, w = image.shape[2:]\n","\t\tassert h % 32 == 0 and w % 32 == 0\n","\t\t\n","\t\t# print(\"labels\", labels.max(), labels.min())\n","\n","\t\tassert labels.max() <= 1.0 and labels.min() >= 0\n","\n","\t\tif stage == \"train\":\n","\t\t\t\n","\t\t\t# decide if perform mixed-up augmentation\n","\t\t\tshould_mixed_up = \"MixedUp\" in CFG.augmentation_names and  np.random.rand(1)[0] <= CFG.mixed_up_probability and self.current_epoch in range(CFG.start_mixedup_epochs, CFG.stop_mixedup_epochs)\n","\t\t\tif should_mixed_up:\n","\t\t\t\t# perform mixed-up\n","\t\t\t\timage, labels = mixup_data(subvolumes, labels)\n","\t\t\t\n","\t\t\t\n","\t\t\tlogit1, logit2 = self.forward(image, stage)\n","\t\t\tif should_mixed_up:\n","\t\t\t\tloss1 = self.loss1(logit1, labels)\n","\t\t\t\tloss2a = self.loss2(logit2, labels)\n","\t\t\t\tloss2b = self.loss3(logit2, labels)\n","\t\t\t\tloss = CFG.loss1_weight * loss1 + CFG.loss2_weight * (loss2a * CFG.loss2a_weight + loss2b * CFG.loss2b_weight)\n","\t\t\telse:\n","\t\t\t\tloss = self.loss1(logit1, labels) * CFG.loss1_weight + CFG.loss2_weight * (self.loss2(logit2, labels) * CFG.loss2a_weight + self.loss3(logit2, labels) * CFG.loss2b_weight)\n","\t\t\t# loss = self.loss2(logit, labels) * CFG.loss2a_weight + self.loss3(logit, labels) * CFG.loss2b_weight\n","\t\t\tlogit = logit2\n","\t\telse:\n","\t\t\tlogit = self.forward(image, stage)\n","\t\t\tloss = self.loss2(logit, labels) * CFG.loss2a_weight + self.loss3(logit, labels) * CFG.loss2b_weight\n","\t\t\n","\t\tprob2 = torch.sigmoid(logit)\n","\n","\t\tpred_mask = (prob2 > CFG.threshold).float()\n","\t\t\n","\t\t# print(\"pred_mask\", pred_mask)\n","\t\t\n","\t\tscore = fbeta_score(pred_mask, labels, threshold=CFG.threshold)\n","\n","\t\ttp, fp, fn, tn = smp.metrics.get_stats(\n","\t\t\tpred_mask.long(), labels.long(), mode=\"binary\"\n","\t\t)\n","\t\t# cur_lr = self.lr_schedulers().get_last_lr()\n","\n","\t\treturn {\n","\t\t\t\"loss\": loss,\n","\t\t\t\"tp\": tp,\n","\t\t\t\"fp\": fp,\n","\t\t\t\"fn\": fn,\n","\t\t\t\"tn\": tn,\n","\t\t\t\"score\": score,\n","\t\t\t# \"lr\": cur_lr,\n","\t\t}\n","\n","\tdef shared_epoch_end(self, outputs, stage):\n","\t\t# aggregate step metics\n","\t\ttp = torch.cat([x[\"tp\"] for x in outputs])\n","\t\tfp = torch.cat([x[\"fp\"] for x in outputs])\n","\t\tfn = torch.cat([x[\"fn\"] for x in outputs])\n","\t\ttn = torch.cat([x[\"tn\"] for x in outputs])\n","\t\t# lr = outputs[0][\"lr\"]\n","\t\tloss = torch.mean(torch.Tensor([x[\"loss\"] for x in outputs]))\n","\t\tfbeta_score = torch.mean(torch.Tensor([x[\"score\"] for x in outputs]))\n","\n","\t\tper_image_iou = smp.metrics.iou_score(\n","\t\t\ttp, fp, fn, tn, reduction=\"micro-imagewise\"\n","\t\t)\n","\n","\t\tdataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n","\n","\t\tmetrics = {\n","\t\t\tf\"{stage}_per_image_iou\": per_image_iou,\n","\t\t\tf\"{stage}_dataset_iou\": dataset_iou,\n","\t\t\tf\"{stage}_loss\": 10000 if loss.item() == 0 else loss.item(),\n","\t\t\tf\"{stage}_tp\": tp.sum().int().item(),\n","\t\t\tf\"{stage}_fp\": fp.sum().int().item(),\n","\t\t\tf\"{stage}_fn\": fn.sum().int().item(),\n","\t\t\tf\"{stage}_tn\": tn.sum().int().item(),\n","\t\t\tf\"{stage}_score\": fbeta_score.item(),\n","\t\t\t# \"lr\": lr,\n","\t\t}\n","\n","\t\tself.log_dict(metrics, prog_bar=True, sync_dist=True)\n","\n","\tdef training_step(self, batch, batch_idx):\n","\t\tout = self.shared_step(batch, \"train\")\n","\t\tself.training_step_outputs.append(out)\n","\t\treturn out\n","\n","\tdef on_train_epoch_end(self):\n","\t\tout = self.shared_epoch_end(self.training_step_outputs, \"train\")\n","\t\tself.training_step_outputs.clear()\n","\t\treturn out\n","\n","\tdef validation_step(self, batch, batch_idx):\n","\t\tout = self.shared_step(batch, \"valid\")\n","\t\tself.validation_step_outputs.append(out)\n","\t\treturn out\n","\n","\tdef on_validation_epoch_end(self):\n","\t\tout = self.shared_epoch_end(self.validation_step_outputs, \"valid\")\n","\t\tself.validation_step_outputs.clear()\n","\t\treturn out\n","\n","\tdef test_step(self, batch, batch_idx):\n","\t\tglobal predictions_map, predictions_map_counts\n","\n","\t\tpatch_batch, loc_batch = batch\n","\n","\t\tloc_batch = loc_batch.long()\n","\t\tpatch_batch = patch_batch.float()\n","\t\tpredictions = self.forward(patch_batch, \"test\")\n","\t\tpredictions = predictions.sigmoid()\n"," \n","\t\tpredictions = torch.permute(predictions, (0, 2, 3, 1)).squeeze(dim=-1)\n","\t\tpredictions = (\n","\t\t\tpredictions.cpu().numpy()\n","\t\t)\n","\t\tloc_batch = loc_batch.cpu().numpy()\n","\t\t\n","\t\tself.test_step_outputs[0].extend(loc_batch)\n","\t\tself.test_step_outputs[1].extend(predictions)        \n","\t\treturn loc_batch, predictions\n","\n","\tdef on_test_epoch_end(self):\n","\t\tglobal predictions_map, predictions_map_counts\n","\n","\t\tlocs = np.array(self.test_step_outputs[0])\n","\t\tpreds = np.array(self.test_step_outputs[1])\n","\t\tprint(\"locs\", locs.shape)\n","\t\tprint(\"preds\", preds.shape)\n","\n","\n","\t\tfor (y, x), pred in zip(locs, preds):\n","\t\t\tpredictions_map[\n","\t\t\t\ty - CFG.BUFFER : y + CFG.BUFFER, x - CFG.BUFFER : x + CFG.BUFFER\n","\t\t\t] += pred\n","\t\t\tpredictions_map_counts[\n","\t\t\t\ty - CFG.BUFFER : y + CFG.BUFFER, x - CFG.BUFFER : x + CFG.BUFFER\n","\t\t\t] += 1\n","\t\t\n","\t\tpredictions_map /= predictions_map_counts + CFG.exp\n","\n","\tdef configure_optimizers(self):\n","\t\t# optimizer = optim.SGD(self.parameters(), lr=CFG.lr)\n","\t\tif CFG.optimizer == \"Adam\":\n","\t\t\toptimizer = optim.Adam(self.parameters(), lr=CFG.lr)\n","\t\telif CFG.optimizer == \"AdamW\":\n","\t\t\toptimizer = optim.AdamW(self.parameters(), lr=CFG.lr)\n","\t\t\n","\t\tif CFG.lr_scheduler_name == \"ReduceLROnPlateau\":\n","\t\t\tscheduler = CFG.lr_scheduler(\n","\t\t\t\toptimizer, mode=\"min\", factor=0.75, patience=5,\n","\t\t\t)\n","\t\t\treturn {\n","\t\t\t\t\"optimizer\": optimizer,\n","\t\t\t\t\"lr_scheduler\": {\"scheduler\": scheduler, \"monitor\": \"valid_loss\"},\n","\t\t\t}\n","\t\telif CFG.lr_scheduler_name == \"CosineAnnealingLR\":\n","\t\t\tscheduler = CFG.lr_scheduler(\n","\t\t\t\toptimizer, T_max=CFG.num_epochs, eta_min=CFG.eta_min_lr,\n","\t\t\t)\n","\t\t\treturn {\n","\t\t\t\t\"optimizer\": optimizer,\n","\t\t\t\t\"lr_scheduler\": scheduler,\n","\t\t\t}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pytorch_lightning.seed_everything(seed=42)\n","torch.set_float32_matmul_precision('high')\n","\n","\n","masks = load_mask(split=\"train\", index=1)\n","labels = load_labels(split=\"train\", index=1)\n","\n","fig, (ax1, ax2) = plt.subplots(1, 2)\n","ax1.set_title(\"mask.png\")\n","ax1.imshow(masks, cmap='gray')\n","ax2.set_title(\"inklabels.png\")\n","ax2.imshow(labels, cmap='gray')\n","plt.show()\n","\n","mask_test_a = load_mask(split=\"test\", index=\"a\")\n","mask_test_b = load_mask(split=\"test\", index=\"b\")\n","\n","mask_train_1 = load_mask(split=\"train\", index=\"1\")\n","labels_train_1 = load_labels(split=\"train\", index=\"1\")\n","\n","mask_train_2a = load_mask(split=\"train\", index=\"2a\")\n","labels_train_2a = load_labels(split=\"train\", index=\"2a\")\n","\n","mask_train_2b = load_mask(split=\"train\", index=\"2b\")\n","labels_train_2b = load_labels(split=\"train\", index=\"2b\")\n","\n","mask_train_3 = load_mask(split=\"train\", index=\"3\")\n","labels_train_3 = load_labels(split=\"train\", index=\"3\")\n","\n","print(f\"mask_test_a: {mask_test_a.shape}\")\n","print(f\"mask_test_b: {mask_test_b.shape}\")\n","print(\"-\")\n","print(f\"mask_train_1: {mask_train_1.shape}\")\n","print(f\"labels_train_1: {labels_train_1.shape}\")\n","print(\"-\")\n","print(f\"mask_train_2a: {mask_train_2a.shape}\")\n","print(f\"labels_train_2a: {labels_train_2a.shape}\")\n","print(\"-\")\n","print(f\"mask_train_2b: {mask_train_2b.shape}\")\n","print(f\"labels_train_2b: {labels_train_2b.shape}\")\n","print(\"-\")\n","print(f\"mask_train_3: {mask_train_3.shape}\")\n","print(f\"labels_train_3: {labels_train_3.shape}\")\n","\n","fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4)\n","\n","ax1.set_title(\"labels_train_1\")\n","ax1.imshow(labels_train_1, cmap='gray')\n","\n","ax2.set_title(\"labels_train_2a\")\n","ax2.imshow(labels_train_2a, cmap='gray')\n","\n","ax3.set_title(\"labels_train_2b\")\n","ax3.imshow(labels_train_2b, cmap='gray')\n","\n","ax4.set_title(\"labels_train_3\")\n","ax4.imshow(labels_train_3, cmap='gray')\n","plt.tight_layout()\n","plt.show()\n","\n","volume_train_1 = load_volume(split=\"train\", index=1)\n","print(f\"volume_train_1: {volume_train_1.shape}, {volume_train_1.dtype}\")\n","\n","volume_train_2a = load_volume(split=\"train\", index=\"2a\")\n","print(f\"volume_train_2a: {volume_train_2a.shape}, {volume_train_2a.dtype}\")\n","\n","volume_train_2b = load_volume(split=\"train\", index=\"2b\")\n","print(f\"volume_train_2b: {volume_train_2b.shape}, {volume_train_2b.dtype}\")\n","\n","volume_train_3 = load_volume(split=\"train\", index=3)\n","print(f\"volume_train_3: {volume_train_3.shape}, {volume_train_3.dtype}\")\n","\n","# volume = np.concatenate([volume_train_1, volume_train_2, volume_train_3], axis=1)\n","# volume = np.concatenate([volume_train_1, volume_train_2], axis=1)\n","# print(f\"total volume: {volume.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sample_data_name = \"3\"\n","sample_volume = load_volume(\"train\", sample_data_name)\n","sample_mask = load_mask(\"train\", sample_data_name)\n","sample_label = load_labels(\"train\", sample_data_name)\n","sample_locations = generate_locations_ds(sample_volume, sample_mask, sample_label, skip_zero=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sample_ds = SubvolumeDataset(\n","    sample_locations,\n","    sample_volume,\n","    sample_label,\n","    CFG.BUFFER,\n","    is_train=False,\n",")\n","\n","id = 600\n","sample_depth = CFG.Z_DIM // 2\n","\n","fig, ax = plt.subplots(1, 2, figsize=(8, 6))\n","\n","img = sample_ds[id][0][sample_depth, :, :]\n","label = sample_ds[id][1][0, :, :]\n","ax[0].imshow(img)\n","ax[1].imshow(label)\n","\n","fig, ax = plt.subplots(figsize=(8, 6))\n","\n","ax.imshow(sample_label)\n","\n","y, x = sample_locations[id]\n","patch = patches.Rectangle([x - CFG.BUFFER, y - CFG.BUFFER], 2 * CFG.BUFFER, 2 * CFG.BUFFER, linewidth=2, edgecolor='g', facecolor='none')\n","ax.add_patch(patch)\n","plt.show()    \n","\n","fig, ax = plt.subplots(CFG.Z_DIM, 1, figsize=(12, 36))\n","\n","for i in range(CFG.Z_DIM):\n","    img, _ = sample_ds[id]\n","    img = img[i, :, :]\n","    ax[i].hist(img.flatten(), bins=1000)  # Plot histogram of the flattened data\n","    ax[i].set_title(f\"Histogram of Channel {i}\")  # Add title to the plot    \n","fig.tight_layout()\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fragment_cropped = sample_ds[id][0]\n","fragment_cropped = ((fragment_cropped * CFG.std) + CFG.mean) * 255\n","fragment_cropped = fragment_cropped.astype(np.uint8)\n","print(fragment_cropped.shape)\n","imageio.mimwrite(\"fragment_crop.mp4\", fragment_cropped, \"ffmpeg\")\n","Video(\"fragment_crop.mp4\", height=CFG.CROP_SIZE, width=CFG.CROP_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:14:39.034124Z","iopub.status.busy":"2023-03-18T23:14:39.033748Z","iopub.status.idle":"2023-03-18T23:28:08.685364Z","shell.execute_reply":"2023-03-18T23:28:08.684324Z","shell.execute_reply.started":"2023-03-18T23:14:39.034086Z"},"trusted":true},"outputs":[],"source":["\n","k_folds = 4\n","kfold = KFold(\n","    n_splits=k_folds,\n",")\n","\n","data_list = [    \n","    [volume_train_1, labels_train_1, mask_train_1],\n","    [volume_train_2a, labels_train_2a, mask_train_2a], \n","    [volume_train_2b, labels_train_2b, mask_train_2b],\n","    [volume_train_3, labels_train_3, mask_train_3],  \n","]\n","\n","predictions_map = None\n","predictions_map_counts = None\n","\n","for fold, (train_data, val_data) in enumerate(kfold.split(data_list)):\n","    print(f'FOLD {fold}')\n","    print('--------------------------------')\n","    print(\"train_data\", train_data)\n","    print(\"val_data\", val_data)\n","    one = data_list[train_data[0]]\n","    two = data_list[train_data[1]]\n","    three = data_list[train_data[2]]\n","    train_height = max(one[0].shape[0], two[0].shape[0], three[0].shape[0])\n","    if one[0].shape[0] < train_height:\n","        diff = train_height - one[0].shape[0]\n","        one[0] = np.pad(one[0], ((0, diff), (0, 0), (0, 0)), constant_values=0)\n","        one[1] = np.pad(one[1], ((0, diff), (0, 0)), constant_values=0)\n","        one[2] = np.pad(one[2], ((0, diff), (0, 0)), constant_values=0)\n","    if two[0].shape[0] < train_height:\n","        diff = train_height - two[0].shape[0]\n","        two[0] = np.pad(two[0], ((0, diff), (0, 0), (0, 0)), constant_values=0)\n","        two[1] = np.pad(two[1], ((0, diff), (0, 0)), constant_values=0)\n","        two[2] = np.pad(two[2], ((0, diff), (0, 0)), constant_values=0)\n","    if three[0].shape[0] < train_height:\n","        diff = train_height - three[0].shape[0]\n","        three[0] = np.pad(three[0], ((0, diff), (0, 0), (0, 0)), constant_values=0)\n","        three[1] = np.pad(three[1], ((0, diff), (0, 0)), constant_values=0)\n","        three[2] = np.pad(three[2], ((0, diff), (0, 0)), constant_values=0)\n","    train_volume = np.concatenate([one[0], two[0], three[0]], axis=1)\n","    train_label = np.concatenate([one[1], two[1], three[1]], axis=1)\n","    train_mask = np.concatenate([one[2], two[2], three[2]], axis=1)\n","    val_volume, val_label, val_mask = data_list[val_data[0]]    \n","\n","    train_locations_ds = generate_locations_ds(train_volume, train_mask, train_label, skip_zero=True)\n","    val_location_ds = generate_locations_ds(val_volume, val_mask, skip_zero=False)\n","\n","    visualize_dataset_patches(train_locations_ds, train_label, \"train\", fold)\n","    visualize_dataset_patches(val_location_ds, val_label, \"val\", fold)\n","    \n","    # Init the neural network\n","    model = Model()\n","\n","    wandb.finish()\n","    # Initialize a trainer\n","    now = datetime.datetime.now()\n","    now = f\"{now}\".replace(\" \", \"\")\n","    \n","    checkpoint_callback = pytorch_lightning.callbacks.ModelCheckpoint(\n","        monitor='valid_score',\n","        dirpath=f'best-results-{now}/',\n","        mode=\"max\",\n","        filename='train-fold-' + str(fold) + '-{epoch:02d}-{valid_score:.3f}',\n","        save_last=True,\n","    )\n","    \n","    trainer = pl.Trainer(\n","        max_epochs=CFG.num_epochs,\n","        devices=\"0,1,2,3\",\n","        accelerator=\"gpu\",\n","        precision=16 if CFG.precision == \"float16\" else 32,\n","        # strategy=\"ddp_find_unused_parameters_false\",\n","        # strategy=\"ddp_fork\",\n","        logger=WandbLogger(\n","            name=f\"2.5dimension-{now}-fold-{fold}-val-{val_data[0]}\",\n","            notes=CFG.WANDB_NOTE,\n","            config=class2dict(CFG()),\n","            group=CFG.exp_name,\n","        ),\n","        callbacks=[checkpoint_callback],\n","        default_root_dir=os.path.join(CFG.DATA_DIR, f\"{now}-fold-{fold}-val-{val_data[0]}\"),\n","    )\n","    \n","    # Sample elements randomly from a given list of ids, no replacement.\n","    train_ds = SubvolumeDataset(\n","        train_locations_ds,\n","        train_volume,\n","        train_label,\n","        CFG.BUFFER,\n","        is_train=True\n","    )\n","    val_ds = SubvolumeDataset(\n","        val_location_ds,\n","        val_volume,\n","        val_label,\n","        CFG.BUFFER,\n","        is_train=False,\n","    )\n","\n","    # Define data loaders for training and testing data in this fold\n","    train_loader = torch.utils.data.DataLoader(\n","        train_ds,\n","        batch_size=CFG.BATCH_SIZE,\n","        num_workers=CFG.num_workers,\n","        shuffle=True,\n","        # persistent_workers=True,\n","        # pin_memory=True,\n","    )\n","    val_loader = torch.utils.data.DataLoader(\n","        val_ds, \n","        batch_size=CFG.BATCH_SIZE,\n","        num_workers=CFG.num_workers,\n","        shuffle=False,\n","        # pin_memory=True,\n","        # persistent_workers=True,\n","    )\n","\n","    # Train the model\n","    trainer.fit(model, train_loader, val_loader)   \n","\n","wandb.finish()\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat":4,"nbformat_minor":4}
