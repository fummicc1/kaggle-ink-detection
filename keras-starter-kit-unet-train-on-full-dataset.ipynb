{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Keras starter kit [full training set, UNet]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:10:40.298967Z","iopub.status.busy":"2023-03-18T23:10:40.298008Z","iopub.status.idle":"2023-03-18T23:10:49.514216Z","shell.execute_reply":"2023-03-18T23:10:49.513058Z","shell.execute_reply.started":"2023-03-18T23:10:40.298921Z"},"trusted":true},"outputs":[],"source":["\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","import pytorch_lightning\n","import segmentation_models_pytorch as smp\n","import pytorch_lightning as pl\n","\n","import ssl\n","ssl._create_default_https_context = ssl._create_unverified_context\n","\n","import glob\n","import time\n","import PIL.Image as Image\n","import matplotlib.pyplot as plt\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import Rectangle\n","import matplotlib.patches as patches\n","from sklearn.model_selection import KFold\n","from tqdm import tqdm\n","import cv2\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data\n","\n","\n","# Data config\n","# DATA_DIR = '/kaggle/input/vesuvius-challenge-ink-detection/'\n","DATA_DIR = '/home/fummicc1/codes/competitions/kaggle-ink-detection'\n","BUFFER = 128  # Half-size of papyrus patches we'll use as model inputs\n","Z_LIST = list(range(0, 65, 4))  # Offset of slices in the z direction\n","Z_DIM = len(Z_LIST)  # Number of slices in the z direction. Max value is 64 - Z_START\n","SHARED_LENGTH = 4000  # Max length(width or height) to resize all papyrii\n","\n","# Model config\n","BATCH_SIZE = 64\n","\n","device = torch.device(\"cuda\")\n","threshold = 0.5\n","num_workers = 4\n","exp = 1e-7\n","\n","num_epochs = 120\n","lr = 1e-4\n","\n","pytorch_lightning.seed_everything(seed=42)\n","torch.set_float32_matmul_precision('high')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:10:49.517657Z","iopub.status.busy":"2023-03-18T23:10:49.516568Z","iopub.status.idle":"2023-03-18T23:10:52.651975Z","shell.execute_reply":"2023-03-18T23:10:52.650919Z","shell.execute_reply.started":"2023-03-18T23:10:49.517615Z"},"trusted":true},"outputs":[],"source":["# plt.imshow(Image.open(DATA_DIR + \"/train/1/ir.png\"), cmap=\"gray\")\n","# plt.imshow(Image.open(DATA_DIR + \"/train/2/ir.png\"), cmap=\"gray\")\n","# plt.imshow(Image.open(DATA_DIR + \"/train/3/ir.png\"), cmap=\"gray\")\n","plt.imshow(Image.open(DATA_DIR + \"/test/a/mask.png\"), cmap=\"gray\")\n","# plt.imshow(Image.open(DATA_DIR + \"/test/b/mask.png\"), cmap=\"gray\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Load up the training data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:10:52.653587Z","iopub.status.busy":"2023-03-18T23:10:52.653064Z","iopub.status.idle":"2023-03-18T23:10:57.915118Z","shell.execute_reply":"2023-03-18T23:10:57.914028Z","shell.execute_reply.started":"2023-03-18T23:10:52.653544Z"},"trusted":true},"outputs":[],"source":["def resize(img):\n","    current_height, current_width = img.shape    \n","    aspect_ratio = current_width / current_height\n","    if current_height > current_width:\n","        new_height = SHARED_LENGTH\n","        new_width = int(SHARED_LENGTH * aspect_ratio)\n","    else:\n","        new_width = SHARED_LENGTH\n","        new_height = int(SHARED_LENGTH / aspect_ratio)        \n","        \n","    new_size = (new_width, new_height)\n","    # (W, H)の順で渡すが結果は(H, W)になっている\n","    img = cv2.resize(img, new_size)\n","    return img\n","\n","def load_mask(split, index):\n","    img = cv2.imread(f\"{DATA_DIR}/{split}/{index}/mask.png\", 0) // 255\n","    img = resize(img)    \n","    return img\n","\n","\n","def load_labels(split, index):\n","    img = cv2.imread(f\"{DATA_DIR}/{split}/{index}/inklabels.png\", 0) // 255\n","    img = resize(img)\n","    return img\n","\n","\n","mask = load_mask(split=\"train\", index=1)\n","labels = load_labels(split=\"train\", index=1)\n","\n","fig, (ax1, ax2) = plt.subplots(1, 2)\n","ax1.set_title(\"mask.png\")\n","ax1.imshow(mask, cmap='gray')\n","ax2.set_title(\"inklabels.png\")\n","ax2.imshow(labels, cmap='gray')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:10:57.922052Z","iopub.status.busy":"2023-03-18T23:10:57.921638Z","iopub.status.idle":"2023-03-18T23:11:00.615559Z","shell.execute_reply":"2023-03-18T23:11:00.614397Z","shell.execute_reply.started":"2023-03-18T23:10:57.922012Z"},"trusted":true},"outputs":[],"source":["mask_test_a = load_mask(split=\"test\", index=\"a\")\n","mask_test_b = load_mask(split=\"test\", index=\"b\")\n","\n","mask_train_1 = load_mask(split=\"train\", index=1)\n","labels_train_1 = load_labels(split=\"train\", index=1)\n","\n","mask_train_2 = load_mask(split=\"train\", index=2)\n","labels_train_2 = load_labels(split=\"train\", index=2)\n","\n","mask_train_3 = load_mask(split=\"train\", index=3)\n","labels_train_3 = load_labels(split=\"train\", index=3)\n","\n","print(f\"mask_test_a: {mask_test_a.shape}\")\n","print(f\"mask_test_b: {mask_test_b.shape}\")\n","print(\"-\")\n","print(f\"mask_train_1: {mask_train_1.shape}\")\n","print(f\"labels_train_1: {labels_train_1.shape}\")\n","print(\"-\")\n","print(f\"mask_train_2: {mask_train_2.shape}\")\n","print(f\"labels_train_2: {labels_train_2.shape}\")\n","print(\"-\")\n","print(f\"mask_train_3: {mask_train_3.shape}\")\n","print(f\"labels_train_3: {labels_train_3.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:11:00.619816Z","iopub.status.busy":"2023-03-18T23:11:00.619103Z","iopub.status.idle":"2023-03-18T23:11:02.849127Z","shell.execute_reply":"2023-03-18T23:11:02.847843Z","shell.execute_reply.started":"2023-03-18T23:11:00.619773Z"},"trusted":true},"outputs":[],"source":["fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n","\n","ax1.set_title(\"labels_train_1\")\n","ax1.imshow(labels_train_1, cmap='gray')\n","\n","ax2.set_title(\"labels_train_2\")\n","ax2.imshow(labels_train_2, cmap='gray')\n","\n","ax3.set_title(\"labels_train_3\")\n","ax3.imshow(labels_train_3, cmap='gray')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:11:02.854397Z","iopub.status.busy":"2023-03-18T23:11:02.85384Z","iopub.status.idle":"2023-03-18T23:11:02.867203Z","shell.execute_reply":"2023-03-18T23:11:02.866021Z","shell.execute_reply.started":"2023-03-18T23:11:02.854351Z"},"trusted":true},"outputs":[],"source":["def load_volume(split, index):\n","    # Load the 3d x-ray scan, one slice at a time\n","    all = sorted(glob.glob(f\"{DATA_DIR}/{split}/{index}/surface_volume/*.tif\"))\n","    z_slices_fnames = [all[i] for i in range(len(all)) if i in Z_LIST]\n","    assert len(z_slices_fnames) == Z_DIM\n","    z_slices = []\n","    for z, filename in  tqdm(enumerate(z_slices_fnames)):\n","        img = cv2.imread(filename, -1)\n","        img = resize(img)\n","        img = (img / (2 ** 8)).astype(np.uint8)\n","        z_slices.append(img)\n","    return np.stack(z_slices, axis=-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:11:02.874585Z","iopub.status.busy":"2023-03-18T23:11:02.872191Z","iopub.status.idle":"2023-03-18T23:14:11.220165Z","shell.execute_reply":"2023-03-18T23:14:11.218002Z","shell.execute_reply.started":"2023-03-18T23:11:02.874541Z"},"trusted":true},"outputs":[],"source":["volume_train_1 = load_volume(split=\"train\", index=1)\n","print(f\"volume_train_1: {volume_train_1.shape}, {volume_train_1.dtype}\")\n","\n","volume_train_2 = load_volume(split=\"train\", index=2)\n","print(f\"volume_train_2: {volume_train_2.shape}, {volume_train_2.dtype}\")\n","\n","volume_train_3 = load_volume(split=\"train\", index=3)\n","print(f\"volume_train_3: {volume_train_3.shape}, {volume_train_3.dtype}\")\n","\n","volume = np.concatenate([volume_train_1, volume_train_2, volume_train_3], axis=1)\n","print(f\"total volume: {volume.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["del volume_train_1\n","del volume_train_2\n","del volume_train_3"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:14:11.222178Z","iopub.status.busy":"2023-03-18T23:14:11.221817Z","iopub.status.idle":"2023-03-18T23:14:11.231554Z","shell.execute_reply":"2023-03-18T23:14:11.230344Z","shell.execute_reply.started":"2023-03-18T23:14:11.222141Z"},"trusted":true},"outputs":[],"source":["labels = np.concatenate([labels_train_1, labels_train_2, labels_train_3], axis=1)\n","print(f\"labels: {labels.shape}, {labels.dtype}\")\n","mask = np.concatenate([mask_train_1, mask_train_2, mask_train_3], axis=1)\n","print(f\"mask: {mask.shape}, {mask.dtype}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Free up memory\n","del labels_train_1\n","del labels_train_2\n","del labels_train_3\n","del mask_train_1\n","del mask_train_2\n","del mask_train_3"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Visualize the training data\n","\n","In this case, not very informative. But remember to always visualize what you're training on, as a sanity check!"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:14:11.233579Z","iopub.status.busy":"2023-03-18T23:14:11.233103Z","iopub.status.idle":"2023-03-18T23:14:16.475332Z","shell.execute_reply":"2023-03-18T23:14:16.474429Z","shell.execute_reply.started":"2023-03-18T23:14:11.233541Z"},"trusted":true},"outputs":[],"source":["fig, axes = plt.subplots(1, 2, figsize=(15, 3))\n","for z, ax in enumerate(axes):\n","    ax.imshow(volume[:, :, z], cmap='gray')\n","    ax.set_xticks([]); ax.set_yticks([])\n","fig.tight_layout()\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Create a dataset in the input volume\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def is_in_masked_zone(location, mask):\n","    return mask[location[0], location[1]] > 0\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["volume.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mask.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:14:17.958942Z","iopub.status.busy":"2023-03-18T23:14:17.958495Z","iopub.status.idle":"2023-03-18T23:14:18.45515Z","shell.execute_reply":"2023-03-18T23:14:18.454126Z","shell.execute_reply.started":"2023-03-18T23:14:17.958902Z"},"trusted":true},"outputs":[],"source":["is_in_mask_train = lambda x: is_in_masked_zone(x, mask)\n","\n","# Create a list to store train locations\n","locations = []\n","\n","# Generate train locations\n","volume_height,volume_width = volume.shape[:-1]\n","\n","for y in range(BUFFER, volume_height - BUFFER, BUFFER // 3):\n","    for x in range(BUFFER, volume_width - BUFFER, BUFFER // 3):\n","        if is_in_mask_train((y, x)):\n","            locations.append((y, x))\n","\n","# Convert the list of train locations to a PyTorch tensor\n","locations_ds = np.stack(locations, axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["locations_ds.shape"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Visualize some training patches\n","\n","Sanity check visually that our patches are where they should be."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:14:18.457083Z","iopub.status.busy":"2023-03-18T23:14:18.456727Z","iopub.status.idle":"2023-03-18T23:14:21.576187Z","shell.execute_reply":"2023-03-18T23:14:21.575222Z","shell.execute_reply.started":"2023-03-18T23:14:18.457045Z"},"trusted":true},"outputs":[],"source":["fig, ax = plt.subplots()\n","ax.imshow(labels)\n","\n","for y, x in locations_ds:\n","    patch = Rectangle([x - BUFFER, y - BUFFER], 2 * BUFFER, 2 * BUFFER, linewidth=2, edgecolor='r', facecolor='none')\n","    ax.add_patch(patch)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from scipy.stats import median_abs_deviation\n","all_MAD = median_abs_deviation(volume, axis=[0, 1])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["all_median = np.median(volume, axis=[0, 1])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mean = np.mean(volume)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mean"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["std = np.std(volume)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["std"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["possible_max_input = ((2 ** 8 - 1) - all_median.min()) / all_MAD.min()\n","possible_max_input"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["possible_min_input = ((0) - all_median.min()) / all_MAD.min()\n","possible_min_input"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"all_median\", all_median)\n","\"all_median\", all_median"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"all_MAD\", all_MAD)\n","\"all_MAD\", all_MAD"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["printed = True\n","\n","def extract_subvolume(location, volume):\n","    global printed\n","    # print(np.unique(volume, return_counts=True, return_index=True))\n","    x = location[0]\n","    y = location[1]\n","    subvolume = volume[x-BUFFER:x+BUFFER, y-BUFFER:y+BUFFER, :].astype(np.float32)\n","    # print(\"subvolume[:, :, 0]\", subvolume[:, :, 0])\n","    median = np.full_like(subvolume, all_median).astype(np.float32)\n","    MAD = np.full_like(subvolume, all_MAD).astype(np.float32)\n","    # mean = np.mean(subvolume, axis=2)\n","    # mean = np.stack([mean for i in range(Z_DIM)], axis=2) + exp\n","    # MAD = median_abs_deviation(subvolume, axis=2)\n","    # print(\"MAD\", MAD[0, 0, :])\n","    # print(\"mean\", mean)\n","    # print(\"median\", median[0, 0, :])\n","    \n","    subvolume = (subvolume - median) / MAD\n","    \n","    if not printed:\n","        print(\"subvolume after taking care of median and MAD\", subvolume)\n","        printed = True\n","    \n","    return subvolume"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## SubvolumeDataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.preprocessing import OneHotEncoder\n","\n","class SubvolumeDataset(Dataset):\n","    def __init__(self, locations, volume, labels, buffer, is_train: bool, return_location: bool = False):\n","        self.locations = locations\n","        self.volume = volume\n","        self.labels = labels        \n","        self.buffer = buffer\n","        self.is_train = is_train\n","        self.return_location = return_location\n","\n","    def __len__(self):\n","        return len(self.locations)\n","\n","    def __getitem__(self, idx):\n","        label = None\n","        location = np.array(self.locations[idx])\n","        y, x = location[0], location[1]\n","\n","        subvolume = extract_subvolume(location, self.volume) - possible_min_input  \n","        # print(\"subvolume\", subvolume)\n","        # print(\"labels\", labels)\n","        # subvolume = subvolume.numpy()\n","        subvolume = subvolume\n","        \n","        if self.labels is not None:\n","            label = self.labels[y - self.buffer:y + self.buffer, x - self.buffer:x + self.buffer]\n","            # print(\"label\", label)\n","            # n_category = 2\n","            # label = np.eye(n_category)[label]\n","            label = np.stack([label], axis=-1)\n","            # label = label.numpy()\n","            # print(\"label.shape\", label.shape\n","        \n","        if self.is_train and label is not None:            \n","            \n","            # print(\"label\", label.dtype)\n","            # print(\"subvolume in dataset (before aug)\", subvolume)    \n","            size = int(BUFFER * 2)\n","            performed = A.Compose([\n","                A.ToFloat(max_value=possible_max_input - possible_min_input),                \n","                A.HorizontalFlip(p=0.5), # 水平方向に反転\n","                A.VerticalFlip(p=0.5), # 水平方向に反転\n","                A.RandomBrightnessContrast(p=0.4),\n","                A.ShiftScaleRotate(p=0.8, border_mode=0), # シフト、スケーリング、回転\n","                # A.PadIfNeeded(min_height=size, min_width=size, always_apply=True, border_mode=0), # 必要に応じてパディングを追加\n","                # A.RandomCrop(height=size, width=size, always_apply=True), # ランダムにクロップ, Moduleの中で計算する際に次元がバッチ内で揃っている必要があるので最後にサイズは揃える\n","                A.Perspective(p=0.5), # パースペクティブ変換                \n","                A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n","                A.CoarseDropout(max_holes=1, max_width=int(size * 0.3), max_height=int(size * 0.3), \n","                                mask_fill_value=0, p=0.5),                \n","                A.Resize(BUFFER * 2, BUFFER * 2, always_apply=True),\n","                # A.Normalize(\n","                #     mean= [0] * Z_DIM,\n","                #     std= [1] * Z_DIM\n","                # ),\n","                # A.FromFloat(max_value=possible_max_input - possible_min_input),\n","            ])(image=subvolume, mask=label)            \n","            subvolume = performed[\"image\"]            \n","            label = performed[\"mask\"]\n","            # print(\"subvolume in dataset (after aug)\", subvolume)\n","            # print(\"label\", label.dtype)\n","            # print(\"subvolume\", subvolume.dtype)\n","            # →C, H, W\n","            subvolume = torch.from_numpy(subvolume.transpose(2, 0, 1).astype(np.float32))\n","            # print(performed)\n","            # print(subvolume.shape, label.shape)\n","            # H, W, C → C, H, W\n","            label = torch.from_numpy(label.transpose(2, 0, 1).astype(np.uint8)) \n","        else:\n","            performed = A.Compose([            \n","                A.ToFloat(max_value=possible_max_input - possible_min_input),\n","                # A.Normalize(\n","                #     mean= [0] * Z_DIM,\n","                #     std= [1] * Z_DIM\n","                # ),\n","                # A.FromFloat(max_value=possible_max_input - possible_min_input),\n","            ])(image=subvolume)\n","            subvolume = performed[\"image\"]\n","            subvolume = torch.from_numpy(subvolume.transpose(2, 0, 1).astype(np.float32))\n","            if label is not None:\n","                label = torch.from_numpy(label.transpose(2, 0, 1).astype(np.uint8)) \n","        if self.return_location:\n","            return subvolume, location\n","        return subvolume, label        "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Visualize validation dataset patches\n","\n","Note that they are partially overlapping, since the stride is half the patch size."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:14:31.265448Z","iopub.status.busy":"2023-03-18T23:14:31.264916Z","iopub.status.idle":"2023-03-18T23:14:35.752995Z","shell.execute_reply":"2023-03-18T23:14:35.751869Z","shell.execute_reply.started":"2023-03-18T23:14:31.265399Z"},"trusted":true},"outputs":[],"source":["def visualize_valid_dataset_patches(val_locations_ds):\n","    fig, ax = plt.subplots()\n","    ax.imshow(labels)\n","\n","    for y, x in val_locations_ds:\n","        patch = patches.Rectangle([x - BUFFER, y - BUFFER], 2 * BUFFER, 2 * BUFFER, linewidth=2, edgecolor='g', facecolor='none')\n","        ax.add_patch(patch)\n","    plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Compute a trivial baseline\n","\n","This is the highest validation score you can reach without looking at the inputs.\n","The model can be considered to have statistical power only if it can beat this baseline."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:14:35.755036Z","iopub.status.busy":"2023-03-18T23:14:35.754646Z","iopub.status.idle":"2023-03-18T23:14:38.733818Z","shell.execute_reply":"2023-03-18T23:14:38.73285Z","shell.execute_reply.started":"2023-03-18T23:14:35.754998Z"},"trusted":true},"outputs":[],"source":["def trivial_baseline(dataset):\n","    total = 0\n","    matches = 0.\n","    for _, batch_label in tqdm(dataset):\n","        batch_label = torch.tensor(batch_label)\n","        matches += torch.sum(batch_label.float())\n","        total += torch.numel(batch_label)\n","    return 1. - matches / total\n","\n","# score = trivial_baseline(val_ds).item()\n","# print(f\"Best validation score achievable trivially: {score * 100:.2f}% accuracy\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","class Model(pl.LightningModule):\n","    \n","    training_step_outputs = []\n","    validation_step_outputs = []\n","    test_step_outputs = []\n","        \n","\n","    def __init__(self, encoder_name, in_channels, out_classes, **kwargs):\n","        super().__init__()\n","        self.model = smp.UnetPlusPlus(\n","            encoder_name=encoder_name, \n","            encoder_weights=\"imagenet\",\n","            # encoder_weights=None,\n","            encoder_depth=5,\n","            decoder_channels=[1024, 512, 256, 128, 64],\n","            in_channels=in_channels,\n","            classes=out_classes,\n","            **kwargs,\n","        )\n","\n","        # preprocessing parameteres for image\n","        params = smp.encoders.get_preprocessing_params(encoder_name)\n","        # self.register_buffer(\"std\", torch.tensor(params[\"std\"]).view(1, 3, 1, 1))\n","        # self.register_buffer(\"mean\", torch.tensor(params[\"mean\"]).view(1, 3, 1, 1))\n","\n","        # for image segmentation dice loss could be the best first choice\n","        self.loss_fn = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n","\n","    def forward(self, image):\n","        # normalize image here\n","        # image = (image - self.mean) / self.std\n","        mask = self.model(image)\n","        return mask\n","\n","    def shared_step(self, batch, stage):\n","        \n","        subvolumes, labels = batch\n","        \n","        image, mask = subvolumes.float(), labels.float()\n","        labels = labels.squeeze(dim=1)\n","        # print(\"torch.unique(subvolumes)\", torch.unique(subvolumes), file=open(\"subvolumes_unique\", \"w\"))\n","\n","        # Shape of the image should be (batch_size, num_channels, height, width)\n","        # if you work with grayscale images, expand channels dim to have [batch_size, 1, height, width]\n","        assert image.ndim == 4\n","\n","        # Check that image dimensions are divisible by 32, \n","        # encoder and decoder connected by `skip connections` and usually encoder have 5 stages of \n","        # downsampling by factor 2 (2 ^ 5 = 32); e.g. if we have image with shape 65x65 we will have \n","        # following shapes of features in encoder and decoder: 84, 42, 21, 10, 5 -> 5, 10, 20, 40, 80\n","        # and we will get an error trying to concat these features\n","        h, w = image.shape[2:]\n","        assert h % 32 == 0 and w % 32 == 0\n","\n","        # Shape of the mask should be [batch_size, num_classes, height, width]\n","        # for binary segmentation num_classes = 1\n","        assert mask.ndim == 4\n","\n","        # Check that mask values in between 0 and 1, NOT 0 and 255 for binary segmentation\n","        assert mask.max() <= 1.0 and mask.min() >= 0\n","\n","        logits_mask = self.forward(image)\n","        \n","        # Predicted mask contains logits, and loss_fn param `from_logits` is set to True\n","        loss = self.loss_fn(logits_mask, mask)\n","\n","        # Lets compute metrics for some threshold\n","        # first convert mask values to probabilities, then \n","        # apply thresholding\n","        prob_mask = logits_mask.sigmoid()\n","        pred_mask = (prob_mask > threshold).float()\n","\n","        # We will compute IoU metric by two ways\n","        #   1. dataset-wise\n","        #   2. image-wise\n","        # but for now we just compute true positive, false positive, false negative and\n","        # true negative 'pixels' for each image and class\n","        # these values will be aggregated in the end of an epoch\n","        tp, fp, fn, tn = smp.metrics.get_stats(pred_mask.long(), mask.long(), mode=\"binary\")\n","\n","        return {\n","            \"loss\": loss,\n","            \"tp\": tp,\n","            \"fp\": fp,\n","            \"fn\": fn,\n","            \"tn\": tn,\n","        }\n","\n","    def shared_epoch_end(self, outputs, stage):\n","        # aggregate step metics\n","        tp = torch.cat([x[\"tp\"] for x in outputs])\n","        fp = torch.cat([x[\"fp\"] for x in outputs])\n","        fn = torch.cat([x[\"fn\"] for x in outputs])\n","        tn = torch.cat([x[\"tn\"] for x in outputs])\n","\n","        # per image IoU means that we first calculate IoU score for each image \n","        # and then compute mean over these scores\n","        per_image_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n","        \n","        # dataset IoU means that we aggregate intersection and union over whole dataset\n","        # and then compute IoU score. The difference between dataset_iou and per_image_iou scores\n","        # in this particular case will not be much, however for dataset \n","        # with \"empty\" images (images without target class) a large gap could be observed. \n","        # Empty images influence a lot on per_image_iou and much less on dataset_iou.\n","        dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n","\n","        metrics = {\n","            f\"{stage}_per_image_iou\": per_image_iou,\n","            f\"{stage}_dataset_iou\": dataset_iou,\n","        }\n","        \n","        self.log_dict(metrics, prog_bar=True)\n","\n","    def training_step(self, batch, batch_idx):\n","        out = self.shared_step(batch, \"train\")\n","        self.training_step_outputs.append(out)\n","        return out\n","\n","    def on_train_epoch_end(self):\n","        out = self.shared_epoch_end(self.training_step_outputs, \"train\")\n","        self.training_step_outputs.clear()\n","        return out\n","\n","    def validation_step(self, batch, batch_idx):\n","        out = self.shared_step(batch, \"valid\")\n","        self.validation_step_outputs.append(out)        \n","        return out\n","\n","    def on_validation_epoch_end(self):\n","        out = self.shared_epoch_end(self.validation_step_outputs, \"valid\")\n","        self.validation_step_outputs.clear()\n","        return out\n","\n","    def test_step(self, batch, batch_idx):\n","        global predictions_map, predictions_map_counts\n","\n","        patch_batch, loc_batch = batch\n","        \n","        loc_batch = loc_batch.long()\n","        patch_batch = patch_batch.float()\n","        predictions: torch.Tensor = self.forward(patch_batch)\n","        # print(\"predictions.shape\", predictions.shape)\n","        # print(\"predictions\", predictions)\n","        predictions = predictions.sigmoid()\n","        # print(\"Softmaxed predictions where conf is gt threshold\", predictions[predictions.gt(threshold)])\n","        # print(\"predictions.shape after sigmoid\", predictions.shape)\n","        # →(BATCH, W, H, C)\n","        predictions = torch.permute(predictions, (0, 3, 2, 1))\n","        predictions = predictions.cpu().numpy()  # move predictions to cpu and convert to numpy\n","        for (y, x), pred in zip(loc_batch, predictions):\n","            # print(\"index: \", index ,\"x, y, pred\", x.item(), y.item(), pred[BUFFER, BUFFER, :].item(), file=open('log.out', 'a'))\n","            print(\"pred\", pred)\n","            predictions_map[\n","                x - BUFFER : x + BUFFER, y - BUFFER : y + BUFFER, :\n","            ] += pred\n","            predictions_map_counts[x - BUFFER : x + BUFFER, y - BUFFER : y + BUFFER, :] += 1\n","        predictions_map /= (predictions_map_counts + exp)\n","        # print(\"predictions_map\", predictions_map)\n","        # print(\"predictions_map_count\", predictions_map_counts)\n","        self.test_step_outputs.append(predictions)\n","        return predictions\n","\n","    def on_test_epoch_end(self):\n","        self.test_step_outputs.clear()\n","        return\n","\n","    def configure_optimizers(self):\n","        optimizer = optim.Adam(self.parameters(), lr=lr)\n","        # Using a scheduler is optional but can be helpful.\n","        # The scheduler reduces the LR if the validation performance hasn't improved for the last N epochs\n","        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.2, patience=20, min_lr=5e-5)\n","        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"valid_dataset_iou\"}\n"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:14:39.034124Z","iopub.status.busy":"2023-03-18T23:14:39.033748Z","iopub.status.idle":"2023-03-18T23:28:08.685364Z","shell.execute_reply":"2023-03-18T23:28:08.684324Z","shell.execute_reply.started":"2023-03-18T23:14:39.034086Z"},"trusted":true},"outputs":[],"source":["\n","k_folds = 2\n","kfold = KFold(\n","    n_splits=k_folds,\n","    shuffle=True\n",")\n","\n","# Init the neural network\n","model = Model(\n","    encoder_name=\"se_resnext50_32x4d\",\n","    in_channels=Z_DIM,\n","    out_classes=1,\n",")\n","\n","# Initialize a trainer\n","trainer = pl.Trainer(\n","    max_epochs=num_epochs,\n","    devices=\"auto\",\n","    accelerator=\"auto\",\n","    log_every_n_steps=BATCH_SIZE // 4,\n",")\n","\n","for fold, (train_ids, val_ids) in enumerate(kfold.split(locations_ds)):\n","    print(f'FOLD {fold}')\n","    print('--------------------------------')\n","    \n","    # Sample elements randomly from a given list of ids, no replacement.\n","    train_ds = SubvolumeDataset(\n","        locations_ds[train_ids],\n","        volume,\n","        labels,\n","        BUFFER,\n","        is_train=True\n","    )\n","    val_ds = SubvolumeDataset(\n","        locations_ds[val_ids],\n","        volume,\n","        labels,\n","        BUFFER,\n","        is_train=False,\n","    )\n","    \n","    # Define data loaders for training and testing data in this fold\n","    train_loader = torch.utils.data.DataLoader(\n","        train_ds, \n","        batch_size=BATCH_SIZE,\n","        num_workers=num_workers,\n","        shuffle=True,\n","    )\n","    val_loader = torch.utils.data.DataLoader(\n","        val_ds, \n","        batch_size=BATCH_SIZE,\n","        num_workers=num_workers,\n","        shuffle=False,\n","    )\n","\n","    # Train the model\n","    trainer.fit(model, train_loader, val_loader)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Clear up memory"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# del volume\n","# del mask\n","# del labels\n","# del train_ds\n","# del val_ds\n","\n","import gc\n","gc.collect()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Compute predictions on test data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:28:09.711867Z","iopub.status.busy":"2023-03-18T23:28:09.711493Z","iopub.status.idle":"2023-03-18T23:28:09.724755Z","shell.execute_reply":"2023-03-18T23:28:09.723509Z","shell.execute_reply.started":"2023-03-18T23:28:09.711827Z"},"trusted":true},"outputs":[],"source":["\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","from tqdm import tqdm\n","from skimage.transform import resize as resize_ski\n","import pathlib\n","\n","predictions_map = None\n","predictions_map_counts = None\n","\n","def compute_predictions_map(split, index):\n","    global predictions_map\n","    global predictions_map_counts\n","    \n","    print(f\"Load data for {split}/{index}\")\n","\n","    test_volume = load_volume(split=split, index=index)\n","    test_mask = load_mask(split=split, index=index)    \n","\n","    test_locations = []\n","    stride = BUFFER // 4\n","    for y in range(BUFFER, test_volume.shape[0] - BUFFER, stride):\n","        for x in range(BUFFER, test_volume.shape[1] - BUFFER, stride):\n","            test_locations.append((y, x))\n","\n","    print(f\"{len(test_locations)} test locations (before filtering by mask)\")\n","\n","    # filter locations inside the mask\n","    test_locations = [loc for loc in test_locations if is_in_masked_zone(loc, test_mask)]\n","    \n","    print(f\"{len(test_locations)} test locations (after filtering by mask)\")\n","\n","    test_ds = SubvolumeDataset(test_locations, test_volume, None, BUFFER, is_train=False, return_location=True)\n","    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, num_workers=num_workers)        \n","\n","    # shape: (X, Y, C)\n","    predictions_map = np.zeros_like(test_volume[:, :, 0]).transpose((1, 0))[:, :, np.newaxis].astype(np.float64)\n","    predictions_map_counts = np.zeros_like(predictions_map).astype(np.uint8)\n","\n","    # print(\"test_volume.shape\", test_volume.shape)\n","    # print(\"predictions_map.shape\", predictions_map.shape)\n","\n","    # print(f\"Compute predictions\")\n","    \n","    model = Model.load_from_checkpoint(\n","        \"weights/weights.ckpt\",\n","        encoder_name=\"se_resnext50_32x4d\",\n","        in_channels=Z_DIM,\n","        out_classes=1,\n","    )\n","\n","    trainer = pl.Trainer(\n","        accelerator=\"gpu\",\n","        devices=\"1\",\n","        max_epochs=num_epochs,\n","    )\n","\n","    trainer.test(\n","        model=model,\n","        dataloaders=test_loader,\n","        verbose=True,\n","    )\n","    # print(\"predictions_map\", predictions_map, file=open(\"predictions_map\", \"w\"))\n","    return predictions_map\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:28:09.727518Z","iopub.status.busy":"2023-03-18T23:28:09.726481Z","iopub.status.idle":"2023-03-18T23:35:39.938224Z","shell.execute_reply":"2023-03-18T23:35:39.937169Z","shell.execute_reply.started":"2023-03-18T23:28:09.727465Z"},"trusted":true},"outputs":[],"source":["def rle(predictions_map, threshold):\n","    flat_img = (np.where(predictions_map.flatten() >= threshold, 1, 0)).astype(np.uint8)\n","    \n","    # Add padding at the beginning and end\n","    flat_img = np.pad(flat_img, pad_width=1, mode='constant', constant_values=0)\n","\n","    starts = np.where((flat_img[:-1] == 0) & (flat_img[1:] == 1))[0]\n","    ends = np.where((flat_img[:-1] == 1) & (flat_img[1:] == 0))[0]\n","\n","    lengths = ends - starts\n","\n","    return \" \".join(map(str, np.c_[starts, lengths].flatten()))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def update_submission(predictions_map, index):\n","    rle_ = rle(predictions_map, threshold=threshold)\n","    print(f\"{index},\" + rle_, file=open('submission.csv', 'a'))\n","    # print(f\"{index},\" + rle_, file=open('/kaggle/working/submission.csv', 'a'))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Resize prediction maps to their original size (for submission)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:35:44.578704Z","iopub.status.busy":"2023-03-18T23:35:44.578139Z","iopub.status.idle":"2023-03-18T23:35:49.82279Z","shell.execute_reply":"2023-03-18T23:35:49.821723Z","shell.execute_reply.started":"2023-03-18T23:35:44.578662Z"},"trusted":true},"outputs":[],"source":["print(\"Id,Predicted\", file=open('submission.csv', 'w'))\n","kind = \"test\"\n","folder = pathlib.Path(DATA_DIR) / kind\n","for p in list(folder.iterdir()):\n","    index = p.stem\n","    predictions_map = compute_predictions_map(split=kind, index=index)\n","    original_size = cv2.imread(DATA_DIR + f\"/{kind}/{index}/mask.png\", 0).shape[:2]\n","    # W, H, C → H, W, C\n","    predictions_map = predictions_map.transpose((1, 0, 2))    \n","    predictions_map = resize_ski(predictions_map, (original_size[0], original_size[1], 1)).squeeze(axis=-1)    \n","    print(\"original predictions_map size\", predictions_map.shape)    \n","    # H, W → W, H\n","    update_submission(predictions_map, index)\n","    plt.imsave(f\"{index}.png\", predictions_map, cmap=\"gray\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat":4,"nbformat_minor":4}
