{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Keras starter kit [full training set, UNet]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Note\n","\n","- 高い解像度でリサイズすることはprecisionの向上につながるため有効\n","- seresnextでチャネル間の相関を見れるので有効\n","- 文字の太さ・書き方に大きくバリアントがあるので、横の相関よりもdepthの相関の方が大事かもしれない\n","- 深さに関して、隣り合う深さ同士に大きな変化はない\n","- fpをfnよりも小さくしたい\n","- valid_scoreはCFG.thd依存\n","- valid_lossはCFG.loss1/loss2依存\n","- encoder内でdepthをクロップしてバッチで繋げた方が精度が良い\n","- maskに対しては有効ではないが、labelは的確なラベルを用いることで精度が向上\n","- depthは22~34 or 24 ~ 36\n","- 画像のサイズは大きい方が良いのか？（BUFFER / SHARED_HEIGHT）\n","    - 比率を同じにして試してみる\n","    - SHARED_HEIGHTをデカくするとデータがメモリに載らなさそうだった\n","- NetのweightsにBatchNormはない方が良い\n","- BUFFER:strideを160:96から160:80にしたら精度が落ちた\n","    - trainの精度は上がっていた\n","    - BUFFERに対してstrideが細かすぎると過学習に繋がっているのかもしれない\n","- 文字が見えなくなるよりも、高いthdを設定して、文字が大きく見えた方が良い（仮説）\n","- 学習時にはstrideは大きくし、過学習を防ぐ。識別時にはstrideは小さくし見落としを減らす\n","- バイリニアよりもバイキュービックが良い\n","- ノイズについて\n","    - 大きすぎると良くない(intensityが(0.0001, 0.0005)くらいが良かった)\n","        - 元画像が255 * 255の範囲であることを踏まえると、1/255の変化で255のずれがある\n","- lossのαとβについて\n","    - 理論的にはαは大きい方が良い（偽陽性fpはスコアを大きく下げるので）\n","    - しかし、thdを用いて偽陽性を防ぐ手法もあるため、学習時にはあえて偽陰性に強いペナルティを与えた方が良いのかもしれない\n","    - Stacked UnetのI層目では高いβで見落としをなくして、二層目で高いαにしてノイズを除去する作戦\n","- 文字サイズとリサイズサイズについて\n","- thresholdを変えるだけでpublic lbが大きく変わる\n","    - 0.5から0.7にしたら0.05上がった（0.58→0.63）"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:10:40.298967Z","iopub.status.busy":"2023-03-18T23:10:40.298008Z","iopub.status.idle":"2023-03-18T23:10:49.514216Z","shell.execute_reply":"2023-03-18T23:10:49.513058Z","shell.execute_reply.started":"2023-03-18T23:10:40.298921Z"},"trusted":true},"outputs":[],"source":["\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import wandb\n","import torchvision\n","import datetime\n","# import cupy\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","import pytorch_lightning\n","import segmentation_models_pytorch as smp\n","import pytorch_lightning as pl\n","import pytorch_lightning.callbacks.model_checkpoint\n","import pytorch_lightning.plugins\n","from skimage.transform import resize as resize_ski\n","from pytorch_lightning.strategies.ddp import DDPStrategy\n","from pytorch_lightning.loggers import WandbLogger\n","from einops import rearrange, reduce, repeat\n","import torch.nn.functional as F\n","import segmentation_models_pytorch as smp\n","from segmentation_models_pytorch.decoders.unet.decoder import UnetDecoder, DecoderBlock\n","from timm.models.resnet import resnet10t, resnet34d, resnet50d, resnet14t, seresnext26d_32x4d, seresnext50_32x4d\n","import os\n","import torch.utils.data\n","from dataclasses import dataclass\n","\n","from scipy.ndimage import distance_transform_edt\n","\n","import ssl\n","ssl._create_default_https_context = ssl._create_unverified_context\n","\n","import glob\n","import time\n","import PIL.Image as Image\n","import matplotlib.pyplot as plt\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import Rectangle\n","import matplotlib.patches as patches\n","from sklearn.model_selection import KFold\n","from tqdm import tqdm\n","import cv2\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data\n","\n","\n","@dataclass\n","class CFG():\n","    # Data config\n","    # DATA_DIR = '/kaggle/input/vesuvius-challenge-ink-detection/'\n","    # DATA_DIR = '/home/fummicc1/codes/competitions/kaggle-ink-detection'\n","    DATA_DIR = '/home/fummicc1/codes/Kaggle/kaggle-ink-detection'\n","    BUFFER = 128 # Half-size of papyrus patches we'll use as model inputs\n","    STRIDE = 64\n","    # Z_LIST = list(range(0, 20, 5)) + list(range(22, 34))  # Offset of slices in the z direction\n","    Z_LIST = [20, 22, 24, 26] + list(range(28, 36)) + [36, 38]\n","    # Z_LIST = list(range(0, 24, 8)) + list(range(24, 36, 2)) + list(range(36, 64, 10))\n","    Z_DIM = len(Z_LIST)  # Number of slices in the z direction. Max value is 64 - Z_START\n","    SHARED_HEIGHT = 3200  # Max height to resize all papyrii\n","\n","    # Model config\n","    BATCH_SIZE = 18\n","\n","    device = torch.device(\"cuda\")\n","    threshold = 0.6\n","    num_workers = 8\n","    exp = 1e-7\n","    mask_padding = BUFFER\n","\n","    num_epochs = 30\n","    lr = 5e-4\n","    eta_min_lr = 1e-6\n","    WANDB_NOTE = \"\"\n","    loss1_alpha = 0.5\n","    loss1_beta = 0.5\n","    loss1_weight = 0.5\n","    loss2_alpha = 0.5\n","    loss2_beta = 0.5\n","    loss2_weight = 0.5\n","    lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts\n","    loss1 = smp.losses.TverskyLoss(\n","        smp.losses.BINARY_MODE,\n","        log_loss=False,\n","        from_logits=True, \n","        smooth=1e-7,\n","        alpha=loss1_alpha,\n","        beta=loss1_beta,\n","    )\n","    loss2 = smp.losses.TverskyLoss(\n","        smp.losses.BINARY_MODE,\n","        log_loss=False,\n","        from_logits=True, \n","        smooth=1e-7,\n","        alpha=loss2_alpha,\n","        beta=loss2_beta,\n","    )\n","    noise_intensity = (0.0001, 0.0005)\n","    use_new_label_mask = True\n","    pretrained = True\n","    \n","def class2dict(c):\n","    return {attr: getattr(c, attr) for attr in dir(c) if not callable(getattr(c, attr)) and not attr.startswith(\"__\")}"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Load up the training data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:10:52.653587Z","iopub.status.busy":"2023-03-18T23:10:52.653064Z","iopub.status.idle":"2023-03-18T23:10:57.915118Z","shell.execute_reply":"2023-03-18T23:10:57.914028Z","shell.execute_reply.started":"2023-03-18T23:10:52.653544Z"},"trusted":true},"outputs":[],"source":["def resize(img):\n","    current_height, current_width = img.shape    \n","    aspect_ratio = current_width / current_height\n","    if CFG.SHARED_HEIGHT is None:\n","        return img\n","    # new_height = CFG.SHARED_HEIGHT\n","    # pad_y = new_height - current_height\n","    # if pad_y > 0:\n","    #     # 元画像が小さい場合は解像度を大きくしないでpaddingをつける\n","    #     img = np.pad(img, [(0, pad_y), (0, 0)], constant_values=0)\n","    # else:\n","    # 既に十分でかい場合はリサイズする\n","    # 本当はpaddingしたいけど、メモリサイズが大きくなる\n","    new_height = CFG.SHARED_HEIGHT\n","    new_width = int(CFG.SHARED_HEIGHT * aspect_ratio)\n","    new_size = (new_width, new_height)\n","    # (W, H)の順で渡すが結果は(H, W)になっている\n","    img = cv2.resize(img, new_size)\n","    return img\n","\n","def load_mask(split, index): \n","    if index == \"2a\" or index == \"2b\":\n","        mode = index[1]\n","        index = \"2\"\n","    img = cv2.imread(f\"{CFG.DATA_DIR}/{split}/{index}/mask.png\", 0) // 255\n","    if index == \"2\":\n","        h = 9456\n","        if mode == \"a\":\n","            img = img[h:, :]\n","        elif mode == \"b\":   \n","            img = img[:h, :]\n","    img = resize(img)\n","    img = np.pad(img, 1, constant_values=0)\n","    dist = distance_transform_edt(img)\n","    img[dist <= CFG.mask_padding] = 0\n","    img = img[1:-1, 1:-1]    \n","    return img\n","\n","\n","def load_labels(split, index):\n","    if index == \"2a\" or index == \"2b\":\n","        mode = index[1]\n","        index = \"2\"\n","    suffix = \"_new\" if CFG.use_new_label_mask else \"\"\n","    img = cv2.imread(f\"{CFG.DATA_DIR}/{split}/{index}/inklabels{suffix}.png\", 0) // 255    \n","    if index == \"2\":\n","        h = 9456\n","        if mode == \"a\":\n","            img = img[h:, :]\n","        elif mode == \"b\":   \n","            img = img[:h, :]\n","    img = resize(img)\n","    return img"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# input shape: (H, W, C)\n","def rotate90(volume: np.ndarray, k=None, reverse=False):    \n","    if k:\n","        volume = np.rot90(volume, k)\n","    else:\n","        volume = np.rot90(volume, 1 if not reverse else 3)\n","    height = volume.shape[0]\n","    width = volume.shape[1]\n","    new_height = CFG.SHARED_HEIGHT\n","    new_width = int(new_height * width / height)\n","    if len(volume.shape) == 2:\n","        return cv2.resize(volume, (new_width, new_height))\n","    return resize_ski(volume, (new_height, new_width, volume.shape[2]))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:11:02.854397Z","iopub.status.busy":"2023-03-18T23:11:02.85384Z","iopub.status.idle":"2023-03-18T23:11:02.867203Z","shell.execute_reply":"2023-03-18T23:11:02.866021Z","shell.execute_reply.started":"2023-03-18T23:11:02.854351Z"},"trusted":true},"outputs":[],"source":["def load_volume(split, index):\n","    if index == \"2a\" or index == \"2b\":\n","        mode = index[1]\n","        index = \"2\"\n","    # Load the 3d x-ray scan, one slice at a time\n","    all = sorted(glob.glob(f\"{CFG.DATA_DIR}/{split}/{index}/surface_volume/*.tif\"))\n","    z_slices_fnames = [all[i] for i in range(len(all)) if i in CFG.Z_LIST]\n","    assert len(z_slices_fnames) == CFG.Z_DIM\n","    z_slices = []\n","    for z, filename in  tqdm(enumerate(z_slices_fnames)):\n","        img = cv2.imread(filename, -1)\n","        if index == \"2\":\n","            h = 9456\n","            if mode == \"a\":\n","                img = img[h:, :]\n","            elif mode == \"b\":\n","                img = img[:h, :]\n","        img = resize(img)\n","        # img = (img / (2 ** 8)).astype(np.uint8)\n","        img = img.astype(np.float32) // 255\n","        z_slices.append(img)\n","    return np.stack(z_slices, axis=-1)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Create a dataset in the input volume\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def is_in_masked_zone(location, mask):\n","    return mask[location[0], location[1]] > 0\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:14:17.958942Z","iopub.status.busy":"2023-03-18T23:14:17.958495Z","iopub.status.idle":"2023-03-18T23:14:18.45515Z","shell.execute_reply":"2023-03-18T23:14:18.454126Z","shell.execute_reply.started":"2023-03-18T23:14:17.958902Z"},"trusted":true},"outputs":[],"source":["def generate_locations_ds(volume, mask, label=None, skip_zero=False):\n","    is_in_mask_train = lambda x: is_in_masked_zone(x, mask)\n","\n","    # Create a list to store train locations\n","    locations = []\n","\n","    # Generate train locations\n","    volume_height, volume_width = volume.shape[:-1]\n","\n","    for y in range(CFG.BUFFER, volume_height - CFG.BUFFER, CFG.STRIDE):\n","        for x in range(CFG.BUFFER, volume_width - CFG.BUFFER, CFG.STRIDE):\n","            if skip_zero and label is not None and np.all(label[y - CFG.BUFFER // 2 : y + CFG.BUFFER // 2, x - CFG.BUFFER // 2 : x + CFG.BUFFER // 2] == 0):\n","                # print(f\"skip location at (y: {y}, x: {x})\")\n","                continue\n","            if is_in_mask_train((y, x)):\n","                locations.append((y, x))\n","\n","    # Convert the list of train locations to a PyTorch tensor\n","    locations_ds = np.stack(locations, axis=0)\n","    return locations_ds"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Visualize some training patches\n","\n","Sanity check visually that our patches are where they should be."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def extract_subvolume(location, volume):\n","    global printed\n","    y = location[0]\n","    x = location[1]\n","    subvolume = volume[y-CFG.BUFFER:y+CFG.BUFFER, x-CFG.BUFFER:x+CFG.BUFFER, :].astype(np.float32)\n","    \n","    return subvolume"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## SubvolumeDataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","from albumentations.core.transforms_interface import ImageOnlyTransform\n","\n","class MultichannelNoise(ImageOnlyTransform):\n","\n","    def __init__(self, intensity=CFG.noise_intensity, always_apply=False, p=0.5):\n","        super().__init__(always_apply, p)\n","        self.intensity = intensity\n","\n","    def apply(self, img, **params):\n","        intensity = np.random.uniform(*self.intensity)\n","        noise = np.random.normal(loc=0, scale=intensity*255, size=img.shape)\n","        img = img + noise\n","        return np.clip(img, 0, 255).astype(np.float32) # クリップして0から255の範囲に保つ\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.preprocessing import OneHotEncoder\n","\n","from albumentations.core.transforms_interface import ImageOnlyTransform\n","\n","class SubvolumeDataset(Dataset):\n","    def __init__(self, locations, volume, labels, buffer, is_train: bool, return_location: bool = False):\n","        self.locations = locations\n","        self.volume = volume\n","        self.labels = labels        \n","        self.buffer = buffer\n","        self.is_train = is_train\n","        self.return_location = return_location\n","\n","    def __len__(self):\n","        return len(self.locations)\n","\n","    def __getitem__(self, idx):\n","        label = None\n","        location = np.array(self.locations[idx])\n","        y, x = location[0], location[1]\n","\n","        subvolume = extract_subvolume(location, self.volume)\n","        \n","        if self.labels is not None:\n","            label = self.labels[y - self.buffer:y + self.buffer, x - self.buffer:x + self.buffer]            \n","            label = np.stack([label], axis=-1)            \n","            \n","        # 段々meanは小さくなる\n","        mean = np.array([0.5 - i / 100 for i in range(0, CFG.Z_DIM)]).reshape(-1, 1, 1)\n","        # 段々stdは小さくなる\n","        std = np.array([0.22 - i / 300 for i in range(0, CFG.Z_DIM)]).reshape(-1, 1, 1)\n","        \n","        if self.is_train and label is not None:          \n","            transformed = A.Compose([\n","                A.HorizontalFlip(p=0.4),\n","                A.VerticalFlip(p=0.4),\n","                A.RandomScale(p=0.4, scale_limit=0.4),\n","                A.Transpose(p=0.4),\n","                A.RandomRotate90(p=0.4),\n","                A.ShiftScaleRotate(p=0.5, scale_limit=0.4,),                \n","                # A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n","                # A.CoarseDropout(\n","                #     max_holes=1, \n","                #     max_width=int(CFG.BUFFER * 2 * 0.3),\n","                #     max_height=int(CFG.BUFFER * 2 * 0.3), \n","                #     mask_fill_value=0, \n","                #     p=0.5\n","                # ),\n","                MultichannelNoise(\n","                    p=0.2,\n","                ),\n","                A.GridDistortion(p=0.6),\n","                # A.CoarseDropout(\n","                #     max_holes=1,\n","                #     max_width=int(self.buffer * 0.15),\n","                #     max_height=int(self.buffer * 0.15),\n","                #     mask_fill_value=0,\n","                #     p=0.5,\n","                # ),\n","                # A.GridDropout(p=0.15),\n","                A.PadIfNeeded(min_height=self.buffer * 2, min_width=self.buffer * 2),\n","                A.Resize(height=self.buffer * 2, width=self.buffer * 2),\n","            ])(image=subvolume, mask=label)\n","            subvolume = transformed[\"image\"]\n","            label = transformed[\"mask\"]\n","            subvolume = np.transpose(subvolume, (2, 0, 1))\n","            label = np.transpose(label, (2, 0, 1))\n","            subvolume /= 255.\n","            subvolume = (subvolume - mean) / std       \n","        else:\n","            if label is None:\n","                subvolume = np.transpose(subvolume, (2, 0, 1))\n","                subvolume /= 255.\n","                subvolume = (subvolume - mean) / std\n","            else:\n","                # print(\"subvolume in val dataset (before aug)\", subvolume, file=open(\"before-val-aug.log\", \"w\")) \n","                subvolume = np.transpose(subvolume, (2, 0, 1))\n","                label = np.transpose(label, (2, 0, 1))\n","                subvolume /= 255.\n","                subvolume = (subvolume - mean) / std\n","        # print(\"subvolume\", subvolume)\n","        if self.return_location:\n","            return subvolume, location\n","        return subvolume, label        "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Visualize validation dataset patches\n","\n","Note that they are partially overlapping, since the stride is half the patch size."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:14:31.265448Z","iopub.status.busy":"2023-03-18T23:14:31.264916Z","iopub.status.idle":"2023-03-18T23:14:35.752995Z","shell.execute_reply":"2023-03-18T23:14:35.751869Z","shell.execute_reply.started":"2023-03-18T23:14:31.265399Z"},"trusted":true},"outputs":[],"source":["def visualize_dataset_patches(locations_ds, labels, mode: str, fold = 0):\n","    fig, ax = plt.subplots()\n","    ax.imshow(labels)\n","\n","    for y, x in locations_ds:\n","        patch = patches.Rectangle([x - CFG.BUFFER, y - CFG.BUFFER], 2 * CFG.BUFFER, 2 * CFG.BUFFER, linewidth=2, edgecolor='g', facecolor='none')\n","        ax.add_patch(patch)\n","    plt.savefig(f\"fold-{fold}-{mode}.png\")\n","    plt.show()    "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Compute a trivial baseline\n","\n","This is the highest validation score you can reach without looking at the inputs.\n","The model can be considered to have statistical power only if it can beat this baseline."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Dataset check"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ref - https://www.kaggle.com/competitions/vesuvius-challenge-ink-detection/discussion/397288\n","def fbeta_score(preds, targets, threshold, beta=0.5, smooth=1e-5):\n","    preds_t = torch.where(preds > threshold, 1.0, 0.0).float()\n","    y_true_count = targets.sum()\n","    \n","    ctp = preds_t[targets==1].sum()\n","    cfp = preds_t[targets==0].sum()\n","    beta_squared = beta * beta\n","\n","    c_precision = ctp / (ctp + cfp + smooth)\n","    c_recall = ctp / (y_true_count + smooth)\n","    dice = (1 + beta_squared) * (c_precision * c_recall) / (beta_squared * c_precision + c_recall + smooth)\n","\n","    return dice"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class SmpUnetDecoder(nn.Module):\n","\tdef __init__(\n","\t\tself,\n","\t\tin_channel,\n","\t\tskip_channel,\n","\t\tout_channel,\n","\t):\n","\t\tsuper().__init__()\n","\t\tself.center = nn.Identity()\n","\n","\t\ti_channel = [\n","\t\t\tin_channel,\n","\t\t] + out_channel[:-1]\n","\t\ts_channel = skip_channel\n","\t\to_channel = out_channel\n","\t\tblock = [\n","\t\t\tDecoderBlock(i, s, o, use_batchnorm=True, attention_type=None)\n","\t\t\tfor i, s, o in zip(i_channel, s_channel, o_channel)\n","\t\t]\n","\t\tself.block = nn.ModuleList(block)\n","\n","\tdef forward(self, feature, skip):\n","\t\td = self.center(feature)\n","\t\tdecode = []\n","\t\tfor i, block in enumerate(self.block):\n","\t\t\ts = skip[i]\n","\t\t\td = block(d, s)\n","\t\t\tdecode.append(d)\n","\n","\t\tlast = d\n","\t\treturn last, decode\n","\n","\n","class Net(nn.Module):\n","\tdef __init__(\n","\t\tself,\n","\t):\n","\t\tsuper().__init__()\n","\t\tself.output_type = [\"inference\", \"loss\"]\n","\n","\t\tconv_dim = 64\n","\t\tencoder1_dim = [\n","\t\t\tconv_dim,\n","\t\t\t256,\n","\t\t\t512,\n","\t\t\t1024,\n","\t\t\t2048,\n","\t\t]\n","\t\tdecoder1_dim = [\n","\t\t\t256,\n","\t\t\t128,\n","\t\t\t64,\n","\t\t\t64,\n","\t\t]\n","\n","\t\tself.encoder1 = seresnext26d_32x4d(pretrained=CFG.pretrained, in_chans=CFG.Z_DIM - 8)\n","\n","\t\tself.decoder1 = SmpUnetDecoder(\n","\t\t\tin_channel=encoder1_dim[-1],\n","\t\t\tskip_channel=encoder1_dim[:-1][::-1],\n","\t\t\tout_channel=decoder1_dim,\n","\t\t)\n","\t\t# -- pool attention weight  \n","\t\tself.weight1 = nn.ModuleList(\n","\t\t\t[\n","\t\t\t\tnn.Sequential(\n","\t\t\t\t\tnn.Conv2d(dim, dim, kernel_size=3, padding=1),\n","\t\t\t\t\tnn.ReLU(inplace=True),\n","\t\t\t\t)\n","\t\t\t\tfor dim in encoder1_dim\n","\t\t\t]\n","\t\t)\n","\t\tself.logit1 = nn.Conv2d(decoder1_dim[-1], 1, kernel_size=1)\n","\n","\t\t# --------------------------------\n","\t\t#\n","\t\tencoder2_dim = [64, 128, 256, 512]  #\n","\t\tdecoder2_dim = [\n","\t\t\t128,\n","\t\t\t64,\n","\t\t\t32,\n","\t\t]\n","\t\tself.encoder2 = resnet10t(pretrained=CFG.pretrained, in_chans=decoder1_dim[-1])\n","\n","\t\tself.decoder2 = SmpUnetDecoder(\n","\t\t\tin_channel=encoder2_dim[-1],\n","\t\t\tskip_channel=encoder2_dim[:-1][::-1],\n","\t\t\tout_channel=decoder2_dim,\n","\t\t)\n","\t\tself.logit2 = nn.Conv2d(decoder2_dim[-1], 1, kernel_size=1)\n","\n","\tdef forward(self, batch):\n","\t\tv = batch\n","\t\tB, C, H, W = v.shape\n","\t\tvv = [\n","\t\t\tv[:, i : i + CFG.Z_DIM - 8]\n","\t\t\tfor i in [0, 3, 4, 5, 8]\n","\t\t]\n","\t\tK = len(vv)\n","\t\tx = torch.cat(vv, 0)\n","\t\t# x = v\n","\n","\t\t# ----------------------\n","\t\tencoder = []\n","\t\te = self.encoder1\n","\t\tx = e.conv1(x)\n","\t\tx = e.bn1(x)\n","\t\tx = e.act1(x)\n","\t\tencoder.append(x)\n","\t\tx = F.avg_pool2d(x, kernel_size=2, stride=2)\n","\t\tx = e.layer1(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer2(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer3(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer4(x)\n","\t\tencoder.append(x)\n","\t\t# print('encoder', [f.shape for f in encoder])\n","\n","\t\tfor i in range(len(encoder)):\n","\t\t\te = encoder[i]\n","\t\t\tf = self.weight1[i](e)\n","\t\t\t_, c, h, w = e.shape\n","\t\t\tf = rearrange(f, \"(K B) c h w -> B K c h w\", K=K, B=B, h=h, w=w)  #\n","\t\t\te = rearrange(e, \"(K B) c h w -> B K c h w\", K=K, B=B, h=h, w=w)  #\n","\t\t\tw = F.softmax(f, 1)\n","\t\t\te = (w * e).sum(1)\n","\t\t\tencoder[i] = e\n","\n","\t\tfeature = encoder[-1]\n","\t\tskip = encoder[:-1][::-1]\n","\t\tlast, decoder = self.decoder1(feature, skip)\n","\t\tlogit1 = self.logit1(last)\n","  \n","\t\tlogit1 = F.interpolate(\n","\t\t\tlogit1, size=(H, W), mode=\"bicubic\", align_corners=False, antialias=True\n","\t\t)\n","\n","\t\t# ----------------------\n","\t\tx = last  # .detach()\n","\t\t# x = F.avg_pool2d(x,kernel_size=2,stride=2)\n","\t\tencoder = []\n","\t\te = self.encoder2\n","\t\tx = e.layer1(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer2(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer3(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer4(x)\n","\t\tencoder.append(x)\n","\n","\t\tfeature = encoder[-1]\n","\t\tskip = encoder[:-1][::-1]\n","\t\tlast, decoder = self.decoder2(feature, skip)\n","\t\tlogit2 = self.logit2(last)\n","\t\tlogit2 = F.interpolate(\n","\t\t\tlogit2, size=(H, W), mode=\"bicubic\", align_corners=False, antialias=True\n","\t\t)\n","\t\treturn logit1, logit2\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Net()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tc = torch\n","def TTA(x:tc.Tensor,model:nn.Module):\n","    #x.shape=(batch,c,h,w)\n","    shape=x.shape\n","    x=[x,*[tc.rot90(x,k=i,dims=(-2,-1)) for i in range(1,4)]]\n","    x=tc.cat(x,dim=0)\n","    _, x = model(x)\n","    x=x.reshape(4,shape[0], 1 ,*shape[2:])\n","    x=[tc.rot90(x[i],k=-i,dims=(-2,-1)) for i in range(4)]\n","    x=tc.stack(x,dim=0)\n","    return x.mean(0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def dice_coef_torch(prob_preds, targets, beta=0.5, smooth=1e-5):\n","    # No need to binarize the predictions\n","    # prob_preds = torch.sigmoid(preds)\n","\n","    # flatten label and prediction tensors\n","    prob_preds = prob_preds.view(-1).float()\n","    targets = targets.view(-1).float()\n","\n","    intersection = (prob_preds * targets).sum()\n","\n","    dice = ((1 + beta**2) * intersection + smooth) / ((beta**2) * prob_preds.sum() + targets.sum() + smooth)\n","\n","    return dice\n","\n","\n","\n","class Model(pl.LightningModule):\n","    training_step_outputs = []\n","    validation_step_outputs = []\n","    test_step_outputs = [[], []]\n","\n","    def __init__(self, **kwargs):\n","        super().__init__()\n","\n","        self.model = Net()        \n","\n","        # self.loss1 = nn.BCEWithLogitsLoss(\n","        #     pos_weight=torch.tensor([0.7])\n","        # )\n","        # self.loss2 = nn.BCEWithLogitsLoss(\n","        #     pos_weight=torch.tensor([0.7])\n","        # )\n","        self.loss1 = CFG.loss1\n","        self.loss2 = CFG.loss2\n","\n","    def forward(self, image, stage):\n","        if stage != \"train\":\n","            mask = TTA(image, self.model)\n","        else:\n","            mask = self.model(image)\n","        return mask\n","\n","    def shared_step(self, batch, stage):\n","        subvolumes, labels = batch\n","\n","        image, labels = subvolumes.float(), labels.float()        \n","        assert image.ndim == 4\n","        \n","        h, w = image.shape[2:]\n","        assert h % 32 == 0 and w % 32 == 0\n","        \n","        # print(\"labels\", labels.max(), labels.min())\n","\n","        assert labels.max() <= 1.0 and labels.min() >= 0\n","\n","        if stage == \"train\":\n","            logit1, logit2 = self.forward(image, stage)\n","            loss = CFG.loss1_weight * self.loss1(logit1, labels) + CFG.loss2_weight * self.loss2(logit2, labels)\n","        elif stage == \"valid\":\n","            logit2 = self.forward(image, stage)\n","            loss = self.loss2(logit2, labels)\n","        \n","        prob2 = torch.sigmoid(logit2)\n","\n","        pred_mask = (prob2 > CFG.threshold).float()\n","        \n","        # print(\"pred_mask\", pred_mask)\n","        \n","        score = fbeta_score(pred_mask, labels, threshold=CFG.threshold)\n","\n","        tp, fp, fn, tn = smp.metrics.get_stats(\n","            pred_mask.long(), labels.long(), mode=\"binary\"\n","        )\n","\n","        return {\n","            \"loss\": loss,\n","            \"tp\": tp,\n","            \"fp\": fp,\n","            \"fn\": fn,\n","            \"tn\": tn,\n","            \"score\": score,\n","        }\n","\n","    def shared_epoch_end(self, outputs, stage):\n","        # aggregate step metics\n","        tp = torch.cat([x[\"tp\"] for x in outputs])\n","        fp = torch.cat([x[\"fp\"] for x in outputs])\n","        fn = torch.cat([x[\"fn\"] for x in outputs])\n","        tn = torch.cat([x[\"tn\"] for x in outputs])\n","        loss = torch.mean(torch.Tensor([x[\"loss\"] for x in outputs]))\n","        fbeta_score = torch.mean(torch.Tensor([x[\"score\"] for x in outputs]))\n","\n","        per_image_iou = smp.metrics.iou_score(\n","            tp, fp, fn, tn, reduction=\"micro-imagewise\"\n","        )\n","\n","        dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n","\n","        metrics = {\n","            f\"{stage}_per_image_iou\": per_image_iou,\n","            f\"{stage}_dataset_iou\": dataset_iou,\n","            f\"{stage}_loss\": 10000 if loss.item() == 0 else loss.item(),\n","            f\"{stage}_tp\": tp.sum().int().item(),\n","            f\"{stage}_fp\": fp.sum().int().item(),\n","            f\"{stage}_fn\": fn.sum().int().item(),\n","            f\"{stage}_tn\": tn.sum().int().item(),\n","            f\"{stage}_score\": fbeta_score.item(),\n","        }\n","\n","        self.log_dict(metrics, prog_bar=True, sync_dist=True)\n","\n","    def training_step(self, batch, batch_idx):\n","        out = self.shared_step(batch, \"train\")\n","        self.training_step_outputs.append(out)\n","        return out\n","\n","    def on_train_epoch_end(self):\n","        out = self.shared_epoch_end(self.training_step_outputs, \"train\")\n","        self.training_step_outputs.clear()\n","        return out\n","\n","    def validation_step(self, batch, batch_idx):\n","        out = self.shared_step(batch, \"valid\")\n","        self.validation_step_outputs.append(out)\n","        return out\n","\n","    def on_validation_epoch_end(self):\n","        out = self.shared_epoch_end(self.validation_step_outputs, \"valid\")\n","        self.validation_step_outputs.clear()\n","        return out\n","\n","    def test_step(self, batch, batch_idx):\n","        global predictions_map, predictions_map_counts\n","\n","        patch_batch, loc_batch = batch\n","\n","        loc_batch = loc_batch.long()\n","        patch_batch = patch_batch.float()\n","        predictions = self.forward(patch_batch, \"test\")\n","        predictions = predictions.sigmoid()\n"," \n","        predictions = torch.permute(predictions, (0, 2, 3, 1)).squeeze(dim=-1)\n","        predictions = (\n","            predictions.cpu().numpy()\n","        )\n","        loc_batch = loc_batch.cpu().numpy()\n","        \n","        self.test_step_outputs[0].extend(loc_batch)\n","        self.test_step_outputs[1].extend(predictions)        \n","        return loc_batch, predictions\n","\n","    def on_test_epoch_end(self):\n","        global predictions_map, predictions_map_counts\n","\n","        locs = np.array(self.test_step_outputs[0])\n","        preds = np.array(self.test_step_outputs[1])\n","        print(\"locs\", locs.shape)\n","        print(\"preds\", preds.shape)\n","\n","\n","        for (y, x), pred in zip(locs, preds):\n","            predictions_map[\n","                y - CFG.BUFFER : y + CFG.BUFFER, x - CFG.BUFFER : x + CFG.BUFFER\n","            ] += pred\n","            predictions_map_counts[\n","                y - CFG.BUFFER : y + CFG.BUFFER, x - CFG.BUFFER : x + CFG.BUFFER\n","            ] += 1\n","        \n","        predictions_map /= predictions_map_counts + CFG.exp\n","\n","    def configure_optimizers(self):\n","        optimizer = optim.AdamW(self.parameters(), lr=CFG.lr)\n","\n","        scheduler = CFG.lr_scheduler(\n","            optimizer, T_0=CFG.num_epochs, T_mult=2, eta_min=CFG.eta_min_lr,\n","        )\n","        return {\n","            \"optimizer\": optimizer,\n","            \"lr_scheduler\": scheduler,\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pytorch_lightning.seed_everything(seed=42)\n","torch.set_float32_matmul_precision('high')\n","\n","\n","masks = load_mask(split=\"train\", index=1)\n","labels = load_labels(split=\"train\", index=1)\n","\n","fig, (ax1, ax2) = plt.subplots(1, 2)\n","ax1.set_title(\"mask.png\")\n","ax1.imshow(masks, cmap='gray')\n","ax2.set_title(\"inklabels.png\")\n","ax2.imshow(labels, cmap='gray')\n","plt.show()\n","\n","mask_test_a = load_mask(split=\"test\", index=\"a\")\n","mask_test_b = load_mask(split=\"test\", index=\"b\")\n","\n","mask_train_1 = load_mask(split=\"train\", index=\"1\")\n","labels_train_1 = load_labels(split=\"train\", index=\"1\")\n","\n","mask_train_2a = load_mask(split=\"train\", index=\"2a\")\n","labels_train_2a = load_labels(split=\"train\", index=\"2a\")\n","\n","mask_train_2b = load_mask(split=\"train\", index=\"2b\")\n","labels_train_2b = load_labels(split=\"train\", index=\"2b\")\n","\n","mask_train_3 = load_mask(split=\"train\", index=\"3\")\n","labels_train_3 = load_labels(split=\"train\", index=\"3\")\n","\n","print(f\"mask_test_a: {mask_test_a.shape}\")\n","print(f\"mask_test_b: {mask_test_b.shape}\")\n","print(\"-\")\n","print(f\"mask_train_1: {mask_train_1.shape}\")\n","print(f\"labels_train_1: {labels_train_1.shape}\")\n","print(\"-\")\n","print(f\"mask_train_2a: {mask_train_2a.shape}\")\n","print(f\"labels_train_2a: {labels_train_2a.shape}\")\n","print(\"-\")\n","print(f\"mask_train_2b: {mask_train_2b.shape}\")\n","print(f\"labels_train_2b: {labels_train_2b.shape}\")\n","print(\"-\")\n","print(f\"mask_train_3: {mask_train_3.shape}\")\n","print(f\"labels_train_3: {labels_train_3.shape}\")\n","\n","fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4)\n","\n","ax1.set_title(\"labels_train_1\")\n","ax1.imshow(labels_train_1, cmap='gray')\n","\n","ax2.set_title(\"labels_train_2a\")\n","ax2.imshow(labels_train_2a, cmap='gray')\n","\n","ax3.set_title(\"labels_train_2b\")\n","ax3.imshow(labels_train_2b, cmap='gray')\n","\n","ax4.set_title(\"labels_train_3\")\n","ax4.imshow(labels_train_3, cmap='gray')\n","plt.tight_layout()\n","plt.show()\n","\n","volume_train_1 = load_volume(split=\"train\", index=1)\n","print(f\"volume_train_1: {volume_train_1.shape}, {volume_train_1.dtype}\")\n","\n","volume_train_2a = load_volume(split=\"train\", index=\"2a\")\n","print(f\"volume_train_2a: {volume_train_2a.shape}, {volume_train_2a.dtype}\")\n","\n","volume_train_2b = load_volume(split=\"train\", index=\"2b\")\n","print(f\"volume_train_2b: {volume_train_2b.shape}, {volume_train_2b.dtype}\")\n","\n","volume_train_3 = load_volume(split=\"train\", index=3)\n","print(f\"volume_train_3: {volume_train_3.shape}, {volume_train_3.dtype}\")\n","\n","# volume = np.concatenate([volume_train_1, volume_train_2, volume_train_3], axis=1)\n","# volume = np.concatenate([volume_train_1, volume_train_2], axis=1)\n","# print(f\"total volume: {volume.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sample_data_name = \"2a\"\n","sample_volume = load_volume(\"train\", sample_data_name)\n","sample_mask = load_mask(\"train\", sample_data_name)\n","sample_label = load_labels(\"train\", sample_data_name)\n","sample_locations = generate_locations_ds(sample_volume, sample_mask, sample_label, skip_zero=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sample_ds = SubvolumeDataset(\n","    sample_locations,\n","    sample_volume,\n","    sample_label,\n","    CFG.BUFFER,\n","    is_train=True,\n",")\n","\n","id = 400\n","sample_depth = CFG.Z_DIM // 2\n","\n","fig, ax = plt.subplots(1, 2, figsize=(8, 6))\n","\n","img = sample_ds[id][0][sample_depth, :, :]\n","label = sample_ds[id][1][0, :, :]\n","ax[0].imshow(img)\n","ax[1].imshow(label)\n","\n","fig, ax = plt.subplots(figsize=(8, 6))\n","\n","ax.imshow(sample_label)\n","\n","y, x = sample_locations[id]\n","patch = patches.Rectangle([x - CFG.BUFFER, y - CFG.BUFFER], 2 * CFG.BUFFER, 2 * CFG.BUFFER, linewidth=2, edgecolor='g', facecolor='none')\n","ax.add_patch(patch)\n","plt.show()    \n","\n","fig, ax = plt.subplots(CFG.Z_DIM, 1, figsize=(12, 24))\n","\n","for i in range(CFG.Z_DIM):\n","    img, _ = sample_ds[id]\n","    img = img[i, :, :]\n","    ax[i].hist(img.flatten(), bins=1000)  # Plot histogram of the flattened data\n","    ax[i].set_title(f\"Histogram of Channel {i}\")  # Add title to the plot    \n","fig.tight_layout()\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-18T23:14:39.034124Z","iopub.status.busy":"2023-03-18T23:14:39.033748Z","iopub.status.idle":"2023-03-18T23:28:08.685364Z","shell.execute_reply":"2023-03-18T23:28:08.684324Z","shell.execute_reply.started":"2023-03-18T23:14:39.034086Z"},"trusted":true},"outputs":[],"source":["\n","k_folds = 4\n","kfold = KFold(\n","    n_splits=k_folds,\n","    shuffle=True\n",")\n","\n","data_list = [    \n","    (volume_train_1, labels_train_1, mask_train_1),\n","    (volume_train_2a, labels_train_2a, mask_train_2a),    \n","    (volume_train_2b, labels_train_2b, mask_train_2b),\n","    (volume_train_3, labels_train_3, mask_train_3),    \n","]\n","\n","predictions_map = None\n","predictions_map_counts = None\n","\n","for fold, (train_data, val_data) in enumerate(kfold.split(data_list)):\n","    print(f'FOLD {fold}')\n","    print('--------------------------------')\n","    print(\"train_data\", train_data)\n","    print(\"val_data\", val_data)\n","    one = data_list[train_data[0]]\n","    two = data_list[train_data[1]]\n","    three = data_list[train_data[2]]\n","    train_volume = np.concatenate([one[0], two[0], three[0]], axis=1)\n","    train_label = np.concatenate([one[1], two[1], three[1]], axis=1)\n","    train_mask = np.concatenate([one[2], two[2], three[2]], axis=1)\n","    val_volume, val_label, val_mask = data_list[val_data[0]]    \n","\n","    train_locations_ds = generate_locations_ds(train_volume, train_mask, train_label, skip_zero=True)\n","    val_location_ds = generate_locations_ds(val_volume, val_mask, skip_zero=False)\n","\n","    visualize_dataset_patches(train_locations_ds, train_label, \"train\", fold)\n","    visualize_dataset_patches(val_location_ds, val_label, \"val\", fold)\n","    \n","    # Init the neural network\n","    model = Model()\n","\n","    wandb.finish()\n","    # Initialize a trainer\n","    now = datetime.datetime.now()\n","    now = f\"{now}\".replace(\" \", \"\")\n","    \n","    checkpoint_callback = pytorch_lightning.callbacks.ModelCheckpoint(\n","        monitor='valid_score',\n","        dirpath=f'best-results-{now}/',\n","        mode=\"max\",\n","        filename='train-fold-' + str(fold) + '-{epoch:02d}-{valid_score:.3f}',\n","        save_last=True,\n","    )\n","    \n","    trainer = pl.Trainer(\n","        max_epochs=CFG.num_epochs,\n","        devices=\"0,1,2\",\n","        accelerator=\"gpu\",\n","        # strategy=\"ddp_find_unused_parameters_false\",\n","        # strategy=\"ddp_fork\",\n","        logger=WandbLogger(\n","            name=f\"2.5dimension-{now}-fold-{fold}-val-{val_data[0]}\",\n","            notes=CFG.WANDB_NOTE,\n","            config=class2dict(CFG()),\n","        ),\n","        callbacks=[checkpoint_callback],\n","        default_root_dir=os.path.join(CFG.DATA_DIR, f\"{now}-fold-{fold}-val-{val_data[0]}\"),\n","    )\n","    \n","    # Sample elements randomly from a given list of ids, no replacement.\n","    train_ds = SubvolumeDataset(\n","        train_locations_ds,\n","        train_volume,\n","        train_label,\n","        CFG.BUFFER,\n","        is_train=True\n","    )\n","    val_ds = SubvolumeDataset(\n","        val_location_ds,\n","        val_volume,\n","        val_label,\n","        CFG.BUFFER,\n","        is_train=False,\n","    )\n","\n","    # Define data loaders for training and testing data in this fold\n","    train_loader = torch.utils.data.DataLoader(\n","        train_ds,\n","        batch_size=CFG.BATCH_SIZE,\n","        num_workers=CFG.num_workers,\n","        shuffle=True,\n","        # persistent_workers=True,\n","        # pin_memory=True,\n","    )\n","    val_loader = torch.utils.data.DataLoader(\n","        val_ds, \n","        batch_size=CFG.BATCH_SIZE,\n","        num_workers=CFG.num_workers,\n","        shuffle=False,\n","        # pin_memory=True,\n","        # persistent_workers=True,\n","    )\n","\n","    # Train the model\n","    trainer.fit(model, train_loader, val_loader)   \n","\n","    # if trainer.global_rank == 0:\n","    #     val_ds = SubvolumeDataset(\n","    #         val_location_ds,\n","    #         val_volume,\n","    #         val_label,\n","    #         CFG.BUFFER,\n","    #         is_train=False,\n","    #         return_location=True,\n","    #     )\n","    #     val_loader = torch.utils.data.DataLoader(\n","    #         val_ds, \n","    #         batch_size=CFG.BATCH_SIZE,\n","    #         num_workers=CFG.num_workers,\n","    #         shuffle=False,\n","    #         # pin_memory=True,\n","    #         # persistent_workers=True,\n","    #     )\n","        \n","    #     test_trainer = pl.Trainer(\n","    #         devices=\"1\",\n","    #         accelerator=\"gpu\",\n","    #     )\n","\n","    #     predictions_map = np.zeros_like(val_volume[:, :, 0]).astype(np.float64)\n","    #     predictions_map_counts = np.zeros_like(predictions_map).astype(np.uint8)\n","        \n","    #     test_model = Model.load_from_checkpoint(\n","    #         f'best-results-{now}/' + \"last.ckpt\"\n","    #     )\n","\n","    #     test_trainer.test(\n","    #         model=test_model,\n","    #         dataloaders=val_loader,\n","    #         verbose=True,\n","    #     )\n","        \n","    #     print(\"predictions_map.shape\", predictions_map.shape)\n","    #     predictions_map = np.where(predictions_map >= CFG.threshold, 255, 0)\n","    #     plt.imsave(f\"2.5dimension-{now}-fold-{fold}-thd-{str(CFG.threshold)}_{str(CFG.BUFFER)}.png\", predictions_map, cmap=\"gray\")        \n","\n","wandb.finish()\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat":4,"nbformat_minor":4}
