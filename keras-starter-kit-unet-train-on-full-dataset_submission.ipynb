{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Keras starter kit [full training set, UNet]"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# import sys\n","\n","# sys.path.append('/kaggle/input/pretrainedmodels/pretrainedmodels-0.7.4')\n","# sys.path.append('/kaggle/input/segmentation-models-pytorch/segmentation_models.pytorch-master')\n","# sys.path.append('/kaggle/input/efficientnet-pytorch/EfficientNet-PyTorch-master')\n","# sys.path.append('/kaggle/input/timm-pytorch-image-models/pytorch-image-models-master')\n","# sys.path.append(\"/kaggle/input/einops/einops-master\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Setup"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-05-10T15:03:39.741181Z","iopub.status.busy":"2023-05-10T15:03:39.740765Z","iopub.status.idle":"2023-05-10T15:03:39.750574Z","shell.execute_reply":"2023-05-10T15:03:39.749341Z","shell.execute_reply.started":"2023-05-10T15:03:39.741144Z"},"trusted":true},"outputs":[],"source":["\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import wandb\n","import torchvision\n","import datetime\n","import imageio\n","# import cupy\n","from sklearn.model_selection import KFold\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","import pytorch_lightning\n","import segmentation_models_pytorch as smp\n","import pytorch_lightning as pl\n","import pytorch_lightning.callbacks.model_checkpoint\n","import pytorch_lightning.plugins\n","from skimage.transform import resize as resize_ski\n","from pytorch_lightning.strategies.ddp import DDPStrategy\n","from pytorch_lightning.loggers import WandbLogger\n","from einops import rearrange, reduce, repeat\n","import torch.nn.functional as F\n","import segmentation_models_pytorch as smp\n","from segmentation_models_pytorch.decoders.unet.decoder import UnetDecoder, DecoderBlock\n","from timm.models.resnet import resnet10t, resnet34d, resnet50d, resnet14t, seresnext26d_32x4d, seresnext50_32x4d\n","from timm.models.mvitv2 import mvitv2_base\n","import os\n","import torch.utils.data\n","from dataclasses import dataclass\n","\n","from scipy.ndimage import distance_transform_edt\n","\n","import ssl\n","ssl._create_default_https_context = ssl._create_unverified_context\n","\n","import glob\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import cv2\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data\n","import os,cv2\n","import gc\n","import sys\n","import matplotlib.patches as patches\n","import random\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import pytorch_lightning as pl\n","\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.cuda import amp\n","from torch.utils.data import Dataset, DataLoader\n","import segmentation_models_pytorch as smp\n","import albumentations as A\n","from IPython.display import Video\n","\n","# sys.path.append(\"/home/fummicc1/codes/competitions/kaggle-ink-detection\")\n","# sys.path.append(\"/kaggle/input/resnet3d\")\n","from resnet3d import generate_model\n","\n","pytorch_lightning.seed_everything(seed=42)\n","torch.set_float32_matmul_precision('high')\n","\n","\n","@dataclass\n","class CFG():\n","    # Data config\n","    # DATA_DIR = '/kaggle/input/vesuvius-challenge-ink-detection/'\n","    DATA_DIR = '/home/fummicc1/codes/competitions/kaggle-ink-detection'\n","    # DATA_DIR = '/home/fummicc1/codes/Kaggle/kaggle-ink-detection'\n","    BUFFER = 112 # Half-size of papyrus patches we'll use as model inputs\n","    CROP_SIZE = BUFFER * 2\n","    STRIDE = 64\n","    # Z_LIST = list(range(0, 20, 5)) + list(range(22, 34))  # Offset of slices in the z direction\n","    # Z_LIST = [20, 22, 24, 26] + list(range(28, 36)) + [36, 38]\n","    # Z_LIST = list(range(16, 48, 2))\n","    Z_LIST = list(range(16, 40, 2))\n","    # Z_LIST = list(range(0, 24, 8)) + list(range(24, 36, 2)) + list(range(36, 64, 10))\n","    Z_DIM = len(Z_LIST)  # Number of slices in the z direction. Max value is 64 - Z_START\n","    BATCH_Z_DIFF = 6\n","    SHARED_HEIGHT = 4800  # Max height to resize all papyrii\n","\n","    # Model config\n","    BATCH_SIZE = 64\n","    \n","    GPU_COUNT = 4\n","\n","    device = torch.device(\"cuda\")\n","    threshold = 0.6\n","    num_workers = 8\n","    exp = 1e-7\n","    mask_padding = SHARED_HEIGHT // 40\n","\n","    num_epochs = 30\n","    lr = 1e-3\n","    eta_min_lr = 1e-5\n","    WANDB_NOTE = \"pos_weightを1より大きくした\"\n","    # augmentation = \"augmentation: Horizontal/Vertical/RandomScale/Transpose/RandomRotate90/ShiftScaleRotate/GridDistortion/MultiChannelNoise/PadIfNeeded\"\n","    augmentation = \"augmentation: Horizontal/Vertical/ShiftScaleRotate/MultiChannelNoise/PadIfNeeded\"\n","    loss1_alpha = None\n","    loss1_beta = None\n","    \n","    loss2a_alpha = None\n","    loss2a_beta = None\n","    \n","    loss1_weight = 0.5\n","    loss2_weight = 0.5\n","    \n","    loss2a_weight = 0.5\n","    loss2b_weight = 0.5\n","    \n","    lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts\n","    optimizer = \"AdamW\"\n","    # loss1 = smp.losses.TverskyLoss(\n","    #     smp.losses.BINARY_MODE,\n","    #     log_loss=False,\n","    #     from_logits=True, \n","    #     smooth=1e-7,\n","    #     alpha=loss1_alpha,\n","    #     beta=loss1_beta,\n","    # )\n","    loss1 = nn.BCEWithLogitsLoss(\n","        pos_weight=torch.tensor([1.5]),\n","    )\n","    # loss2a = smp.losses.TverskyLoss(\n","    #     smp.losses.BINARY_MODE,\n","    #     log_loss=False,\n","    #     from_logits=True, \n","    #     smooth=1e-7,\n","    #     alpha=loss2a_alpha,\n","    #     beta=loss2a_beta,\n","    # )\n","    loss2a = nn.BCEWithLogitsLoss(\n","        pos_weight=torch.tensor([1.5]),\n","    )\n","    loss2b = nn.BCEWithLogitsLoss(\n","        pos_weight=torch.tensor([1.5]),\n","    )\n","    noise_intensity = (0.00005, 0.00025)\n","    use_new_label_mask = True\n","    pretrained = True\n","    # MODEL_DEPTH = 34\n","    # MODEL_KIND = \"KM\"\n","    \n","    exp_name = \"038-gpu17\"\n","    prev_exp_name = \"\"\n","    \n","def class2dict(c):\n","    return {attr: getattr(c, attr) for attr in dir(c) if not callable(getattr(c, attr)) and not attr.startswith(\"__\")}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import cupy as cp\n","xp = cp\n","\n","delta_lookup = {\n","    \"xx\": xp.array([[1, -2, 1]], dtype=float),\n","    \"yy\": xp.array([[1], [-2], [1]], dtype=float),\n","    \"xy\": xp.array([[1, -1], [-1, 1]], dtype=float),\n","}\n","\n","def operate_derivative(img_shape, pair):\n","    assert len(img_shape) == 2\n","    delta = delta_lookup[pair]\n","    fft = xp.fft.fftn(delta, img_shape)\n","    return fft * xp.conj(fft)\n","\n","def soft_threshold(vector, threshold):\n","    return xp.sign(vector) * xp.maximum(xp.abs(vector) - threshold, 0)\n","\n","def back_diff(input_image, dim):\n","    assert dim in (0, 1)\n","    r, n = xp.shape(input_image)\n","    size = xp.array((r, n))\n","    position = xp.zeros(2, dtype=int)\n","    temp1 = xp.zeros((r+1, n+1), dtype=float)\n","    temp2 = xp.zeros((r+1, n+1), dtype=float)\n","    \n","    temp1[position[0]:size[0], position[1]:size[1]] = input_image\n","    temp2[position[0]:size[0], position[1]:size[1]] = input_image\n","    \n","    size[dim] += 1\n","    position[dim] += 1\n","    temp2[position[0]:size[0], position[1]:size[1]] = input_image\n","    temp1 -= temp2\n","    size[dim] -= 1\n","    return temp1[0:size[0], 0:size[1]]\n","\n","\n","def forward_diff(input_image, dim):\n","    assert dim in (0, 1)\n","    r, n = xp.shape(input_image)\n","    size = xp.array((r, n))\n","    position = xp.zeros(2, dtype=int)\n","    temp1 = xp.zeros((r+1, n+1), dtype=float)\n","    temp2 = xp.zeros((r+1, n+1), dtype=float)\n","        \n","    size[dim] += 1\n","    position[dim] += 1\n","\n","    temp1[position[0]:size[0], position[1]:size[1]] = input_image\n","    temp2[position[0]:size[0], position[1]:size[1]] = input_image\n","    \n","    size[dim] -= 1\n","    temp2[0:size[0], 0:size[1]] = input_image\n","    temp1 -= temp2\n","    size[dim] += 1\n","    return -temp1[position[0]:size[0], position[1]:size[1]]\n","\n","def iter_deriv(input_image, b, scale, mu, dim1, dim2):\n","    g = back_diff(forward_diff(input_image, dim1), dim2)\n","    d = soft_threshold(g + b, 1 / mu)\n","    b = b + (g - d)\n","    L = scale * back_diff(forward_diff(d - b, dim2), dim1)\n","    return L, b\n","\n","def iter_xx(*args):\n","    return iter_deriv(*args, dim1=1, dim2=1)\n","\n","def iter_yy(*args):\n","    return iter_deriv(*args, dim1=0, dim2=0)\n","\n","def iter_xy(*args):\n","    return iter_deriv(*args, dim1=0, dim2=1)\n","\n","def iter_sparse(input_image, bsparse, scale, mu):\n","    d = soft_threshold(input_image + bsparse, 1 / mu)\n","    bsparse = bsparse + (input_image - d)\n","    Lsparse = scale * (d - bsparse)\n","    return Lsparse, bsparse\n","\n","def denoise_image(input_image, iter_num=100, fidelity=150, sparsity_scale=10, continuity_scale=0.5, mu=1):\n","    image_size = xp.shape(input_image)\n","    #print(\"Initialize denoising\")\n","    norm_array = (\n","        operate_derivative(image_size, \"xx\") + \n","        operate_derivative(image_size, \"yy\") + \n","        2 * operate_derivative(image_size, \"xy\")\n","    )\n","    norm_array += (fidelity / mu) + sparsity_scale ** 2\n","    b_arrays = {\n","        \"xx\": xp.zeros(image_size, dtype=float),\n","        \"yy\": xp.zeros(image_size, dtype=float),\n","        \"xy\": xp.zeros(image_size, dtype=float),\n","        \"L1\": xp.zeros(image_size, dtype=float),\n","    }\n","    g_update = xp.multiply(fidelity / mu, input_image)\n","    for i in tqdm(range(iter_num), total=iter_num):\n","        #print(f\"Starting iteration {i+1}\")\n","        g_update = xp.fft.fftn(g_update)\n","        if i == 0:\n","            g = xp.fft.ifftn(g_update / (fidelity / mu)).real\n","        else:\n","            g = xp.fft.ifftn(xp.divide(g_update, norm_array)).real\n","        g_update = xp.multiply((fidelity / mu), input_image)\n","        \n","        #print(\"XX update\")\n","        L, b_arrays[\"xx\"] = iter_xx(g, b_arrays[\"xx\"], continuity_scale, mu)\n","        g_update += L\n","        \n","        #print(\"YY update\")\n","        L, b_arrays[\"yy\"] = iter_yy(g, b_arrays[\"yy\"], continuity_scale, mu)\n","        g_update += L\n","        \n","        #print(\"XY update\")\n","        L, b_arrays[\"xy\"] = iter_xy(g, b_arrays[\"xy\"], 2 * continuity_scale, mu)\n","        g_update += L\n","        \n","        #print(\"L1 update\")\n","        L, b_arrays[\"L1\"] = iter_sparse(g, b_arrays[\"L1\"], sparsity_scale, mu)\n","        g_update += L\n","        \n","    g_update = xp.fft.fftn(g_update)\n","    g = xp.fft.ifftn(xp.divide(g_update, norm_array)).real\n","    \n","    g[g < 0] = 0\n","    g -= g.min()\n","    g /= g.max()\n","    return g"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def is_in_masked_zone(location, mask):\n","    return mask[location[0], location[1]] > 0\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def resize(img):\n","    current_height, current_width = img.shape    \n","    aspect_ratio = current_width / current_height\n","    if CFG.SHARED_HEIGHT is None:\n","        return img\n","    # new_height = CFG.SHARED_HEIGHT\n","    # pad_y = new_height - current_height\n","    # if pad_y > 0:\n","    #     # 元画像が小さい場合は解像度を大きくしないでpaddingをつける\n","    #     img = np.pad(img, [(0, pad_y), (0, 0)], constant_values=0)\n","    # else:\n","    # 既に十分でかい場合はリサイズする\n","    # 本当はpaddingしたいけど、メモリサイズが大きくなる\n","    if CFG.SHARED_HEIGHT > current_height:\n","        # 既に小さい場合はリサイズしない\n","        return img    \n","    new_height = CFG.SHARED_HEIGHT\n","    new_width = int(CFG.SHARED_HEIGHT * aspect_ratio)\n","    new_size = (new_width, new_height)\n","    # (W, H)の順で渡すが結果は(H, W)になっている\n","    img = cv2.resize(img, new_size)\n","    return img\n","\n","def load_mask(split, index):     \n","    img = cv2.imread(f\"{CFG.DATA_DIR}/{split}/{index}/mask.png\", 0) // 255\n","    img = resize(img)\n","    return img\n","\n","\n","def load_labels(split, index):    \n","    suffix = \"_new\" if CFG.use_new_label_mask else \"\"\n","    img = cv2.imread(f\"{CFG.DATA_DIR}/{split}/{index}/inklabels{suffix}.png\", 0) // 255    \n","    img = resize(img)\n","    return img"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def extract_subvolume(location, volume):\n","    global printed\n","    y = location[0]\n","    x = location[1]\n","    subvolume = volume[y-CFG.BUFFER:y+CFG.BUFFER, x-CFG.BUFFER:x+CFG.BUFFER, :].astype(np.float32)\n","    \n","    return subvolume"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def load_volume(split, index):    \n","    # Load the 3d x-ray scan, one slice at a time\n","    all = sorted(glob.glob(f\"{CFG.DATA_DIR}/{split}/{index}/surface_volume/*.tif\"))\n","    z_slices_fnames = [all[i] for i in range(len(all)) if i in CFG.Z_LIST]\n","    assert len(z_slices_fnames) == CFG.Z_DIM\n","    z_slices = []\n","    for z, filename in  tqdm(enumerate(z_slices_fnames)):\n","        img = cv2.imread(filename, -1)        \n","        img = resize(img)\n","        # img = (img / (2 ** 8)).astype(np.uint8)\n","        img = img.astype(np.float32) // 255\n","        z_slices.append(img)\n","    return np.stack(z_slices, axis=-1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def generate_locations_ds(volume, mask, label=None, skip_zero=False):\n","    is_in_mask_train = lambda x: is_in_masked_zone(x, mask)\n","\n","    # Create a list to store train locations\n","    locations = []\n","\n","    # Generate train locations\n","    volume_height, volume_width = volume.shape[:-1]\n","\n","    for y in range(CFG.BUFFER, volume_height - CFG.BUFFER, CFG.STRIDE):\n","        for x in range(CFG.BUFFER, volume_width - CFG.BUFFER, CFG.STRIDE):\n","            if skip_zero and label is not None and np.all(label[y - CFG.STRIDE // 2 : y + CFG.STRIDE // 2, x - CFG.STRIDE // 2 : x + CFG.STRIDE // 2] == 0):\n","                # print(f\"skip location at (y: {y}, x: {x})\")\n","                continue\n","            if is_in_mask_train((y, x)):\n","                locations.append((y, x))\n","\n","    # Convert the list of train locations to a PyTorch tensor\n","    locations_ds = np.stack(locations, axis=0)\n","    return locations_ds"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","from albumentations.core.transforms_interface import ImageOnlyTransform\n","\n","class MultichannelNoise(ImageOnlyTransform):\n","\n","    def __init__(self, intensity=CFG.noise_intensity, always_apply=False, p=0.5):\n","        super().__init__(always_apply, p)\n","        self.intensity = intensity\n","\n","    def apply(self, img, **params):\n","        intensity = np.random.uniform(*self.intensity)\n","        noise = np.random.normal(loc=0, scale=intensity*255, size=img.shape)\n","        img = img + noise\n","        return np.clip(img, 0, 255).astype(np.float32) # クリップして0から255の範囲に保つ\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.preprocessing import OneHotEncoder\n","\n","from albumentations.core.transforms_interface import ImageOnlyTransform\n","\n","class SubvolumeDataset(Dataset):\n","    def __init__(self, locations, volume, labels, buffer, is_train: bool, return_location: bool = False):\n","        self.locations = locations\n","        self.volume = volume\n","        self.labels = labels        \n","        self.buffer = buffer\n","        self.is_train = is_train\n","        self.return_location = return_location\n","\n","    def __len__(self):\n","        return len(self.locations)\n","\n","    def __getitem__(self, idx):\n","        label = None\n","        location = np.array(self.locations[idx])\n","        y, x = location[0], location[1]\n","\n","        subvolume = extract_subvolume(location, self.volume)\n","        \n","        if self.labels is not None:\n","            label = self.labels[y - self.buffer:y + self.buffer, x - self.buffer:x + self.buffer]            \n","            label = np.stack([label], axis=-1)            \n","            \n","        # 段々meanは小さくなる\n","        mean = np.array([0.45 - i / 200 for i in range(0, CFG.Z_DIM)]).reshape(-1, 1, 1)\n","        # 段々stdは小さくなる\n","        std = np.array([0.25 for i in range(0, CFG.Z_DIM)]).reshape(-1, 1, 1)\n","        \n","        if self.is_train and label is not None:    \n","            # label = smooth_labels(label, alpha=0.1)      \n","            transformed = A.Compose([\n","                A.HorizontalFlip(p=0.5),\n","                A.VerticalFlip(p=0.5),                \n","                A.RandomScale(p=0.4, scale_limit=0.3),\n","                A.Transpose(p=0.4),         \n","                A.RandomRotate90(p=0.4,),            \n","                A.ShiftScaleRotate(p=0.8, scale_limit=0.3, rotate_limit=80),\n","                A.GridDistortion(num_steps=5, distort_limit=0.15, p=0.4),\n","                # A.CoarseDropout(\n","                #     max_holes=1, \n","                #     max_width=int(CFG.BUFFER * 2 * 0.3),\n","                #     max_height=int(CFG.BUFFER * 2 * 0.3), \n","                #     mask_fill_value=0, \n","                #     p=0.3,\n","                # ),\n","                MultichannelNoise(\n","                    p=0.15,\n","                ),\n","                A.PadIfNeeded(min_height=self.buffer * 2, min_width=self.buffer * 2),\n","                A.Resize(height=self.buffer * 2, width=self.buffer * 2),\n","            ])(image=subvolume, mask=label)\n","            subvolume = transformed[\"image\"]\n","            label = transformed[\"mask\"]\n","            subvolume = np.transpose(subvolume, (2, 0, 1))\n","            label = np.transpose(label, (2, 0, 1))\n","            subvolume /= 255.\n","            subvolume = (subvolume - mean) / std       \n","        else:\n","            if label is None:\n","                subvolume = np.transpose(subvolume, (2, 0, 1))\n","                subvolume /= 255.\n","                subvolume = (subvolume - mean) / std\n","            else:\n","                # print(\"subvolume in val dataset (before aug)\", subvolume, file=open(\"before-val-aug.log\", \"w\")) \n","                subvolume = np.transpose(subvolume, (2, 0, 1))\n","                label = np.transpose(label, (2, 0, 1))\n","                subvolume /= 255.\n","                subvolume = (subvolume - mean) / std\n","        # print(\"subvolume\", subvolume)\n","        if self.return_location:\n","            return subvolume, location\n","        return subvolume, label        "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class SmpUnetDecoder(nn.Module):\n","\tdef __init__(\n","\t\tself,\n","\t\tin_channel,\n","\t\tskip_channel,\n","\t\tout_channel,\n","\t):\n","\t\tsuper().__init__()\n","\t\tself.center = nn.Identity()\n","\n","\t\ti_channel = [\n","\t\t\tin_channel,\n","\t\t] + out_channel[:-1]\n","\t\ts_channel = skip_channel\n","\t\to_channel = out_channel\n","\t\t# print(\"i:\", i_channel)\n","\t\t# print(\"s:\", s_channel)\n","\t\t# print(\"o:\", o_channel)\n","\t\tblock = [\n","\t\t\tDecoderBlock(i, s, o, use_batchnorm=True, attention_type=None)\n","\t\t\tfor i, s, o in zip(i_channel, s_channel, o_channel)\n","\t\t]\n","\t\tself.block = nn.ModuleList(block)\n","\n","\tdef forward(self, feature, skip):\n","\t\t# for s in skip:\n","\t\t\t# print(\"feature\", feature.shape, \"skip\", s.shape)\n","\t\td = self.center(feature)\n","\t\tdecode = []\n","\t\tfor i, block in enumerate(self.block):\n","\t\t\ts = skip[i]\n","\t\t\td = block(d, s)\n","\t\t\tdecode.append(d)\n","\n","\t\tlast = d\n","\t\treturn last, decode\n","\n","\n","class Net(nn.Module):\n","\tdef __init__(\n","\t\tself,\n","\t):\n","\t\tsuper().__init__()\n","\t\tself.output_type = [\"inference\", \"loss\"]\n","\n","\t\tconv_dim = 64\n","\t\tencoder1_dim = [\n","\t\t\tconv_dim,\n","\t\t\t256,\n","\t\t\t512,\n","\t\t\t1024,\n","\t\t\t2048,\n","\t\t]\n","\t\tdecoder1_dim = [\n","\t\t\t1024,\n","\t\t\t512,\n","\t\t\t256,\n","\t\t\t64,\n","\t\t]\n","\n","\t\tself.encoder1 = seresnext26d_32x4d(pretrained=CFG.pretrained, in_chans=CFG.Z_DIM - CFG.BATCH_Z_DIFF)\n","\n","\t\tself.decoder1 = SmpUnetDecoder(\n","\t\t\tin_channel=encoder1_dim[-1],\n","\t\t\tskip_channel=encoder1_dim[:-1][::-1],\n","\t\t\tout_channel=decoder1_dim,\n","\t\t)\n","\t\t# -- pool attention weight  \n","\t\tself.weight1 = nn.ModuleList(\n","\t\t\t[\n","\t\t\t\tnn.Sequential(\n","\t\t\t\t\tnn.Conv2d(dim, dim, kernel_size=3, padding=1),\n","\t\t\t\t\tnn.BatchNorm2d(dim),\n","\t\t\t\t\tnn.ReLU(inplace=True),\n","\t\t\t\t)\n","\t\t\t\tfor dim in encoder1_dim\n","\t\t\t]\n","\t\t)\n","\t\tself.logit1 = nn.Conv2d(decoder1_dim[-1], 1, kernel_size=1)\n","\n","\t\t# --------------------------------\n","\t\t#\n","\t\tencoder2_dim = [64, 128, 256, 512]  #\n","\t\tdecoder2_dim = [\n","\t\t\t256,\n","\t\t\t128,\n","\t\t\t64,\n","\t\t]\n","\t\tself.encoder2 = resnet10t(pretrained=CFG.pretrained, in_chans=decoder1_dim[-1])\n","\n","\t\tself.decoder2 = SmpUnetDecoder(\n","\t\t\tin_channel=encoder2_dim[-1],\n","\t\t\tskip_channel=encoder2_dim[:-1][::-1],\n","\t\t\tout_channel=decoder2_dim,\n","\t\t)\n","\t\tself.logit2 = nn.Conv2d(decoder2_dim[-1], 1, kernel_size=1)\n","\n","\tdef forward(self, batch):\n","\t\tv = batch\n","\t\tB, C, H, W = v.shape\n","\t\tvv = [\n","\t\t\tv[:, i : i + CFG.Z_DIM - CFG.BATCH_Z_DIFF]\n","\t\t\tfor i in range(0, CFG.BATCH_Z_DIFF + 1, 2)\n","\t\t]\n","\t\tK = len(vv)\n","\t\tx = torch.cat(vv, 0)\n","\t\t# x = v\n","\n","\t\t# ----------------------\n","\t\tencoder = []\n","\t\te = self.encoder1\n","\t\tx = e.conv1(x)\n","\t\tx = e.bn1(x)\n","\t\tx = e.act1(x)\n","\t\tencoder.append(x)\n","\t\tx = F.avg_pool2d(x, kernel_size=2, stride=2)\n","\t\tx = e.layer1(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer2(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer3(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer4(x)\n","\t\tencoder.append(x)\n","\t\t# print('encoder', [f.shape for f in encoder])\n","\n","\t\tfor i in range(len(encoder)):\n","\t\t\te = encoder[i]\n","\t\t\t# e = F.avg_pool2d(e, 4, 4)\n","\t\t\tf = self.weight1[i](e)\n","\t\t\t_, c, h, w = e.shape\n","\t\t\tf = rearrange(f, \"(K B) c h w -> B K c h w\", K=K, B=B, h=h, w=w)  #\n","\t\t\te = rearrange(e, \"(K B) c h w -> B K c h w\", K=K, B=B, h=h, w=w)  #\n","\t\t\tw = F.softmax(f, 1)\n","\t\t\te = (w * e).sum(1)\n","\t\t\tencoder[i] = e\n","\n","\t\tfeature = encoder[-1]\n","\t\tskip = encoder[:-1][::-1]\n","\t\t# print('encoder after weighted', [f.shape for f in encoder])\n","\t\t# print('skip after weighted', [f.shape for f in skip])\n","\t\tlast, decoder = self.decoder1(feature, skip)\n","\t\tlogit1 = self.logit1(last)\n","\t\t# print(\"logit1.shape\", logit1.shape)\n","\t\t# print(\"encoder1 shapes\", list(map(lambda a: a.shape, encoder)))\n","  \n","\t\tlogit1 = F.interpolate(\n","\t\t\tlogit1, size=(H, W), mode=\"bilinear\", align_corners=False, antialias=True\n","\t\t)\n","\n","\t\t# ----------------------\n","\t\tx = last  # .detach()\n","\t\t# x = F.avg_pool2d(x,kernel_size=2,stride=2)\n","\t\tencoder = []\n","\t\te = self.encoder2\n","\t\tx = e.layer1(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer2(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer3(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer4(x)\n","\t\tencoder.append(x)\n","\n","\t\tfeature = encoder[-1]\n","\t\tskip = encoder[:-1][::-1]\n","\t\tlast, decoder = self.decoder2(feature, skip)\n","\t\tlogit2 = self.logit2(last)\n","\t\t# print(\"logit2.shape\", logit2.shape)\n","\t\tlogit2 = F.interpolate(\n","\t\t\tlogit2, size=(H, W), mode=\"bilinear\", align_corners=False, antialias=True\n","\t\t)\n","\t\treturn logit1, logit2\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ref - https://www.kaggle.com/competitions/vesuvius-challenge-ink-detection/discussion/397288\n","def fbeta_score(preds, targets, threshold, beta=0.5, smooth=1e-5):\n","    preds_t = torch.where(preds > threshold, 1.0, 0.0).float()\n","    y_true_count = targets.sum()\n","    \n","    ctp = preds_t[targets==1].sum()\n","    cfp = preds_t[targets==0].sum()\n","    beta_squared = beta * beta\n","\n","    c_precision = ctp / (ctp + cfp + smooth)\n","    c_recall = ctp / (y_true_count + smooth)\n","    dice = (1 + beta_squared) * (c_precision * c_recall) / (beta_squared * c_precision + c_recall + smooth)\n","\n","    return dice"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tc = torch\n","def TTA(x:tc.Tensor,model:nn.Module):\n","    #x.shape=(batch,c,h,w)\n","    shape=x.shape\n","    x=[x,*[tc.rot90(x,k=i,dims=(-2,-1)) for i in range(1,4)]]\n","    x=tc.cat(x,dim=0)\n","    _, x = model(x)\n","    x=x.reshape(4,shape[0], 1 ,*shape[2:])\n","    x=[tc.rot90(x[i],k=-i,dims=(-2,-1)) for i in range(4)]\n","    x=tc.stack(x,dim=0)\n","    return x.mean(0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","class Model(pl.LightningModule):\n","    training_step_outputs = []\n","    validation_step_outputs = []\n","    test_step_outputs = [[], []]\n","\n","    def __init__(self, **kwargs):\n","        super().__init__()\n","\n","        self.model = Net()        \n","\n","        self.loss1 = CFG.loss1\n","        self.loss2 = CFG.loss2a\n","        self.loss3 = CFG.loss2b\n","\n","    def forward(self, image, stage):\n","        if stage != \"train\":\n","            mask = TTA(image, self.model)\n","            # _, mask = self.model(image)\n","        else:\n","            mask = self.model(image)\n","        return mask\n","\n","    def shared_step(self, batch, stage):\n","        subvolumes, labels = batch\n","\n","        image, labels = subvolumes.float(), labels.float()        \n","        assert image.ndim == 4\n","        \n","        h, w = image.shape[2:]\n","        assert h % 32 == 0 and w % 32 == 0\n","        \n","        # print(\"labels\", labels.max(), labels.min())\n","\n","        assert labels.max() <= 1.0 and labels.min() >= 0\n","\n","        if stage == \"train\":\n","            logit1, logit2 = self.forward(image, stage)\n","            loss = self.loss1(logit1, labels) * CFG.loss1_weight + CFG.loss2_weight * (self.loss2(logit2, labels) * CFG.loss2a_weight + self.loss3(logit2, labels) * CFG.loss2b_weight)\n","            logit = logit2\n","        else:\n","            logit = self.forward(image, stage)\n","            loss = self.loss2(logit, labels) * CFG.loss2a_weight + self.loss3(logit, labels) * CFG.loss2b_weight\n","        \n","        prob2 = torch.sigmoid(logit)\n","\n","        pred_mask = (prob2 > CFG.threshold).float()\n","        \n","        # print(\"pred_mask\", pred_mask)\n","        \n","        score = fbeta_score(pred_mask, labels, threshold=CFG.threshold)\n","\n","        tp, fp, fn, tn = smp.metrics.get_stats(\n","            pred_mask.long(), labels.long(), mode=\"binary\"\n","        )\n","\n","        return {\n","            \"loss\": loss,\n","            \"tp\": tp,\n","            \"fp\": fp,\n","            \"fn\": fn,\n","            \"tn\": tn,\n","            \"score\": score,\n","        }\n","\n","    def shared_epoch_end(self, outputs, stage):\n","        # aggregate step metics\n","        tp = torch.cat([x[\"tp\"] for x in outputs])\n","        fp = torch.cat([x[\"fp\"] for x in outputs])\n","        fn = torch.cat([x[\"fn\"] for x in outputs])\n","        tn = torch.cat([x[\"tn\"] for x in outputs])\n","        loss = torch.mean(torch.Tensor([x[\"loss\"] for x in outputs]))\n","        fbeta_score = torch.mean(torch.Tensor([x[\"score\"] for x in outputs]))\n","\n","        per_image_iou = smp.metrics.iou_score(\n","            tp, fp, fn, tn, reduction=\"micro-imagewise\"\n","        )\n","\n","        dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n","\n","        metrics = {\n","            f\"{stage}_per_image_iou\": per_image_iou,\n","            f\"{stage}_dataset_iou\": dataset_iou,\n","            f\"{stage}_loss\": 10000 if loss.item() == 0 else loss.item(),\n","            f\"{stage}_tp\": tp.sum().int().item(),\n","            f\"{stage}_fp\": fp.sum().int().item(),\n","            f\"{stage}_fn\": fn.sum().int().item(),\n","            f\"{stage}_tn\": tn.sum().int().item(),\n","            f\"{stage}_score\": fbeta_score.item(),\n","        }\n","\n","        self.log_dict(metrics, prog_bar=True, sync_dist=True)\n","\n","    def training_step(self, batch, batch_idx):\n","        out = self.shared_step(batch, \"train\")\n","        self.training_step_outputs.append(out)\n","        return out\n","\n","    def on_train_epoch_end(self):\n","        out = self.shared_epoch_end(self.training_step_outputs, \"train\")\n","        self.training_step_outputs.clear()\n","        return out\n","\n","    def validation_step(self, batch, batch_idx):\n","        out = self.shared_step(batch, \"valid\")\n","        self.validation_step_outputs.append(out)\n","        return out\n","\n","    def on_validation_epoch_end(self):\n","        out = self.shared_epoch_end(self.validation_step_outputs, \"valid\")\n","        self.validation_step_outputs.clear()\n","        return out\n","\n","    def test_step(self, batch, batch_idx):\n","        global predictions_map, predictions_map_counts\n","\n","        patch_batch, loc_batch = batch\n","\n","        loc_batch = loc_batch.long()\n","        patch_batch = patch_batch.float()\n","        predictions = self.forward(patch_batch, \"test\")\n","        predictions = predictions.sigmoid()\n"," \n","        predictions = torch.permute(predictions, (0, 2, 3, 1)).squeeze(dim=-1)\n","        predictions = (\n","            predictions.cpu().numpy()\n","        )\n","        loc_batch = loc_batch.cpu().numpy()\n","        \n","        self.test_step_outputs[0].extend(loc_batch)\n","        self.test_step_outputs[1].extend(predictions)        \n","        return loc_batch, predictions\n","\n","    def on_test_epoch_end(self):\n","        global predictions_map, predictions_map_counts\n","\n","        locs = np.array(self.test_step_outputs[0])\n","        preds = np.array(self.test_step_outputs[1])\n","        print(\"locs\", locs.shape)\n","        print(\"preds\", preds.shape)\n","\n","        new_predictions_map = np.zeros_like(predictions_map[:, :, 0])\n","        new_predictions_map_counts = np.zeros_like(predictions_map_counts[:, :, 0])\n","\n","        for (y, x), pred in zip(locs, preds):\n","            new_predictions_map[\n","                y - CFG.BUFFER : y + CFG.BUFFER, x - CFG.BUFFER : x + CFG.BUFFER\n","            ] += pred\n","            new_predictions_map_counts[\n","                y - CFG.BUFFER : y + CFG.BUFFER, x - CFG.BUFFER : x + CFG.BUFFER\n","            ] += 1\n","        \n","        new_predictions_map /= new_predictions_map_counts + CFG.exp\n","        new_predictions_map = new_predictions_map[:, :, np.newaxis]\n","        predictions_map = np.concatenate(\n","            [predictions_map, new_predictions_map], axis=-1\n","        )\n","\n","\n","    def configure_optimizers(self):\n","        optimizer = optim.AdamW(self.parameters(), lr=CFG.lr)\n","\n","        scheduler = CFG.lr_scheduler(\n","            optimizer, T_0=CFG.num_epochs, T_mult=2, eta_min=CFG.eta_min_lr,\n","        )\n","        return {\n","            \"optimizer\": optimizer,\n","            \"lr_scheduler\": scheduler,\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class EnsembleModel:\n","    def __init__(self, test_loader, test_volume):\n","        super().__init__()\n","        self.test_loader = test_loader\n","        self.test_volume = test_volume\n","        self.list = []\n","        for fold in [1, 2, 3, 4]:\n","            _model = Model.load_from_checkpoint(\n","                f\"weights/weights_fold-{fold}.ckpt\",               \n","            )\n","            trainer = pl.Trainer(\n","                accelerator=\"gpu\",\n","                devices=\"1\",\n","                enable_checkpointing=False,\n","            )\n","\n","            self.list.append((_model, trainer))\n","    \n","    def forward(self):\n","        global predictions_map, predictions_map_counts\n","        predictions_map = np.empty_like(self.test_volume[:, :, 0])[:, :, np.newaxis].astype(np.float64)\n","        predictions_map_counts = np.empty_like(predictions_map).astype(np.uint8)\n","        for i, (model, trainer) in enumerate(self.list):\n","            model: Model = model\n","            model.test_step_outputs = [[], []]\n","            model.eval()\n","            trainer.test(\n","                model=model,\n","                dataloaders=self.test_loader,\n","                verbose=True,\n","            )\n","            self.list[i] = None\n","            gc.collect()\n","        predictions_map = predictions_map[:, :, 1:].mean(axis=-1, keepdims=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","from tqdm import tqdm\n","from skimage.transform import resize as resize_ski\n","import pathlib\n","import gc\n","\n","def compute_predictions_map(split, index):\n","    global predictions_map\n","    global predictions_map_counts\n","    \n","    print(f\"Load data for {split}/{index}\")\n","\n","    test_volume = load_volume(split=split, index=index)\n","    test_mask = load_mask(split=split, index=index)    \n","    \n","    #get coord\n","    stride = CFG.BUFFER\n","    H,W,D  = test_volume.shape\n","\n","    ##pad #assume H,W >size\n","    px, py = W % stride, H % stride\n","    if (px != 0) or (py != 0):\n","        px = stride - px\n","        py = stride - py\n","        test_volume = np.pad(test_volume, [(0, py), (0, px), (0, 0)], constant_values=0)\n","        test_mask = np.pad(test_mask, [(0, py), (0, px)], constant_values=0)  \n","        \n","    test_locations = generate_locations_ds(test_volume, test_mask)  \n","    \n","    print(f\"{len(test_locations)} test locations (before filtering by mask)\")\n","\n","    # filter locations inside the mask\n","    test_locations = [loc for loc in test_locations if is_in_masked_zone(loc, test_mask)]\n","    \n","    print(f\"{len(test_locations)} test locations (after filtering by mask)\")\n","\n","    test_ds = SubvolumeDataset(test_locations, test_volume, None, CFG.BUFFER, is_train=False, return_location=True)\n","    test_loader = DataLoader(test_ds, batch_size=CFG.BATCH_SIZE, num_workers=CFG.num_workers, pin_memory=True)        \n","\n","    # # shape: (Y, X, C)\n","    predictions_map = np.zeros_like(test_volume[:, :, 0])[:, :, np.newaxis].astype(np.float64)\n","    predictions_map_counts = np.zeros_like(predictions_map).astype(np.uint8)\n","\n","    print(\"test_volume.shape\", test_volume.shape)\n","    print(\"predictions_map.shape\", predictions_map.shape)\n","\n","    print(f\"Compute predictions\")\n","    \n","    model = EnsembleModel(test_loader, test_volume)\n","    model.forward()\n","    del model\n","    del test_locations\n","    del test_loader\n","    del test_ds\n","    del test_volume\n","    del test_mask\n","    gc.collect()\n","    \n","    # print(\"predictions_map\", predictions_map, file=open(\"predictions_map\", \"w\"))\n","    return predictions_map[:H, :W, :]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fast run length encoding, from https://www.kaggle.com/code/hackerpoet/even-faster-run-length-encoder/script\n","# (H, W)の形式\n","def rle (img: np.ndarray, threshold: float):\n","    flat_img = img.flatten()\n","    flat_img = np.where(flat_img > threshold, 1, 0).astype(np.uint8)\n","\n","    starts = np.array((flat_img[:-1] == 0) & (flat_img[1:] == 1))\n","    ends = np.array((flat_img[:-1] == 1) & (flat_img[1:] == 0))\n","    starts_ix = np.where(starts)[0] + 2\n","    ends_ix = np.where(ends)[0] + 2\n","    lengths = ends_ix - starts_ix\n","\n","    return \" \".join(map(str, sum(zip(starts_ix, lengths), ())))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def update_submission(predictions_map, index):\n","    rle_ = rle(predictions_map, threshold=CFG.threshold)\n","    print(f\"{index},\" + rle_, file=open('submission.csv', 'a'))\n","    # print(f\"{index},\" + rle_, file=open('/kaggle/working/submission.csv', 'a'))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Load data for test/a\n"]},{"name":"stderr","output_type":"stream","text":["12it [00:03,  3.70it/s]\n"]},{"name":"stdout","output_type":"stream","text":["2625 test locations (before filtering by mask)\n","2625 test locations (after filtering by mask)\n","test_volume.shape (2800, 6384, 12)\n","predictions_map.shape (2800, 6384, 1)\n","Compute predictions\n"]},{"name":"stderr","output_type":"stream","text":["GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"12eb91ee494d40408345142cbbc96392","version_major":2,"version_minor":0},"text/plain":["Testing: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 3.06 GiB (GPU 0; 47.54 GiB total capacity; 30.73 GiB already allocated; 978.81 MiB free; 33.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[1;32m/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(folder\u001b[39m.\u001b[39miterdir()):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     index \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39mstem\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     predictions_map \u001b[39m=\u001b[39m compute_predictions_map(split\u001b[39m=\u001b[39;49mkind, index\u001b[39m=\u001b[39;49mindex)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     predictions_map\u001b[39m=\u001b[39mxp\u001b[39m.\u001b[39marray(predictions_map[:, :, \u001b[39m0\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# train/2の場合、先にリサイズするとout of memoryになる\u001b[39;00m\n","\u001b[1;32m/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb Cell 21\u001b[0m in \u001b[0;36mcompute_predictions_map\u001b[0;34m(split, index)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCompute predictions\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m model \u001b[39m=\u001b[39m EnsembleModel(test_loader, test_volume)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m model\u001b[39m.\u001b[39;49mforward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mdel\u001b[39;00m model\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mdel\u001b[39;00m test_locations\n","\u001b[1;32m/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb Cell 21\u001b[0m in \u001b[0;36mEnsembleModel.forward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m model\u001b[39m.\u001b[39mtest_step_outputs \u001b[39m=\u001b[39m [[], []]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtest(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     dataloaders\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtest_loader,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlist[i] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m gc\u001b[39m.\u001b[39mcollect()\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:794\u001b[0m, in \u001b[0;36mTrainer.test\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    792\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_unwrap_optimized(model)\n\u001b[1;32m    793\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 794\u001b[0m \u001b[39mreturn\u001b[39;00m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    795\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_test_impl, model, dataloaders, ckpt_path, verbose, datamodule\n\u001b[1;32m    796\u001b[0m )\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[39m.\u001b[39m_call_teardown_hook()\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:842\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tested_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mckpt_path  \u001b[39m# TODO: remove in v1.8\u001b[39;00m\n\u001b[1;32m    841\u001b[0m \u001b[39m# run test\u001b[39;00m\n\u001b[0;32m--> 842\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    844\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    845\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtesting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1112\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1110\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> 1112\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   1114\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1115\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1188\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mdispatch(\u001b[39mself\u001b[39m)\n\u001b[1;32m   1187\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluating:\n\u001b[0;32m-> 1188\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_evaluate()\n\u001b[1;32m   1189\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1190\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1228\u001b[0m, in \u001b[0;36mTrainer._run_evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1223\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m   1225\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrun_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstage\u001b[39m}\u001b[39;00m\u001b[39m_evaluation\u001b[39m\u001b[39m\"\u001b[39m), _evaluation_context(\n\u001b[1;32m   1226\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inference_mode\n\u001b[1;32m   1227\u001b[0m ):\n\u001b[0;32m-> 1228\u001b[0m     eval_loop_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1230\u001b[0m \u001b[39m# remove the tensors from the eval results\u001b[39;00m\n\u001b[1;32m   1231\u001b[0m \u001b[39mfor\u001b[39;00m result \u001b[39min\u001b[39;00m eval_loop_results:\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:152\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_dataloaders \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    151\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdataloader_idx\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataloader_idx\n\u001b[0;32m--> 152\u001b[0m dl_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher, dl_max_batches, kwargs)\n\u001b[1;32m    154\u001b[0m \u001b[39m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs\u001b[39m.\u001b[39mappend(dl_outputs)\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:137\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[1;32m    136\u001b[0m \u001b[39m# lightning module methods\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    138\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_evaluation_step_end(output)\n\u001b[1;32m    140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:234\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"The evaluation step (validation_step or test_step depending on the trainer's state).\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \n\u001b[1;32m    225\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[39m    the outputs of the step\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    233\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtest_step\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 234\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_strategy_hook(hook_name, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    236\u001b[0m \u001b[39mreturn\u001b[39;00m output\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1494\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1491\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1494\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1496\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:399\u001b[0m, in \u001b[0;36mStrategy.test_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mtest_step_context():\n\u001b[1;32m    398\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, TestStep)\n\u001b[0;32m--> 399\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtest_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","\u001b[1;32m/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb Cell 21\u001b[0m in \u001b[0;36mModel.test_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=117'>118</a>\u001b[0m loc_batch \u001b[39m=\u001b[39m loc_batch\u001b[39m.\u001b[39mlong()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=118'>119</a>\u001b[0m patch_batch \u001b[39m=\u001b[39m patch_batch\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=119'>120</a>\u001b[0m predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(patch_batch, \u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=120'>121</a>\u001b[0m predictions \u001b[39m=\u001b[39m predictions\u001b[39m.\u001b[39msigmoid()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=122'>123</a>\u001b[0m predictions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mpermute(predictions, (\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39msqueeze(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n","\u001b[1;32m/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb Cell 21\u001b[0m in \u001b[0;36mModel.forward\u001b[0;34m(self, image, stage)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, image, stage):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mif\u001b[39;00m stage \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m         mask \u001b[39m=\u001b[39m TTA(image, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m         \u001b[39m# _, mask = self.model(image)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m         mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(image)\n","\u001b[1;32m/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb Cell 21\u001b[0m in \u001b[0;36mTTA\u001b[0;34m(x, model)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m x\u001b[39m=\u001b[39m[x,\u001b[39m*\u001b[39m[tc\u001b[39m.\u001b[39mrot90(x,k\u001b[39m=\u001b[39mi,dims\u001b[39m=\u001b[39m(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m4\u001b[39m)]]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m x\u001b[39m=\u001b[39mtc\u001b[39m.\u001b[39mcat(x,dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m _, x \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m x\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39mreshape(\u001b[39m4\u001b[39m,shape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m ,\u001b[39m*\u001b[39mshape[\u001b[39m2\u001b[39m:])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m x\u001b[39m=\u001b[39m[tc\u001b[39m.\u001b[39mrot90(x[i],k\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mi,dims\u001b[39m=\u001b[39m(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m4\u001b[39m)]\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","\u001b[1;32m/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb Cell 21\u001b[0m in \u001b[0;36mNet.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=114'>115</a>\u001b[0m encoder\u001b[39m.\u001b[39mappend(x)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=115'>116</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mavg_pool2d(x, kernel_size\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, stride\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=116'>117</a>\u001b[0m x \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39;49mlayer1(x)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=117'>118</a>\u001b[0m encoder\u001b[39m.\u001b[39mappend(x)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.17/home/fummicc1/codes/competitions/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=118'>119</a>\u001b[0m x \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39mlayer2(x)\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/timm/models/resnet.py:463\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    460\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_path(x)\n\u001b[1;32m    462\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 463\u001b[0m     shortcut \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownsample(shortcut)\n\u001b[1;32m    464\u001b[0m x \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m shortcut\n\u001b[1;32m    465\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact3(x)\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    183\u001b[0m )\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2452\u001b[0m )\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.06 GiB (GPU 0; 47.54 GiB total capacity; 30.73 GiB already allocated; 978.81 MiB free; 33.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["predictions_map = None\n","predictions_map_counts = None\n","print(\"Id,Predicted\", file=open('submission.csv', 'w'))\n","kind = \"test\"\n","folder = pathlib.Path(CFG.DATA_DIR) / kind\n","for p in list(folder.iterdir()):\n","    index = p.stem\n","    predictions_map = compute_predictions_map(split=kind, index=index)\n","    predictions_map=xp.array(predictions_map[:, :, 0])\n","    # train/2の場合、先にリサイズするとout of memoryになる\n","    predictions_map=denoise_image(predictions_map, iter_num=50)\n","    predictions_map=predictions_map.get()\n","    original_size = cv2.imread(CFG.DATA_DIR + f\"/{kind}/{index}/mask.png\", 0).shape[:2]\n","    resized_predictions_map = resize_ski(predictions_map, (original_size[0], original_size[1]), anti_aliasing=True)\n","    print(\"original size\", original_size)    \n","    update_submission(resized_predictions_map, index)\n","    resized_predictions_map = np.where(resized_predictions_map >= CFG.threshold, 255, 0)\n","    plt.imsave(f\"{index}_{str(CFG.threshold)}_{str(CFG.BUFFER)}.png\", resized_predictions_map, cmap=\"gray\")\n","    del resized_predictions_map\n","    gc.collect()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat":4,"nbformat_minor":4}
