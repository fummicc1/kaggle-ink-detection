{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Keras starter kit [full training set, UNet]"]},{"cell_type":"code","execution_count":407,"metadata":{},"outputs":[],"source":["# import sys\n","\n","# sys.path.append('/kaggle/input/pretrainedmodels/pretrainedmodels-0.7.4')\n","# sys.path.append('/kaggle/input/segmentation-models-pytorch/segmentation_models.pytorch-master')\n","# sys.path.append('/kaggle/input/efficientnet-pytorch/EfficientNet-PyTorch-master')\n","# sys.path.append('/kaggle/input/timm-pytorch-image-models/pytorch-image-models-master')\n","# sys.path.append(\"/kaggle/input/einops/einops-master\")"]},{"cell_type":"code","execution_count":408,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["/bin/bash: /home/fummicc1/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n"]}],"source":["!export CUDA_VISIBLE_DEVICES=0"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Setup"]},{"cell_type":"code","execution_count":409,"metadata":{"execution":{"iopub.execute_input":"2023-05-10T15:03:39.741181Z","iopub.status.busy":"2023-05-10T15:03:39.740765Z","iopub.status.idle":"2023-05-10T15:03:39.750574Z","shell.execute_reply":"2023-05-10T15:03:39.749341Z","shell.execute_reply.started":"2023-05-10T15:03:39.741144Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Global seed set to 42\n"]}],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import wandb\n","import torchvision\n","import datetime\n","import imageio\n","\n","# import cupy\n","from sklearn.model_selection import KFold\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","import pytorch_lightning\n","import segmentation_models_pytorch as smp\n","import pytorch_lightning as pl\n","import pytorch_lightning.callbacks.model_checkpoint\n","import pytorch_lightning.plugins\n","from skimage.transform import resize as resize_ski\n","from pytorch_lightning.strategies.ddp import DDPStrategy\n","from pytorch_lightning.loggers import WandbLogger\n","from einops import rearrange, reduce, repeat\n","import torch.nn.functional as F\n","import segmentation_models_pytorch as smp\n","from segmentation_models_pytorch.decoders.unet.decoder import UnetDecoder, DecoderBlock\n","from timm.models.resnet import (\n","    resnet10t,\n","    resnet34d,\n","    resnet50d,\n","    resnet14t,\n","    seresnext26d_32x4d,\n","    seresnext50_32x4d,\n","    seresnext26tn_32x4d,\n",")\n","from timm.models.mvitv2 import mvitv2_base\n","import os\n","import torch.utils.data\n","from dataclasses import dataclass\n","\n","from scipy.ndimage import distance_transform_edt\n","\n","import ssl\n","\n","ssl._create_default_https_context = ssl._create_unverified_context\n","\n","import glob\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import cv2\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data\n","import os, cv2\n","import gc\n","import sys\n","import matplotlib.patches as patches\n","import random\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import pytorch_lightning as pl\n","\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.cuda import amp\n","from torch.utils.data import Dataset, DataLoader\n","import segmentation_models_pytorch as smp\n","import albumentations as A\n","from IPython.display import Video\n","\n","pytorch_lightning.seed_everything(seed=42)\n","torch.set_float32_matmul_precision(\"high\")\n","\n","\n","@dataclass\n","class CFG:\n","    # Data config\n","    # DATA_DIR = '/kaggle/input/vesuvius-challenge-ink-detection/'\n","    DATA_DIR = \"/home/fummicc1/codes/competitions/kaggle-ink-detection\"\n","    # DATA_DIR = '/home/fummicc1/codes/Kaggle/kaggle-ink-detection'\n","    BUFFER = 192  # Half-size of papyrus patches we'll use as model inputs\n","    CROP_SIZE = BUFFER * 2 # 384px\n","    STRIDE = 128 # 128\n","    Z_LIST = list(range(20, 40, 2)) # 10チャネル\n","    # Z_LIST = list(range(4, 16, 2)) + list(range(16, 36)) + list(range(36, 44, 2))\n","    Z_DIM = len(\n","        Z_LIST\n","    )  # Number of slices in the z direction. Max value is 64 - Z_START\n","    BATCH_Z_DIFF = 4\n","    BATCH_Z_INFO = [\n","        # (start, end)\n","        (0, 4),\n","        (2, 6),\n","        (4, 8),\n","        (6, 10),\n","    ]\n","    SHARED_HEIGHT = None  # Max height to resize all papyrii\n","\n","    # Model config\n","    BATCH_SIZE = 1\n","\n","    GPU_COUNT = 4\n","\n","    device = torch.device(\"cuda\")\n","    threshold = 0.5\n","    num_workers = 0\n","    exp = 1e-7\n","    mask_padding = (SHARED_HEIGHT // 80) if SHARED_HEIGHT is not None else 100\n","\n","    num_epochs = 40\n","    lr = 1e-3 # initial lr\n","    lr_scheduler_name = \"CosineAnnealingLR\"\n","    eta_min_lr = 1e-5\n","    WANDB_NOTE = \"data_listの順番を変更して[1, 3, 2a, 2b]になった.\"\n","    augmentation_names = [\n","        \"Horizontal\",\n","        \"Vertical\",\n","        \"RandomScale\",\n","        \"Transpose\",\n","        \"RandomRotate\",\n","        \"ShiftScaleRotate\",\n","        # \"Blur\",\n","        \"GridDistortion\",\n","        \"MultiChannelNoise\",\n","        \"PadIfNeeded\",\n","        \"Resize\",\n","        \"MixedUp\",\n","    ]\n","    \n","    mixed_up_alpha = 0.9\n","    mixed_up_probability = 0.4\n","    mixed_up_epochs = list(range(0, 7)) + list(range(num_epochs - 12, num_epochs - 7))\n","    \n","    loss1_alpha = 0.8\n","    loss1_beta = None\n","\n","    loss2a_alpha = 1.2\n","    loss2a_beta = None\n","\n","    loss2b_alpha = 1.2\n","\n","    loss1_weight = 0.5\n","    loss2_weight = 0.5\n","\n","    loss2a_weight = 0.5\n","    loss2b_weight = 0.5\n","\n","    lr_scheduler = optim.lr_scheduler.CosineAnnealingLR\n","    optimizer = \"AdamW\"\n","    # loss1 = smp.losses.TverskyLoss(\n","    #     smp.losses.BINARY_MODE,\n","    #     log_loss=False,\n","    #     from_logits=True,\n","    #     smooth=1e-7,\n","    #     alpha=loss1_alpha,\n","    #     beta=loss1_beta,\n","    # )\n","    loss1 = smp.losses.SoftBCEWithLogitsLoss(\n","        pos_weight=torch.tensor([loss1_alpha]),\n","    )\n","    # loss2a = smp.losses.TverskyLoss(\n","    #     smp.losses.BINARY_MODE,\n","    #     log_loss=False,\n","    #     from_logits=True,\n","    #     smooth=1e-7,\n","    #     alpha=loss2a_alpha,\n","    #     beta=loss2a_beta,\n","    # )\n","    loss2a = smp.losses.SoftBCEWithLogitsLoss(\n","        pos_weight=torch.tensor([loss2a_alpha]),\n","    )\n","    loss2b = nn.BCEWithLogitsLoss(\n","        pos_weight=torch.tensor([loss2b_alpha]),\n","    )\n","\n","    noise_intensity = (0.000025, 0.00005)\n","    \n","    mean = 0.42\n","    std = 0.3\n","    \n","    use_new_label_mask = False\n","    pretrained = False\n","    init_weight_bias = False\n","    SKIP_ZERO_ON_TRAIN = False\n","    model_name = \"seresnext26d_32x4d\"\n","    # MODEL_DEPTH = 34\n","    # MODEL_KIND = \"KM\"\n","    \n","    precision = \"float32\" # float16\n","\n","    interpolation = \"bicubic\" # PixelShuffling\n","    # use_pixel_shuffle = True\n","\n","    exp_name = \"123-gpu17\"\n","    prev_exp_name = \"102-gpu17\"\n","    \n","    attention_type = None\n","\n","\n","def class2dict(c):\n","    return {\n","        attr: getattr(c, attr)\n","        for attr in dir(c)\n","        if not callable(getattr(c, attr)) and not attr.startswith(\"__\")\n","    }"]},{"cell_type":"code","execution_count":410,"metadata":{},"outputs":[],"source":["import cupy as cp\n","xp = cp\n","\n","delta_lookup = {\n","    \"xx\": xp.array([[1, -2, 1]], dtype=float),\n","    \"yy\": xp.array([[1], [-2], [1]], dtype=float),\n","    \"xy\": xp.array([[1, -1], [-1, 1]], dtype=float),\n","}\n","\n","def operate_derivative(img_shape, pair):\n","    assert len(img_shape) == 2\n","    delta = delta_lookup[pair]\n","    fft = xp.fft.fftn(delta, img_shape)\n","    return fft * xp.conj(fft)\n","\n","def soft_threshold(vector, threshold):\n","    return xp.sign(vector) * xp.maximum(xp.abs(vector) - threshold, 0)\n","\n","def back_diff(input_image, dim):\n","    assert dim in (0, 1)\n","    r, n = xp.shape(input_image)\n","    size = xp.array((r, n))\n","    position = xp.zeros(2, dtype=int)\n","    temp1 = xp.zeros((r+1, n+1), dtype=float)\n","    temp2 = xp.zeros((r+1, n+1), dtype=float)\n","    \n","    temp1[position[0]:size[0], position[1]:size[1]] = input_image\n","    temp2[position[0]:size[0], position[1]:size[1]] = input_image\n","    \n","    size[dim] += 1\n","    position[dim] += 1\n","    temp2[position[0]:size[0], position[1]:size[1]] = input_image\n","    temp1 -= temp2\n","    size[dim] -= 1\n","    return temp1[0:size[0], 0:size[1]]\n","\n","\n","def forward_diff(input_image, dim):\n","    assert dim in (0, 1)\n","    r, n = xp.shape(input_image)\n","    size = xp.array((r, n))\n","    position = xp.zeros(2, dtype=int)\n","    temp1 = xp.zeros((r+1, n+1), dtype=float)\n","    temp2 = xp.zeros((r+1, n+1), dtype=float)\n","        \n","    size[dim] += 1\n","    position[dim] += 1\n","\n","    temp1[position[0]:size[0], position[1]:size[1]] = input_image\n","    temp2[position[0]:size[0], position[1]:size[1]] = input_image\n","    \n","    size[dim] -= 1\n","    temp2[0:size[0], 0:size[1]] = input_image\n","    temp1 -= temp2\n","    size[dim] += 1\n","    return -temp1[position[0]:size[0], position[1]:size[1]]\n","\n","def iter_deriv(input_image, b, scale, mu, dim1, dim2):\n","    g = back_diff(forward_diff(input_image, dim1), dim2)\n","    d = soft_threshold(g + b, 1 / mu)\n","    b = b + (g - d)\n","    L = scale * back_diff(forward_diff(d - b, dim2), dim1)\n","    return L, b\n","\n","def iter_xx(*args):\n","    return iter_deriv(*args, dim1=1, dim2=1)\n","\n","def iter_yy(*args):\n","    return iter_deriv(*args, dim1=0, dim2=0)\n","\n","def iter_xy(*args):\n","    return iter_deriv(*args, dim1=0, dim2=1)\n","\n","def iter_sparse(input_image, bsparse, scale, mu):\n","    d = soft_threshold(input_image + bsparse, 1 / mu)\n","    bsparse = bsparse + (input_image - d)\n","    Lsparse = scale * (d - bsparse)\n","    return Lsparse, bsparse\n","\n","def denoise_image(input_image, iter_num=100, fidelity=150, sparsity_scale=10, continuity_scale=0.5, mu=1):\n","    image_size = xp.shape(input_image)\n","    #print(\"Initialize denoising\")\n","    norm_array = (\n","        operate_derivative(image_size, \"xx\") + \n","        operate_derivative(image_size, \"yy\") + \n","        2 * operate_derivative(image_size, \"xy\")\n","    )\n","    norm_array += (fidelity / mu) + sparsity_scale ** 2\n","    b_arrays = {\n","        \"xx\": xp.zeros(image_size, dtype=float),\n","        \"yy\": xp.zeros(image_size, dtype=float),\n","        \"xy\": xp.zeros(image_size, dtype=float),\n","        \"L1\": xp.zeros(image_size, dtype=float),\n","    }\n","    g_update = xp.multiply(fidelity / mu, input_image)\n","    for i in tqdm(range(iter_num), total=iter_num):\n","        #print(f\"Starting iteration {i+1}\")\n","        g_update = xp.fft.fftn(g_update)\n","        if i == 0:\n","            g = xp.fft.ifftn(g_update / (fidelity / mu)).real\n","        else:\n","            g = xp.fft.ifftn(xp.divide(g_update, norm_array)).real\n","        g_update = xp.multiply((fidelity / mu), input_image)\n","        \n","        #print(\"XX update\")\n","        L, b_arrays[\"xx\"] = iter_xx(g, b_arrays[\"xx\"], continuity_scale, mu)\n","        g_update += L\n","        \n","        #print(\"YY update\")\n","        L, b_arrays[\"yy\"] = iter_yy(g, b_arrays[\"yy\"], continuity_scale, mu)\n","        g_update += L\n","        \n","        #print(\"XY update\")\n","        L, b_arrays[\"xy\"] = iter_xy(g, b_arrays[\"xy\"], 2 * continuity_scale, mu)\n","        g_update += L\n","        \n","        #print(\"L1 update\")\n","        L, b_arrays[\"L1\"] = iter_sparse(g, b_arrays[\"L1\"], sparsity_scale, mu)\n","        g_update += L\n","        \n","    g_update = xp.fft.fftn(g_update)\n","    g = xp.fft.ifftn(xp.divide(g_update, norm_array)).real\n","    \n","    g[g < 0] = 0\n","    g -= g.min()\n","    g /= g.max()\n","    return g"]},{"cell_type":"code","execution_count":411,"metadata":{},"outputs":[],"source":["def is_in_masked_zone(location, mask):\n","    return mask[location[0], location[1]] > 0\n"]},{"cell_type":"code","execution_count":412,"metadata":{},"outputs":[],"source":["def resize(img):\n","    current_height, current_width = img.shape    \n","    aspect_ratio = current_width / current_height\n","    if CFG.SHARED_HEIGHT is None:\n","        return img\n","    # new_height = CFG.SHARED_HEIGHT\n","    # pad_y = new_height - current_height\n","    # if pad_y > 0:\n","    #     # 元画像が小さい場合は解像度を大きくしないでpaddingをつける\n","    #     img = np.pad(img, [(0, pad_y), (0, 0)], constant_values=0)\n","    # else:\n","    # 既に十分でかい場合はリサイズする\n","    # 本当はpaddingしたいけど、メモリサイズが大きくなる\n","    if CFG.SHARED_HEIGHT > current_height:\n","        # 既に小さい場合はリサイズしない\n","        return img    \n","    new_height = CFG.SHARED_HEIGHT\n","    new_width = int(CFG.SHARED_HEIGHT * aspect_ratio)\n","    new_size = (new_width, new_height)\n","    # (W, H)の順で渡すが結果は(H, W)になっている\n","    img = cv2.resize(img, new_size)\n","    return img\n","\n","def load_mask(split, index):     \n","    img = cv2.imread(f\"{CFG.DATA_DIR}/{split}/{index}/mask.png\", 0) // 255\n","    img = resize(img)\n","    return img\n","\n","\n","def load_labels(split, index):    \n","    suffix = \"_new\" if CFG.use_new_label_mask else \"\"\n","    img = cv2.imread(f\"{CFG.DATA_DIR}/{split}/{index}/inklabels{suffix}.png\", 0) // 255    \n","    img = resize(img)\n","    return img"]},{"cell_type":"code","execution_count":413,"metadata":{},"outputs":[],"source":["def extract_subvolume(location, volume):\n","    global printed\n","    y = location[0]\n","    x = location[1]\n","    subvolume = volume[y-CFG.BUFFER:y+CFG.BUFFER, x-CFG.BUFFER:x+CFG.BUFFER, :].astype(np.float32)\n","    \n","    return subvolume"]},{"cell_type":"code","execution_count":414,"metadata":{},"outputs":[],"source":["def load_volume(split, index):    \n","    # Load the 3d x-ray scan, one slice at a time\n","    all = sorted(glob.glob(f\"{CFG.DATA_DIR}/{split}/{index}/surface_volume/*.tif\"))\n","    z_slices_fnames = [all[i] for i in range(len(all)) if i in CFG.Z_LIST]\n","    assert len(z_slices_fnames) == CFG.Z_DIM\n","    z_slices = []\n","    for z, filename in  tqdm(enumerate(z_slices_fnames)):\n","        img = cv2.imread(filename, -1)        \n","        img = resize(img)\n","        # img = (img / (2 ** 8)).astype(np.uint8)\n","        img = img.astype(np.float32) / 255\n","        z_slices.append(img)\n","    return np.stack(z_slices, axis=-1)"]},{"cell_type":"code","execution_count":415,"metadata":{},"outputs":[],"source":["def generate_locations_ds(volume, mask, label=None, skip_zero=False):\n","    is_in_mask_train = lambda x: is_in_masked_zone(x, mask)\n","\n","    # Create a list to store train locations\n","    locations = []\n","\n","    # Generate train locations\n","    volume_height, volume_width = volume.shape[:-1]\n","\n","    for y in range(CFG.BUFFER, volume_height - CFG.BUFFER, CFG.STRIDE):\n","        for x in range(CFG.BUFFER, volume_width - CFG.BUFFER, CFG.STRIDE):\n","            if skip_zero and CFG.SKIP_ZERO_ON_TRAIN and label is not None and np.all(label[y - CFG.BUFFER : y + CFG.BUFFER, x - CFG.BUFFER : x + CFG.BUFFER] == 0):\n","                # print(f\"skip location at (y: {y}, x: {x})\")\n","                continue\n","            if is_in_mask_train((y, x)):\n","                locations.append((y, x))\n","\n","    # Convert the list of train locations to a PyTorch tensor\n","    locations_ds = np.stack(locations, axis=0)\n","    return locations_ds"]},{"cell_type":"code","execution_count":416,"metadata":{},"outputs":[],"source":["import torch\n","import numpy as np\n","\n","def mixup_data(x, y):\n","    alpha = CFG.mixed_up_alpha\n","    if alpha > 0.:\n","        lam = np.random.beta(alpha, alpha)\n","    else:\n","        lam = 1.\n","\n","    batch_size = x.size()[0]\n","    index = torch.randperm(batch_size)\n","\n","    mixed_x = lam * x + (1 - lam) * x[index]    \n","    y_a, y_b = y, y[index]\n","    \n","    mixed_x = mixed_x.float()\n","    y_a = y_a.float()\n","    y_b = y_b.float()\n","    \n","    y = y_a * lam + y_b * (1 - lam)\n","\n","    return mixed_x, y, y_a, y_b, lam\n"]},{"cell_type":"code","execution_count":417,"metadata":{},"outputs":[],"source":["import numpy as np\n","from albumentations.core.transforms_interface import ImageOnlyTransform\n","\n","class MultichannelNoise(ImageOnlyTransform):\n","\n","    def __init__(self, intensity=CFG.noise_intensity, always_apply=False, p=0.5):\n","        super().__init__(always_apply, p)\n","        self.intensity = intensity\n","\n","    def apply(self, img, **params):\n","        intensity = np.random.uniform(*self.intensity)\n","        noise = np.random.normal(loc=0, scale=intensity, size=img.shape)\n","        # print(\"img\", img)\n","        img = img + noise        \n","        return np.clip(img, 0, 1).astype(np.float32) # クリップして0から255の範囲に保つ\n"]},{"cell_type":"code","execution_count":418,"metadata":{},"outputs":[],"source":["\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.preprocessing import OneHotEncoder\n","\n","from albumentations.core.transforms_interface import ImageOnlyTransform\n","\n","class SubvolumeDataset(Dataset):\n","    def __init__(self, locations, volume, labels, buffer, is_train: bool, return_location: bool = False):\n","        self.locations = locations\n","        self.volume = volume\n","        self.labels = labels        \n","        self.buffer = buffer\n","        self.is_train = is_train\n","        self.return_location = return_location\n","\n","    def __len__(self):\n","        return len(self.locations)\n","\n","    def __getitem__(self, idx):\n","        label = None\n","        location = np.array(self.locations[idx])\n","        y, x = location[0], location[1]\n","\n","        subvolume = extract_subvolume(location, self.volume)\n","        \n","        if self.labels is not None:\n","            label = self.labels[y - self.buffer:y + self.buffer, x - self.buffer:x + self.buffer]            \n","            label = np.stack([label], axis=-1)            \n","            \n","        # 段々meanは小さくなる\n","        mean = np.array([CFG.mean for i in range(0, CFG.Z_DIM)]).reshape(-1, 1, 1)\n","        # 段々stdは小さくなる\n","        std = np.array([CFG.std for i in range(0, CFG.Z_DIM)]).reshape(-1, 1, 1)\n","        \n","        if self.is_train and label is not None:    \n","            # label = smooth_labels(label, alpha=0.1)                  \n","            transformed = A.Compose([\n","                A.ToFloat(max_value=255),\n","                A.HorizontalFlip(p=0.5) if \"Horizontal\" in CFG.augmentation_names else A.Compose([]),\n","                A.VerticalFlip(p=0.5) if \"Vertical\" in CFG.augmentation_names else A.Compose([]),  \n","                A.RandomScale(p=0.4, scale_limit=0.3) if \"RandomScale\" in CFG.augmentation_names else A.Compose([]),\n","                A.Transpose(p=0.5) if \"Transpose\" in CFG.augmentation_names else A.Compose([]), \n","                A.RandomRotate90(p=0.5,) if \"RandomRotate\" in CFG.augmentation_names else A.Compose([]), \n","                A.ShiftScaleRotate(p=0.7, scale_limit=0.35, rotate_limit=45) if \"ShiftScaleRotate\" in CFG.augmentation_names else A.Compose([]),\n","                A.MotionBlur(p=0.2) if \"Blur\" in CFG.augmentation_names else A.Compose([]),\n","                A.GridDistortion(num_steps=5, distort_limit=0.15, p=0.3) if \"GridDistortion\" in CFG.augmentation_names else A.Compose([]),\n","                MultichannelNoise(\n","                    p=0.2,\n","                ) if \"MultichannelNoise\" in CFG.augmentation_names else A.Compose([]),\n","                A.PadIfNeeded(min_height=self.buffer * 2, min_width=self.buffer * 2) if \"PadIfNeeded\" in CFG.augmentation_names else A.Compose([]),\n","                A.Resize(height=self.buffer * 2, width=self.buffer * 2) if \"Resize\" in CFG.augmentation_names else A.Compose([]),\n","            ])(image=subvolume, mask=label)            \n","            subvolume = transformed[\"image\"]\n","            label = transformed[\"mask\"]\n","            subvolume = np.transpose(subvolume, (2, 0, 1))\n","            label = np.transpose(label, (2, 0, 1))\n","            subvolume = (subvolume - mean) / std            \n","        else:\n","            if label is None:\n","                subvolume = np.transpose(subvolume, (2, 0, 1))\n","                subvolume /= 255.\n","                subvolume = (subvolume - mean) / std\n","            else:\n","                # print(\"subvolume in val dataset (before aug)\", subvolume, file=open(\"before-val-aug.log\", \"w\")) \n","                subvolume = np.transpose(subvolume, (2, 0, 1))\n","                label = np.transpose(label, (2, 0, 1))\n","                subvolume /= 255.\n","                subvolume = (subvolume - mean) / std\n","        # print(\"subvolume\", subvolume)\n","        if self.return_location:\n","            return subvolume, location\n","        return subvolume, label        "]},{"cell_type":"code","execution_count":419,"metadata":{},"outputs":[],"source":["def initialize_weights(model: nn.Module):\n","    for module in model.modules():\n","        if isinstance(module, (nn.Conv2d, nn.Linear)):\n","            nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n","            if module.bias is not None:\n","                nn.init.constant_(module.bias, 0)\n","        elif isinstance(module, nn.BatchNorm2d):\n","            nn.init.constant_(module.weight, 1)\n","            nn.init.constant_(module.bias, 0)"]},{"cell_type":"code","execution_count":420,"metadata":{},"outputs":[],"source":["def init_subpixel(weight):\n","    co, ci, h, w = weight.shape\n","    co2 = co // 4\n","    # initialize sub kernel\n","    k = torch.empty([co2, ci, h, w])\n","    nn.init.kaiming_uniform_(k)\n","    # repeat 4 times\n","    k = k.repeat_interleave(4, dim=0)\n","    weight.data.copy_(k)"]},{"cell_type":"code","execution_count":421,"metadata":{},"outputs":[],"source":["def init_linear(m, relu=True):\n","    if relu: nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n","    else: nn.init.xavier_uniform_(m.weight)\n","    if m.bias is not None: nn.init.zeros_(m.bias)"]},{"cell_type":"code","execution_count":422,"metadata":{},"outputs":[],"source":["def initialize_weights(model: nn.Module):\n","    for module in model.modules():\n","        if isinstance(module, (nn.Conv2d, nn.Linear)):\n","            nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n","            if module.bias is not None:\n","                nn.init.constant_(module.bias, 0)\n","        elif isinstance(module, nn.BatchNorm2d):\n","            nn.init.constant_(module.weight, 1)\n","            nn.init.constant_(module.bias, 0)"]},{"cell_type":"code","execution_count":423,"metadata":{},"outputs":[],"source":["class SmpUnetDecoder(nn.Module):\n","\tdef __init__(\n","\t\tself,\n","\t\tin_channel,\n","\t\tskip_channel,\n","\t\tout_channel,\n","\t):\n","\t\tsuper().__init__()\n","\t\tself.center = nn.Identity()\n","\n","\t\ti_channel = [\n","\t\t\tin_channel,\n","\t\t] + out_channel[:-1]\n","\t\ts_channel = skip_channel\n","\t\to_channel = out_channel\n","\t\t# print(\"i:\", i_channel)\n","\t\t# print(\"s:\", s_channel)\n","\t\t# print(\"o:\", o_channel)\n","\t\tblock = [\n","\t\t\tDecoderBlock(i, s, o, use_batchnorm=True, attention_type=CFG.attention_type)\n","\t\t\tfor i, s, o in zip(i_channel, s_channel, o_channel)\n","\t\t]\n","\t\tself.block = nn.ModuleList(block)\n","\n","\tdef forward(self, feature, skip):\n","\t\t# for s in skip:\n","\t\t\t# print(\"feature\", feature.shape, \"skip\", s.shape)\n","\t\td = self.center(feature)\n","\t\tdecode = []\n","\t\tfor i, block in enumerate(self.block):\n","\t\t\ts = skip[i]\n","\t\t\td = block(d, s)\n","\t\t\tdecode.append(d)\n","\n","\t\tlast = d\n","\t\treturn last, decode\n","\n","class Net(nn.Module):\n","\tdef __init__(\n","\t\tself,\n","\t):\n","\t\tsuper().__init__()\n","\t\tself.output_type = [\"inference\", \"loss\"]\n","\n","\t\tconv_dim = 64\n","\t\tencoder1_dim = [\n","\t\t\tconv_dim,\n","\t\t\t256,\n","\t\t\t512,\n","\t\t\t1024,\n","\t\t\t2048,\n","\t\t]\n","\t\tdecoder1_dim = [\n","\t\t\t1024,\n","\t\t\t512,\n","\t\t\t256,\n","\t\t\t64,\n","\t\t]\n","\t\tif CFG.model_name == \"seresnext50_32x4d\":\n","\t\t\tself.encoder1 = seresnext50_32x4d(pretrained=CFG.pretrained, in_chans=CFG.BATCH_Z_DIFF)\n","\t\telif CFG.model_name == \"seresnext26d_32x4d\":\n","\t\t\tself.encoder1 = seresnext26d_32x4d(pretrained=CFG.pretrained, in_chans=CFG.BATCH_Z_DIFF)\n","\t\telif CFG.model_name == \"seresnext26tn_32x4d\":\n","\t\t\tself.encoder1 = seresnext26tn_32x4d(pretrained=CFG.pretrained, in_chans=CFG.BATCH_Z_DIFF)\n","\n","\t\tself.decoder1 = SmpUnetDecoder(\n","\t\t\tin_channel=encoder1_dim[-1],\n","\t\t\tskip_channel=encoder1_dim[:-1][::-1],\n","\t\t\tout_channel=decoder1_dim,\n","\t\t)\n","\t\t# -- pool attention weight  \n","\t\tself.weight1 = nn.ModuleList(\n","\t\t\t[\n","\t\t\t\tnn.Sequential(\n","\t\t\t\t\tnn.Conv2d(dim, dim, kernel_size=3, padding=1),\n","\t\t\t\t\tnn.BatchNorm2d(dim),\n","\t\t\t\t\tnn.ReLU(inplace=True),\n","\t\t\t\t)\n","\t\t\t\tfor dim in encoder1_dim\n","\t\t\t]\n","\t\t)\n","\t\tself.logit1 = nn.Conv2d(decoder1_dim[-1], 1, kernel_size=1)\n","\n","\t\t# --------------------------------\t\t\n","\t\tencoder2_dim = [64, 64, 128, 256, 512]\n","\t\tdecoder2_dim = [\n","\t\t\t256,\n","\t\t\t128,\n","\t\t\t64,\n","\t\t\t64,\n","\t\t]\n","\t\tself.encoder2 = resnet10t(pretrained=CFG.pretrained, in_chans=decoder1_dim[-1])\n","\n","\t\tself.decoder2 = SmpUnetDecoder(\n","\t\t\tin_channel=encoder2_dim[-1],\n","\t\t\tskip_channel=encoder2_dim[:-1][::-1],\n","\t\t\tout_channel=decoder2_dim,\n","\t\t)\n","\t\tself.logit2 = nn.Conv2d(decoder2_dim[-1], 1, kernel_size=1)\n","\n","\t\tif CFG.init_weight_bias:\n","\t\t\tinitialize_weights(self.logit1)\n","\t\t\tinitialize_weights(self.logit2)\n","\t\t\tinitialize_weights(self.weight1)\n","\n","\tdef forward(self, batch):\n","\t\tv = batch\n","\t\tB, C, H, W = v.shape\n","\t\tvv = [\n","\t\t\tv[:, a: b]\n","\t\t\tfor a, b in CFG.BATCH_Z_INFO\n","\t\t]\n","\t\t# print(list(map(lambda a: a.shape, vv)))\n","\t\tK = len(vv)\n","\t\tx = torch.cat(vv, 0)\n","\t\t# x = v\n","\n","\t\t# ----------------------\n","\t\tencoder = []\n","\t\te = self.encoder1\n","\t\tx = e.conv1(x)\n","\t\tx = e.bn1(x)\n","\t\tx = e.act1(x)\n","\t\tencoder.append(x)\n","\t\tx = F.avg_pool2d(x, kernel_size=2, stride=2)\n","\t\tx = e.layer1(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer2(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer3(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer4(x)\n","\t\tencoder.append(x)\n","\t\t# print('encoder', [f.shape for f in encoder])\n","\n","\t\tfor i in range(len(encoder)):\n","\t\t\te = encoder[i]\n","\t\t\te = F.avg_pool2d(e, 2, 2)\n","\t\t\tf = self.weight1[i](e)\n","\t\t\t_, c, h, w = e.shape\n","\t\t\tf = rearrange(f, \"(K B) c h w -> B K c h w\", K=K, B=B, h=h, w=w)  #\n","\t\t\te = rearrange(e, \"(K B) c h w -> B K c h w\", K=K, B=B, h=h, w=w)  #\n","\t\t\tw = F.softmax(f, 1)\n","\t\t\te = (w * e).sum(1)\n","\t\t\tencoder[i] = e\n","\n","\t\tfeature = encoder[-1]\n","\t\tskip = encoder[:-1][::-1]\n","\t\t# print('encoder after weighted', [f.shape for f in encoder])\n","\t\t# print('skip after weighted', [f.shape for f in skip])\n","\t\tlast, decoder = self.decoder1(feature, skip)\n","\t\tlogit1 = self.logit1(last)\n","\t\t# print(\"last.shape\", last.shape)\n","\t\t# print(\"logit1.shape\", logit1.shape)\n","\t\t# print(\"encoder1 shapes\", list(map(lambda a: a.shape, encoder)))\n","  \n","\t\tlogit1 = F.interpolate(\n","\t\t\tlogit1, size=(H, W), mode=CFG.interpolation, align_corners=False, antialias=True\n","\t\t)\n","\n","\t\t# ----------------------\n","\t\tx = last  # .detach()\n","\t\t# x = F.avg_pool2d(x,kernel_size=2,stride=2)\n","\t\tencoder = []\n","\t\te = self.encoder2\n","\t\tx = e.conv1(x)\n","\t\tx = e.bn1(x)\n","\t\tx = e.act1(x)\n","\t\tencoder.append(x)\n","\t\tx = F.avg_pool2d(x, kernel_size=2, stride=2)\n","\t\tx = e.layer1(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer2(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer3(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer4(x)\n","\t\tencoder.append(x)\n","\t\t\n","\t\tfeature = encoder[-1]\n","\t\tskip = encoder[:-1][::-1]\n","\n","\t\t# print(\"feature.shape\", list(map(lambda a: a.shape, feature)))\n","\t\t# print(\"skip.shape\", list(map(lambda a: a.shape, skip)))\n","\n","\t\tlast, decoder = self.decoder2(feature, skip)\n","\t\tlogit2 = self.logit2(last)\n","\t\t# print(\"logit2.shape\", logit2.shape)\n","\t\tlogit2 = F.interpolate(\n","\t\t\tlogit2, size=(H, W), mode=CFG.interpolation, align_corners=False, antialias=True\n","\t\t)\n","\t\treturn logit1, logit2\n","\t\t# return logit1"]},{"cell_type":"code","execution_count":424,"metadata":{},"outputs":[],"source":["# ref - https://www.kaggle.com/competitions/vesuvius-challenge-ink-detection/discussion/397288\n","def fbeta_score(preds, targets, threshold, beta=0.5, smooth=1e-5):\n","    preds_t = torch.where(preds > threshold, 1.0, 0.0).float()\n","    y_true_count = targets.sum()\n","    \n","    ctp = preds_t[targets==1].sum()\n","    cfp = preds_t[targets==0].sum()\n","    beta_squared = beta * beta\n","\n","    c_precision = ctp / (ctp + cfp + smooth)\n","    c_recall = ctp / (y_true_count + smooth)\n","    dice = (1 + beta_squared) * (c_precision * c_recall) / (beta_squared * c_precision + c_recall + smooth)\n","\n","    return dice"]},{"cell_type":"code","execution_count":425,"metadata":{},"outputs":[],"source":["tc = torch\n","def TTA(x:tc.Tensor,model:nn.Module):\n","    #x.shape=(batch,c,h,w)\n","    shape=x.shape\n","    x=[x,*[tc.rot90(x,k=i,dims=(-2,-1)) for i in range(1,4)]]\n","    x=tc.cat(x,dim=0)\n","    _, x = model(x)\n","    x=x.reshape(4,shape[0], 1 ,*shape[2:])\n","    x=[tc.rot90(x[i],k=-i,dims=(-2,-1)) for i in range(4)]\n","    x=tc.stack(x,dim=0)\n","    return x.mean(0)"]},{"cell_type":"code","execution_count":426,"metadata":{},"outputs":[],"source":["\n","\n","class Model(pl.LightningModule):\n","\ttraining_step_outputs = []\n","\tvalidation_step_outputs = []\n","\ttest_step_outputs = [[], []]\n","\n","\tdef __init__(self, **kwargs):\n","\t\tsuper().__init__()\n","\n","\t\tself.model = Net()        \n","\n","\t\tself.loss1 = CFG.loss1\n","\t\tself.loss2 = CFG.loss2a\n","\t\tself.loss3 = CFG.loss2b\n","\n","\tdef forward(self, image, stage):\n","\t\tif stage != \"train\":\n","\t\t\tmask = TTA(image, self.model)\n","\t\t\t# _, mask = self.model(image)\n","\t\telse:\n","\t\t\tmask = self.model(image)\n","\t\treturn mask\n","\n","\tdef shared_step(self, batch, stage):\n","\t\tsubvolumes, labels = batch\n","\n","\t\timage, labels = subvolumes.float(), labels.float()        \n","\t\tassert image.ndim == 4\n","\t\t\n","\t\th, w = image.shape[2:]\n","\t\tassert h % 32 == 0 and w % 32 == 0\n","\t\t\n","\t\t# print(\"labels\", labels.max(), labels.min())\n","\n","\t\tassert labels.max() <= 1.0 and labels.min() >= 0\n","\n","\t\tif stage == \"train\":\n","\t\t\t\n","\t\t\t# decide if perform mixed-up augmentation\n","\t\t\tshould_mixed_up = \"MixedUp\" in CFG.augmentation_names and  np.random.rand(1)[0] <= CFG.mixed_up_probability and self.current_epoch in range(CFG.start_mixedup_epochs, CFG.stop_mixedup_epochs)\n","\t\t\tif should_mixed_up:\n","\t\t\t\t# perform mixed-up\n","\t\t\t\timage, labels, labels_a, labels_b, lam = mixup_data(subvolumes, labels)\n","\t\t\t\n","\t\t\t\n","\t\t\tlogit1, logit2 = self.forward(image, stage)\n","\t\t\tif should_mixed_up:\n","\t\t\t\tloss1 = self.loss1(logit1, labels)\n","\t\t\t\tloss2a = self.loss2(logit2, labels)\n","\t\t\t\tloss2b = self.loss3(logit2, labels_a) * lam + self.loss3(logit2, labels_b) * (1 - lam)\n","\t\t\t\t# loss2b = self.loss3(logit2, labels)\n","\t\t\t\tloss = CFG.loss1_weight * loss1 + CFG.loss2_weight * (loss2a * CFG.loss2a_weight + loss2b * CFG.loss2b_weight)\n","\t\t\telse:\n","\t\t\t\tloss = self.loss1(logit1, labels) * CFG.loss1_weight + CFG.loss2_weight * (self.loss2(logit2, labels) * CFG.loss2a_weight + self.loss3(logit2, labels) * CFG.loss2b_weight)\n","\t\t\t# loss = self.loss2(logit, labels) * CFG.loss2a_weight + self.loss3(logit, labels) * CFG.loss2b_weight\n","\t\t\tlogit = logit2\n","\t\telse:\n","\t\t\tlogit = self.forward(image, stage)\n","\t\t\tloss = self.loss2(logit, labels) * CFG.loss2a_weight + self.loss3(logit, labels) * CFG.loss2b_weight\n","\t\t\n","\t\tprob2 = torch.sigmoid(logit)\n","\n","\t\tpred_mask = (prob2 > CFG.threshold).float()\n","\t\t\n","\t\t# print(\"pred_mask\", pred_mask)\n","\t\t\n","\t\tscore = fbeta_score(pred_mask, labels, threshold=CFG.threshold)\n","\n","\t\ttp, fp, fn, tn = smp.metrics.get_stats(\n","\t\t\tpred_mask.long(), labels.long(), mode=\"binary\"\n","\t\t)\n","\t\t# cur_lr = self.lr_schedulers().get_last_lr()\n","\n","\t\treturn {\n","\t\t\t\"loss\": loss,\n","\t\t\t\"tp\": tp,\n","\t\t\t\"fp\": fp,\n","\t\t\t\"fn\": fn,\n","\t\t\t\"tn\": tn,\n","\t\t\t\"score\": score,\n","\t\t\t# \"lr\": cur_lr,\n","\t\t}\n","\n","\tdef shared_epoch_end(self, outputs, stage):\n","\t\t# aggregate step metics\n","\t\ttp = torch.cat([x[\"tp\"] for x in outputs])\n","\t\tfp = torch.cat([x[\"fp\"] for x in outputs])\n","\t\tfn = torch.cat([x[\"fn\"] for x in outputs])\n","\t\ttn = torch.cat([x[\"tn\"] for x in outputs])\n","\t\t# lr = outputs[0][\"lr\"]\n","\t\tloss = torch.mean(torch.Tensor([x[\"loss\"] for x in outputs]))\n","\t\tfbeta_score = torch.mean(torch.Tensor([x[\"score\"] for x in outputs]))\n","\n","\t\tper_image_iou = smp.metrics.iou_score(\n","\t\t\ttp, fp, fn, tn, reduction=\"micro-imagewise\"\n","\t\t)\n","\n","\t\tdataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n","\n","\t\tmetrics = {\n","\t\t\tf\"{stage}_per_image_iou\": per_image_iou,\n","\t\t\tf\"{stage}_dataset_iou\": dataset_iou,\n","\t\t\tf\"{stage}_loss\": 10000 if loss.item() == 0 else loss.item(),\n","\t\t\tf\"{stage}_tp\": tp.sum().int().item(),\n","\t\t\tf\"{stage}_fp\": fp.sum().int().item(),\n","\t\t\tf\"{stage}_fn\": fn.sum().int().item(),\n","\t\t\tf\"{stage}_tn\": tn.sum().int().item(),\n","\t\t\tf\"{stage}_score\": fbeta_score.item(),\n","\t\t\t# \"lr\": lr,\n","\t\t}\n","\n","\t\tself.log_dict(metrics, prog_bar=True, sync_dist=True)\n","\n","\tdef training_step(self, batch, batch_idx):\n","\t\tout = self.shared_step(batch, \"train\")\n","\t\tself.training_step_outputs.append(out)\n","\t\treturn out\n","\n","\tdef on_train_epoch_end(self):\n","\t\tout = self.shared_epoch_end(self.training_step_outputs, \"train\")\n","\t\tself.training_step_outputs.clear()\n","\t\treturn out\n","\n","\tdef validation_step(self, batch, batch_idx):\n","\t\tout = self.shared_step(batch, \"valid\")\n","\t\tself.validation_step_outputs.append(out)\n","\t\treturn out\n","\n","\tdef on_validation_epoch_end(self):\n","\t\tout = self.shared_epoch_end(self.validation_step_outputs, \"valid\")\n","\t\tself.validation_step_outputs.clear()\n","\t\treturn out\n","\n","\tdef test_step(self, batch, batch_idx):\n","\t\tglobal predictions_map, predictions_map_counts\n","\n","\t\tpatch_batch, loc_batch = batch\n","\n","\t\tloc_batch = loc_batch.long()\n","\t\tpatch_batch = patch_batch.float()\n","\t\tpredictions = self.forward(patch_batch, \"test\")\n","\t\tpredictions = predictions.sigmoid()\n"," \n","\t\tpredictions = torch.permute(predictions, (0, 2, 3, 1)).squeeze(dim=-1)\n","\t\tpredictions = (\n","\t\t\tpredictions.cpu().numpy()\n","\t\t)\n","\t\tloc_batch = loc_batch.cpu().numpy()\n","\t\t\n","\t\tself.test_step_outputs[0].extend(loc_batch)\n","\t\tself.test_step_outputs[1].extend(predictions)        \n","\t\treturn loc_batch, predictions\n","\n","\tdef on_test_epoch_end(self):\n","\t\tglobal predictions_map, predictions_map_counts\n","\n","\t\tlocs = np.array(self.test_step_outputs[0])\n","\t\tpreds = np.array(self.test_step_outputs[1])\n","\t\tprint(\"locs\", locs.shape)\n","\t\tprint(\"preds\", preds.shape)\n","\n","\t\tnew_predictions_map = np.zeros_like(predictions_map[:, :, 0])\n","\t\tnew_predictions_map_counts = np.zeros_like(predictions_map_counts[:, :, 0])\n","\n","\t\tfor (y, x), pred in zip(locs, preds):\n","\t\t\tnew_predictions_map[\n","\t\t\t\ty - CFG.BUFFER : y + CFG.BUFFER, x - CFG.BUFFER : x + CFG.BUFFER\n","\t\t\t] += pred\n","\t\t\tnew_predictions_map_counts[\n","\t\t\t\ty - CFG.BUFFER : y + CFG.BUFFER, x - CFG.BUFFER : x + CFG.BUFFER\n","\t\t\t] += 1\n","\t\t\n","\t\tnew_predictions_map /= new_predictions_map_counts + CFG.exp\n","\t\tnew_predictions_map = new_predictions_map[:, :, np.newaxis]\n","\t\tpredictions_map = np.concatenate(\n","\t\t\t[predictions_map, new_predictions_map], axis=-1\n","\t\t)\n","\t\n","\tdef configure_optimizers(self):\n","\t\t# optimizer = optim.SGD(self.parameters(), lr=CFG.lr)\n","\t\tif CFG.optimizer == \"Adam\":\n","\t\t\toptimizer = optim.Adam(self.parameters(), lr=CFG.lr)\n","\t\telif CFG.optimizer == \"AdamW\":\n","\t\t\toptimizer = optim.AdamW(self.parameters(), lr=CFG.lr)\n","\t\t\n","\t\tif CFG.lr_scheduler_name == \"ReduceLROnPlateau\":\n","\t\t\tscheduler = CFG.lr_scheduler(\n","\t\t\t\toptimizer, mode=\"min\", factor=0.75, patience=5,\n","\t\t\t)\n","\t\t\treturn {\n","\t\t\t\t\"optimizer\": optimizer,\n","\t\t\t\t\"lr_scheduler\": {\"scheduler\": scheduler, \"monitor\": \"valid_loss\"},\n","\t\t\t}\n","\t\telif CFG.lr_scheduler_name == \"CosineAnnealingLR\":\n","\t\t\tscheduler = CFG.lr_scheduler(\n","\t\t\t\toptimizer, T_max=CFG.num_epochs, eta_min=CFG.eta_min_lr,\n","\t\t\t)\n","\t\t\treturn {\n","\t\t\t\t\"optimizer\": optimizer,\n","\t\t\t\t\"lr_scheduler\": scheduler,\n","\t\t\t}"]},{"cell_type":"code","execution_count":427,"metadata":{},"outputs":[],"source":["class EnsembleModel:\n","    def __init__(self, test_loader, test_volume):\n","        super().__init__()\n","        self.test_loader = test_loader\n","        self.test_volume = test_volume\n","        self.list = []\n","        for fold in [1, 2, 3, 4]:\n","            _model = Model.load_from_checkpoint(\n","                f\"weights/weights_fold-{fold}.ckpt\",               \n","            )\n","            trainer = pl.Trainer(\n","                accelerator=\"gpu\",\n","                devices=\"1\",\n","                precision=16 if CFG.precision == \"float16\" else 32,\n","                enable_checkpointing=False,\n","            )\n","\n","            self.list.append((_model, trainer))\n","    \n","    def forward(self):\n","        global predictions_map, predictions_map_counts\n","        predictions_map = np.empty_like(self.test_volume[:, :, 0])[:, :, np.newaxis].astype(np.float64)\n","        predictions_map_counts = np.empty_like(predictions_map).astype(np.uint16)\n","        for i, (model, trainer) in enumerate(self.list):\n","            model: Model = model\n","            model.test_step_outputs = [[], []]\n","            model.eval()\n","            trainer.test(\n","                model=model,\n","                dataloaders=self.test_loader,\n","                verbose=True,\n","            )\n","            self.list[i] = None\n","            gc.collect()\n","        predictions_map = predictions_map[:, :, 1:].mean(axis=-1, keepdims=True)"]},{"cell_type":"code","execution_count":428,"metadata":{},"outputs":[],"source":["def opening(img):\n","    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (20, 20))\n","    return cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel, iterations=2)"]},{"cell_type":"code","execution_count":429,"metadata":{},"outputs":[],"source":["\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","from tqdm import tqdm\n","from skimage.transform import resize as resize_ski\n","import pathlib\n","import gc\n","\n","def compute_predictions_map(split, index):\n","    global predictions_map\n","    global predictions_map_counts\n","    \n","    print(f\"Load data for {split}/{index}\")\n","\n","    test_volume = load_volume(split=split, index=index)\n","    test_mask = load_mask(split=split, index=index)    \n","    \n","    #get coord\n","    stride = CFG.BUFFER\n","    H,W,D  = test_volume.shape\n","\n","    ##pad #assume H,W >size\n","    px, py = W % stride, H % stride\n","    if (px != 0) or (py != 0):\n","        px = stride - px\n","        py = stride - py\n","        test_volume = np.pad(test_volume, [(0, py), (0, px), (0, 0)], constant_values=0)\n","        test_mask = np.pad(test_mask, [(0, py), (0, px)], constant_values=0)  \n","        \n","    test_locations = generate_locations_ds(test_volume, test_mask)  \n","    \n","    print(f\"{len(test_locations)} test locations (before filtering by mask)\")\n","\n","    # filter locations inside the mask\n","    test_locations = [loc for loc in test_locations if is_in_masked_zone(loc, test_mask)]\n","    \n","    print(f\"{len(test_locations)} test locations (after filtering by mask)\")\n","\n","    test_ds = SubvolumeDataset(test_locations, test_volume, None, CFG.BUFFER, is_train=False, return_location=True)\n","    test_loader = DataLoader(test_ds, batch_size=CFG.BATCH_SIZE, num_workers=CFG.num_workers, pin_memory=True)        \n","\n","    # # shape: (Y, X, C)\n","    predictions_map = np.zeros_like(test_volume[:, :, 0])[:, :, np.newaxis].astype(np.float64)\n","    predictions_map_counts = np.zeros_like(predictions_map).astype(np.uint8)\n","\n","    print(\"test_volume.shape\", test_volume.shape)\n","    print(\"predictions_map.shape\", predictions_map.shape)\n","\n","    print(f\"Compute predictions\")\n","    \n","    model = EnsembleModel(test_loader, test_volume)\n","    model.forward()\n","    del model\n","    del test_locations\n","    del test_loader\n","    del test_ds\n","    del test_volume\n","    del test_mask\n","    gc.collect()\n","    \n","    # print(\"predictions_map\", predictions_map, file=open(\"predictions_map\", \"w\"))\n","    return predictions_map[:H, :W, :]\n"]},{"cell_type":"code","execution_count":430,"metadata":{},"outputs":[],"source":["import numpy as np\n","import itertools\n","\n","# (H, W)の形式\n","def rle(img: np.ndarray, threshold: float):\n","    flat_img = img.flatten()\n","    flat_img = np.where(flat_img > threshold, 1, 0).astype(np.uint8)\n","\n","    starts = np.array((flat_img[:-1] == 0) & (flat_img[1:] == 1))\n","    ends = np.array((flat_img[:-1] == 1) & (flat_img[1:] == 0))\n","\n","    starts_ix = np.where(starts)[0]\n","    ends_ix = np.where(ends)[0]\n","\n","    # If the array ends in a run, we need to add a manual end index at the end of the array\n","    if flat_img[-1] == 1:\n","        ends_ix = np.append(ends_ix, flat_img.size - 1)\n","\n","    starts_ix += 1  # Indices are 1-based, not 0-based\n","    lengths = ends_ix - starts_ix + 1\n","\n","    runs = list(itertools.chain.from_iterable(zip(starts_ix, lengths)))\n","    \n","    return \" \".join(map(str, runs))\n"]},{"cell_type":"code","execution_count":431,"metadata":{},"outputs":[],"source":["def update_submission(predictions_map, index):\n","    rle_ = rle(predictions_map, threshold=CFG.threshold)\n","    print(f\"{index},\" + rle_, file=open('submission.csv', 'a'))\n","    # print(f\"{index},\" + rle_, file=open('/kaggle/working/submission.csv', 'a'))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Submission"]},{"cell_type":"code","execution_count":432,"metadata":{},"outputs":[],"source":["# # now = datetime.datetime.now()\n","# # now = f\"{now}\".replace(\" \", \"\")\n","# now = \"\"\n","# predictions_map = None\n","# predictions_map_counts = None\n","# print(\"Id,Predicted\", file=open('submission.csv', 'w'))\n","# kind = \"test\"\n","# folder = pathlib.Path(CFG.DATA_DIR) / kind\n","# for p in list(folder.iterdir()):\n","#     index = p.stem\n","#     predictions_map = compute_predictions_map(split=kind, index=index).squeeze(axis=-1)\n","#     original_size = cv2.imread(CFG.DATA_DIR + f\"/{kind}/{index}/mask.png\", 0).shape[:2]\n","#     print(\"original size\", original_size)\n","#     print(\"predictions_map\", predictions_map.shape)\n","#     predictions_map=xp.array(predictions_map[:, :])\n","#     # train/2の場合、先にリサイズするとout of memoryになる\n","#     predictions_map=denoise_image(predictions_map, iter_num=100)\n","#     predictions_map=predictions_map.get()    \n","# #     predictions_map = opening(predictions_map)\n","# #     predictions_map = resize_ski(predictions_map, (original_size[0], original_size[1]), anti_aliasing=True)\n","#     update_submission(predictions_map, index)\n","#     predictions_map = np.where(predictions_map >= CFG.threshold, 255, 0)\n","#     plt.imsave(f\"{index}_{str(CFG.threshold)}_{str(CFG.BUFFER)}_{now}.png\", predictions_map, cmap=\"gray\")\n","#     del predictions_map\n","#     del predictions_map_counts\n","#     gc.collect()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Validation"]},{"cell_type":"code","execution_count":433,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Load data for train/1\n"]},{"name":"stderr","output_type":"stream","text":["10it [00:00, 21.40it/s]\n"]},{"name":"stdout","output_type":"stream","text":["771 test locations (before filtering by mask)\n","771 test locations (after filtering by mask)\n","test_volume.shape (8256, 6336, 10)\n","predictions_map.shape (8256, 6336, 1)\n","Compute predictions\n"]},{"name":"stderr","output_type":"stream","text":["GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a7fe64dc9c3a45cbb35ba0ac1dd8c81c","version_major":2,"version_minor":0},"text/plain":["Testing: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["locs (771, 2)\n","preds (771, 384, 384)\n"]},{"name":"stderr","output_type":"stream","text":["LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9011f927336d48c2af1588638a262623","version_major":2,"version_minor":0},"text/plain":["Testing: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["locs (771, 2)\n","preds (771, 384, 384)\n"]},{"name":"stderr","output_type":"stream","text":["LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eae67ff2688e4094a2ea0044c0ac26dc","version_major":2,"version_minor":0},"text/plain":["Testing: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["locs (771, 2)\n","preds (771, 384, 384)\n"]},{"name":"stderr","output_type":"stream","text":["LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6af75696a0ca4843a27ea4fbc7d37c47","version_major":2,"version_minor":0},"text/plain":["Testing: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["locs (771, 2)\n","preds (771, 384, 384)\n","original size (8181, 6330)\n","predictions_map (8181, 6330)\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 100/100 [00:39<00:00,  2.56it/s]\n"]},{"name":"stdout","output_type":"stream","text":["cv score with thd=0.47 of train/1: 0.9222993850708008\n","cv score with thd=0.475 of train/1: 0.9222012162208557\n","cv score with thd=0.495 of train/1: 0.9214496612548828\n","cv score with thd=0.5 of train/1: 0.9210970997810364\n"]}],"source":["now = datetime.datetime.now()\n","now = f\"{now}\".replace(\" \", \"\")\n","# now = \"\"\n","predictions_map = None\n","predictions_map_counts = None\n","print(\"Id,Predicted\", file=open('submission.csv', 'w'))\n","kind = \"train\"\n","folder = pathlib.Path(CFG.DATA_DIR) / kind\n","cv_file = pathlib.Path(CFG.DATA_DIR) / kind / \"1\"\n","# for p in list(folder.iterdir()):\n","for p in [cv_file]:\n","    index = p.stem\n","    predictions_map = compute_predictions_map(split=kind, index=index).squeeze(axis=-1)\n","    original_size = cv2.imread(CFG.DATA_DIR + f\"/{kind}/{index}/mask.png\", 0).shape[:2]\n","    print(\"original size\", original_size)\n","    print(\"predictions_map\", predictions_map.shape)\n","    predictions_map=xp.array(predictions_map[:, :])\n","    # train/2の場合、先にリサイズするとout of memoryになる\n","    predictions_map=denoise_image(predictions_map, iter_num=100)\n","    predictions_map=predictions_map.get()\n","    # predictions_map = opening(predictions_map)\n","#     predictions_map = resize_ski(predictions_map, (original_size[0], original_size[1]), anti_aliasing=True)\n","\n","    # Check cv results\n","\n","    labels = load_labels(kind, index)\n","    labels = torch.from_numpy(labels)\n","    for thd in [0.47, 0.475, 0.495, 0.5]:\n","        _predictions_map = torch.from_numpy(predictions_map)\n","        score = fbeta_score(_predictions_map, labels, thd)\n","        print(f\"cv score with thd={thd} of {kind}/{index}: {score}\")\n","        __predictions_map = np.where(predictions_map >= thd, 255, 0)\n","        plt.imsave(f\"{index}_{str(thd)}_{str(CFG.BUFFER)}_{now}.png\", __predictions_map, cmap=\"gray\")\n","        del _predictions_map\n","        del __predictions_map\n","        gc.collect()\n","    \n","    del predictions_map\n","    del predictions_map_counts\n","    gc.collect()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Train/1\n","\n","- best thd = 0.47\n","- best stride = 128\n","\n","## Train/3\n","\n","- best thd = 0.495\n","- best stride = 128\n","\n","\n","## Conclusion\n","\n","- stride = 128 if possible"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 1-fold\n","\n","## stride=192, TTA\n","\n","### Train/1\n","\n","- thd=0.47: 0.5684219598770142\n","- thd=0.475: 0.5668601989746094\n","- thd=0.49: 0.5621239542961121\n","- thd=0.495: 0.5607137680053711\n","- thd=0.5: 0.5592559576034546\n","- thd=0.51: 0.5560135245323181\n","\n","\n","### Train/3\n","\n","- thd=0.475: 0.6189125180244446\n","- thd=0.49: 0.6188754439353943\n","- thd=0.495: 0.6188588738441467\n","- thd=0.5: 0.6188358664512634\n","- thd=0.51: 0.6186419725418091\n","\n","## stride=128, TTA\n","\n","### Train/1\n","\n","- thd=0.475: 0.5764451026916504\n","- thd=0.49: 0.5726862549781799\n","- thd=0.5: 0.5697850584983826\n","- thd=0.51: 0.5667483806610107\n","\n","### Train/3\n","\n","- thd=0.475: 0.6339520215988159\n","- thd=0.49: 0.6342535018920898\n","- thd=0.495: 0.6342722773551941\n","- thd=0.5: 0.6343271136283875\n","- thd=0.51: 0.6342480778694153"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Ensemble\n","\n","## stride=128, TTA\n","\n","### Train/1\n","\n","- thd=0.47: 0.9222993850708008\n","- thd=0.475: 0.9222012162208557\n","- thd=0.495: 0.9214496612548828\n","- thd=0.5: 0.9210970997810364 \n","\n","### Train/3\n","\n","- thd=0.475: \n","- thd=0.49: \n","- thd=0.495: \n","- thd=0.5: \n","- thd=0.51: "]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat":4,"nbformat_minor":4}
