{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Keras starter kit [full training set, UNet]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T09:04:23.702624Z","iopub.status.busy":"2023-05-08T09:04:23.702304Z","iopub.status.idle":"2023-05-08T09:04:32.404721Z","shell.execute_reply":"2023-05-08T09:04:32.403572Z","shell.execute_reply.started":"2023-05-08T09:04:23.702594Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","import glob\n","import time\n","import PIL.Image as Image\n","import matplotlib.pyplot as plt\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import Rectangle\n","import matplotlib.patches as patches\n","from tqdm import tqdm\n","\n","# Data config\n","DATA_DIR = '/kaggle/input/vesuvius-challenge-ink-detection/'\n","BUFFER = 128  # Half-size of papyrus patches we'll use as model inputs\n","Z_DIM = 4   # Number of slices in the z direction. Max value is 64 - Z_START\n","Z_START = 60  # Offset of slices in the z direction\n","SHARED_HEIGHT = 1600  # Height to resize all papyrii\n","\n","# (x, y)\n","val_location = (200, 800)\n","val_zone_size = (BUFFER * 2, BUFFER * 2)\n","\n","# Model config\n","BATCH_SIZE = 32\n","USE_MIXED_PRECISION = False\n","USE_JIT_COMPILE = False"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["\n","class UNet(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(UNet, self).__init__()\n","\n","        def conv_block(in_channels, out_channels):\n","            return nn.Sequential(\n","                nn.ReLU(),\n","                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(out_channels),\n","            )\n","\n","        def transpose_conv_block(in_channels, out_channels):\n","            return nn.Sequential(\n","                nn.ReLU(),\n","                nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(out_channels),\n","            )\n","\n","        self.encoder = nn.ModuleList([\n","            nn.Sequential(\n","                nn.Conv2d(in_channels if i == 1 else 64 * 2**(i - 1), 64 * 2**i, kernel_size=3, stride=2, padding=1),\n","                nn.BatchNorm2d(64 * 2**i),\n","                nn.ReLU(),\n","                nn.Conv2d(64 * 2**i, 64 * 2**i, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(64 * 2**i),\n","                nn.ReLU(),\n","            )\n","            for i in range(1, 3)\n","        ])\n","\n","\n","        self.middle = nn.Sequential(\n","            conv_block(256, 256),\n","            conv_block(256, 256),\n","        )\n","        \n","        self.decoder = nn.ModuleList([\n","            nn.Sequential(\n","                transpose_conv_block(2 ** (i + 7), 2 ** (i + 6)),\n","                transpose_conv_block(2 ** (i + 6), 2 ** (i + 5)),\n","                nn.Upsample(scale_factor=2, mode=\"nearest\"),\n","            )\n","            for i in range(2, 0, -1)\n","        ])\n","        self.final_decoder = nn.Sequential(\n","            nn.Conv2d(64, out_channels, kernel_size=3, padding=1),\n","        )\n","        self.activation = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        skip_connections = []\n","        for layer in self.encoder:\n","            x = layer(x)\n","            skip_connections.append(x)\n","\n","        x = self.middle(x)\n","        \n","        # print(\"encoder ok\", x.shape)\n","        for i, layer in enumerate(self.decoder):            \n","            # print(f\"decoder will {i}: ok\", x.shape)\n","            x = torch.cat([x, skip_connections[-i-1]], dim=1)  # Concatenate along channel dimension\n","            # print(f\"decoder with skip connection {i}: ok\", x.shape)            \n","            x = layer(x)            \n","            # print(f\"decoder {i}: ok\", x.shape)\n","        # print(\"decoder ok\")\n","        x = self.final_decoder(x)\n","        x = self.activation(x)\n","        return x"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["device = torch.device(\"cuda\")"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["model = UNet(Z_DIM, 1)\n","model = nn.DataParallel(model)\n","# model.load_state_dict(torch.load(f\"{DATA_DIR}/model.pt\"))\n","model = model.to(device)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Load up the training data"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T09:04:32.407446Z","iopub.status.busy":"2023-05-08T09:04:32.406726Z","iopub.status.idle":"2023-05-08T09:04:32.414915Z","shell.execute_reply":"2023-05-08T09:04:32.413627Z","shell.execute_reply.started":"2023-05-08T09:04:32.407412Z"},"trusted":true},"outputs":[],"source":["def resize(img):\n","    current_width, current_height = img.size\n","    aspect_ratio = current_width / current_height\n","    new_width = int(SHARED_HEIGHT * aspect_ratio)\n","    new_size = (new_width, SHARED_HEIGHT)\n","    img = img.resize(new_size)\n","    return img\n","\n","def load_mask(split, index):\n","    img = Image.open(f\"{DATA_DIR}/{split}/{index}/mask.png\").convert(\"1\")\n","    img = resize(img)\n","    return torch.tensor(np.array(img), dtype=torch.bool)\n","\n","\n","def load_labels(split, index):\n","    img = Image.open(f\"{DATA_DIR}/{split}/{index}/inklabels.png\").convert(\"1\")\n","    img = resize(img)\n","    return torch.tensor(np.array(img), dtype=torch.bool)\n","\n","def load_volume(split, index):\n","    # Load the 3d x-ray scan, one slice at a time\n","    z_slices_fnames = sorted(glob.glob(f\"{DATA_DIR}/{split}/{index}/surface_volume/*.tif\"))[Z_START:Z_START + Z_DIM]\n","    z_slices = []\n","    for z, filename in  tqdm(enumerate(z_slices_fnames)):\n","        img = Image.open(filename).convert(\"1\")\n","        img = resize(img)\n","        z_slice = np.array(img, dtype=\"float32\")\n","        z_slices.append(z_slice)\n","    return np.stack(z_slices, axis=-1)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T09:04:32.417450Z","iopub.status.busy":"2023-05-08T09:04:32.416496Z","iopub.status.idle":"2023-05-08T09:04:35.551992Z","shell.execute_reply":"2023-05-08T09:04:35.550830Z","shell.execute_reply.started":"2023-05-08T09:04:32.417412Z"},"trusted":true},"outputs":[],"source":["def extract_subvolume(location, volume):\n","    x = location[0]\n","    y = location[1]\n","    subvolume = volume[x-BUFFER:x+BUFFER, y-BUFFER:y+BUFFER, :]\n","    subvolume = torch.from_numpy(subvolume).float() / 65535.\n","    return subvolume\n","\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","from tqdm import tqdm\n","import pathlib\n","\n","def is_in_masked_zone(location, mask):\n","    return mask[location[0], location[1]]\n","\n","def compute_predictions_map(split, index):\n","    print(f\"Load data for {split}/{index}\")\n","\n","    test_volume = load_volume(split=split, index=index)\n","    test_mask = load_mask(split=split, index=index)\n","\n","    test_locations = []\n","    stride = BUFFER // 2\n","    for x in range(BUFFER, test_volume.shape[0] - BUFFER, stride):\n","        for y in range(BUFFER, test_volume.shape[1] - BUFFER, stride):\n","            test_locations.append((x, y))\n","\n","    print(f\"{len(test_locations)} test locations (before filtering by mask)\")\n","\n","    # filter locations inside the mask\n","    test_locations = [loc for loc in test_locations if is_in_masked_zone(loc, test_mask)]\n","\n","    class TestDataset(Dataset):\n","        def __init__(self, test_locations, test_volume):\n","            self.test_locations = test_locations\n","            self.test_volume = test_volume\n","\n","        def __len__(self):\n","            return len(self.test_locations)\n","\n","        def __getitem__(self, idx):\n","            location = torch.tensor(self.test_locations[idx])        \n","            subvolume = extract_subvolume(location, self.test_volume)\n","            subvolume = torch.permute(subvolume, (2, 1, 0))\n","            return location, subvolume\n","\n","    test_ds = TestDataset(test_locations, test_volume)\n","    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n","\n","    predictions_map = np.zeros(test_volume.shape[:2] + (1,), dtype=\"float16\")\n","    predictions_map_counts = np.zeros(test_volume.shape[:2] + (1,), dtype=\"int8\")\n","\n","    print(f\"Compute predictions\")\n","\n","    model.eval()  # set model to evaluation mode\n","    with torch.no_grad():\n","        for loc_batch, patch_batch in tqdm(test_loader):\n","            loc_batch = loc_batch.to(device)\n","            patch_batch = patch_batch.to(device)\n","            predictions = model(patch_batch)\n","            predictions = torch.permute(predictions, (0, 3, 2, 1))\n","            predictions = predictions.cpu().numpy()  # move predictions to cpu and convert to numpy\n","            for (x, y), pred in zip(loc_batch, predictions):\n","                predictions_map[x - BUFFER : x + BUFFER, y - BUFFER : y + BUFFER, :] += pred\n","                predictions_map_counts[x - BUFFER : x + BUFFER, y - BUFFER : y + BUFFER, :] += 1\n","    predictions_map /= (predictions_map_counts + 1e-7)\n","    return predictions_map\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["from skimage.transform import resize as resize_ski"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["def rle(predictions_map, threshold):\n","    flat_img = predictions_map.flatten()\n","    flat_img = np.where(flat_img > threshold, 1, 0).astype(np.uint8)\n","\n","    starts = np.array((flat_img[:-1] == 0) & (flat_img[1:] == 1))\n","    ends = np.array((flat_img[:-1] == 1) & (flat_img[1:] == 0))\n","    starts_ix = np.where(starts)[0] + 2\n","    ends_ix = np.where(ends)[0] + 2\n","    lengths = ends_ix - starts_ix\n","    return \" \".join(map(str, sum(zip(starts_ix, lengths), ())))"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/working/submission.csv'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m threshold \u001b[39m=\u001b[39m \u001b[39m0.01\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mId,Predicted\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, file\u001b[39m=\u001b[39m\u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39m/kaggle/working/submission.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_submission\u001b[39m(predictions_map, index):\n\u001b[1;32m      6\u001b[0m     rle_ \u001b[39m=\u001b[39m rle(predictions_map, threshold\u001b[39m=\u001b[39mthreshold)\n","File \u001b[0;32m~/anaconda3/envs/ink-detection/lib/python3.11/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/submission.csv'"]}],"source":["threshold = 0.01\n","\n","print(\"Id,Predicted\\n\", file=open('/kaggle/working/submission.csv', 'w'))\n","\n","def update_submission(predictions_map, index):\n","    rle_ = rle(predictions_map, threshold=threshold)\n","    print(f\"{index},\" + rle_ + \"\\n\", file=open('/kaggle/working/submission.csv', 'a'))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["folder = pathlib.Path(DATA_DIR) / \"test\"\n","for p in folder.iterdir():\n","    index = p.stem\n","    predictions_map = compute_predictions_map(split=\"test\", index=index)\n","    original_size = Image.open(DATA_DIR + f\"/test/{index}/mask.png\").size\n","    predictions_map = resize_ski(predictions_map, (original_size[1], original_size[0])).squeeze()\n","    update_submission(predictions_map, index)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":4}
