{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Keras starter kit [full training set, UNet]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import sys\n","\n","# sys.path.append('/kaggle/input/pretrainedmodels/pretrainedmodels-0.7.4')\n","# sys.path.append('/kaggle/input/segmentation-models-pytorch/segmentation_models.pytorch-master')\n","# sys.path.append('/kaggle/input/efficientnet-pytorch/EfficientNet-PyTorch-master')\n","# sys.path.append('/kaggle/input/timm-pytorch-image-models/pytorch-image-models-master')\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-10T15:03:39.741181Z","iopub.status.busy":"2023-05-10T15:03:39.740765Z","iopub.status.idle":"2023-05-10T15:03:39.750574Z","shell.execute_reply":"2023-05-10T15:03:39.749341Z","shell.execute_reply.started":"2023-05-10T15:03:39.741144Z"},"trusted":true},"outputs":[],"source":["\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torchvision\n","# import cupy\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","import pytorch_lightning\n","import segmentation_models_pytorch as smp\n","import pytorch_lightning as pl\n","import pytorch_lightning.plugins\n","from skimage.transform import resize as resize_ski\n","from pytorch_lightning.strategies.ddp import DDPStrategy\n","from pytorch_lightning.loggers import WandbLogger\n","import os\n","\n","\n","from scipy.ndimage import distance_transform_edt\n","\n","import ssl\n","ssl._create_default_https_context = ssl._create_unverified_context\n","\n","import glob\n","import time\n","import PIL.Image as Image\n","import matplotlib.pyplot as plt\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import Rectangle\n","import matplotlib.patches as patches\n","from sklearn.model_selection import KFold\n","from tqdm import tqdm\n","import cv2\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data\n","\n","\n","# Data config\n","# DATA_DIR = '/kaggle/input/vesuvius-challenge-ink-detection/'\n","# DATA_DIR = '/home/fummicc1/codes/competitions/kaggle-ink-detection'\n","DATA_DIR = '/home/fummicc1/codes/Kaggle/kaggle-ink-detection'\n","BUFFER = 80 # Half-size of papyrus patches we'll use as model inputs\n","# Z_LIST = list(range(0, 20, 5)) + list(range(22, 34))  # Offset of slices in the z direction\n","Z_LIST = list(range(32-6, 32+6))  # Offset of slices in the z direction\n","Z_DIM = len(Z_LIST)  # Number of slices in the z direction. Max value is 64 - Z_START\n","SHARED_HEIGHT = 4000  # Max height to resize all papyrii\n","\n","# Model config\n","BATCH_SIZE = 64\n","\n","# backbone = \"mit_b2\"\n","# backbone = \"efficientnet-b5\"\n","backbone = \"se_resnext101_32x4d\"\n","# backbone = \"resnext50_32x4d\"\n","# backbone = \"resnet50\"\n","\n","device = torch.device(\"cuda\")\n","threshold = 0.25\n","num_workers = 8\n","exp = 1e-7\n","mask_padding = 200\n","\n","num_epochs = 30\n","lr = 1e-3\n","\n","pytorch_lightning.seed_everything(seed=42)\n","torch.set_float32_matmul_precision('high')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.imshow(Image.open(DATA_DIR + \"/train/1/ir.png\"), cmap=\"gray\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# input shape: (H, W, C)\n","def rotate90(volume: np.ndarray, k=None, reverse=False):    \n","    if k:\n","        volume = np.rot90(volume, k)\n","    else:\n","        volume = np.rot90(volume, 1 if not reverse else 3)\n","    height = volume.shape[0]\n","    width = volume.shape[1]\n","    new_height = SHARED_HEIGHT\n","    new_width = int(new_height * width / height)\n","    if len(volume.shape) == 2:\n","        return cv2.resize(volume, (new_width, new_height))\n","    return resize_ski(volume, (new_height, new_width, volume.shape[2]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import cupy as cp\n","# xp = cp\n","\n","# delta_lookup = {\n","#     \"xx\": xp.array([[1, -2, 1]], dtype=float),\n","#     \"yy\": xp.array([[1], [-2], [1]], dtype=float),\n","#     \"xy\": xp.array([[1, -1], [-1, 1]], dtype=float),\n","# }\n","\n","# def operate_derivative(img_shape, pair):\n","#     assert len(img_shape) == 2\n","#     delta = delta_lookup[pair]\n","#     fft = xp.fft.fftn(delta, img_shape)\n","#     return fft * xp.conj(fft)\n","\n","# def soft_threshold(vector, threshold):\n","#     return xp.sign(vector) * xp.maximum(xp.abs(vector) - threshold, 0)\n","\n","# def back_diff(input_image, dim):\n","#     assert dim in (0, 1)\n","#     r, n = xp.shape(input_image)\n","#     size = xp.array((r, n))\n","#     position = xp.zeros(2, dtype=int)\n","#     temp1 = xp.zeros((r+1, n+1), dtype=float)\n","#     temp2 = xp.zeros((r+1, n+1), dtype=float)\n","    \n","#     temp1[position[0]:size[0], position[1]:size[1]] = input_image\n","#     temp2[position[0]:size[0], position[1]:size[1]] = input_image\n","    \n","#     size[dim] += 1\n","#     position[dim] += 1\n","#     temp2[position[0]:size[0], position[1]:size[1]] = input_image\n","#     temp1 -= temp2\n","#     size[dim] -= 1\n","#     return temp1[0:size[0], 0:size[1]]\n","\n","# def forward_diff(input_image, dim):\n","#     assert dim in (0, 1)\n","#     r, n = xp.shape(input_image)\n","#     size = xp.array((r, n))\n","#     position = xp.zeros(2, dtype=int)\n","#     temp1 = xp.zeros((r+1, n+1), dtype=float)\n","#     temp2 = xp.zeros((r+1, n+1), dtype=float)\n","        \n","#     size[dim] += 1\n","#     position[dim] += 1\n","\n","#     temp1[position[0]:size[0], position[1]:size[1]] = input_image\n","#     temp2[position[0]:size[0], position[1]:size[1]] = input_image\n","    \n","#     size[dim] -= 1\n","#     temp2[0:size[0], 0:size[1]] = input_image\n","#     temp1 -= temp2\n","#     size[dim] += 1\n","#     return -temp1[position[0]:size[0], position[1]:size[1]]\n","\n","# def iter_deriv(input_image, b, scale, mu, dim1, dim2):\n","#     g = back_diff(forward_diff(input_image, dim1), dim2)\n","#     d = soft_threshold(g + b, 1 / mu)\n","#     b = b + (g - d)\n","#     L = scale * back_diff(forward_diff(d - b, dim2), dim1)\n","#     return L, b\n","\n","# def iter_xx(*args):\n","#     return iter_deriv(*args, dim1=1, dim2=1)\n","\n","# def iter_yy(*args):\n","#     return iter_deriv(*args, dim1=0, dim2=0)\n","\n","# def iter_xy(*args):\n","#     return iter_deriv(*args, dim1=0, dim2=1)\n","\n","# def iter_sparse(input_image, bsparse, scale, mu):\n","#     d = soft_threshold(input_image + bsparse, 1 / mu)\n","#     bsparse = bsparse + (input_image - d)\n","#     Lsparse = scale * (d - bsparse)\n","#     return Lsparse, bsparse\n","\n","\n","# def denoise_image(input_image, iter_num=100, fidelity=150, sparsity_scale=10, continuity_scale=0.5, mu=1):\n","#     image_size = xp.shape(input_image)\n","#     #print(\"Initialize denoising\")\n","#     print(\"input_size\", input_image.shape)\n","#     print(\"image_size\", image_size)\n","#     norm_array = (\n","#         operate_derivative(image_size, \"xx\") + \n","#         operate_derivative(image_size, \"yy\") + \n","#         2 * operate_derivative(image_size, \"xy\")\n","#     )\n","#     norm_array += (fidelity / mu) + sparsity_scale ** 2\n","#     b_arrays = {\n","#         \"xx\": xp.zeros(image_size, dtype=float),\n","#         \"yy\": xp.zeros(image_size, dtype=float),\n","#         \"xy\": xp.zeros(image_size, dtype=float),\n","#         \"L1\": xp.zeros(image_size, dtype=float),\n","#     }\n","#     g_update = xp.multiply(fidelity / mu, input_image)\n","#     for i in tqdm(range(iter_num), total=iter_num):\n","#         #print(f\"Starting iteration {i+1}\")\n","#         g_update = xp.fft.fftn(g_update)\n","#         if i == 0:\n","#             g = xp.fft.ifftn(g_update / (fidelity / mu)).real\n","#         else:\n","#             g = xp.fft.ifftn(xp.divide(g_update, norm_array)).real\n","#         g_update = xp.multiply((fidelity / mu), input_image)\n","        \n","#         #print(\"XX update\")\n","#         L, b_arrays[\"xx\"] = iter_xx(g, b_arrays[\"xx\"], continuity_scale, mu)\n","#         g_update += L\n","        \n","#         #print(\"YY update\")\n","#         L, b_arrays[\"yy\"] = iter_yy(g, b_arrays[\"yy\"], continuity_scale, mu)\n","#         g_update += L\n","        \n","#         #print(\"XY update\")\n","#         L, b_arrays[\"xy\"] = iter_xy(g, b_arrays[\"xy\"], 2 * continuity_scale, mu)\n","#         g_update += L\n","        \n","#         #print(\"L1 update\")\n","#         L, b_arrays[\"L1\"] = iter_sparse(g, b_arrays[\"L1\"], sparsity_scale, mu)\n","#         g_update += L\n","        \n","#     g_update = xp.fft.fftn(g_update)\n","#     g = xp.fft.ifftn(xp.divide(g_update, norm_array)).real\n","    \n","#     g[g < 0] = 0\n","#     g -= g.min()\n","#     g /= g.max()\n","#     return g"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def is_in_masked_zone(location, mask):\n","    return mask[location[0], location[1]] > 0\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def resize(img):\n","    current_height, current_width = img.shape    \n","    aspect_ratio = current_width / current_height\n","    new_height = SHARED_HEIGHT\n","    new_width = int(SHARED_HEIGHT * aspect_ratio)\n","    new_size = (new_width, new_height)\n","    # (W, H)の順で渡すが結果は(H, W)になっている\n","    img = cv2.resize(img, new_size)\n","    return img\n","\n","def load_mask(split, index):\n","    img = cv2.imread(f\"{DATA_DIR}/{split}/{index}/mask.png\", 0) // 255\n","    img = np.pad(img, 1, constant_values=0)\n","    dist = distance_transform_edt(img)\n","    img[dist <= mask_padding] = 0\n","    img = img[1:-1, 1:-1]\n","    img = resize(img)    \n","    return img\n","\n","\n","def load_labels(split, index):\n","    img = cv2.imread(f\"{DATA_DIR}/{split}/{index}/inklabels.png\", 0) // 255\n","    img = resize(img)\n","    return img"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def load_volume(split, index):\n","    # Load the 3d x-ray scan, one slice at a time\n","    all = sorted(glob.glob(f\"{DATA_DIR}/{split}/{index}/surface_volume/*.tif\"))\n","    z_slices_fnames = [all[i] for i in range(len(all)) if i in Z_LIST]\n","    assert len(z_slices_fnames) == Z_DIM\n","    z_slices = []\n","    for z, filename in  tqdm(enumerate(z_slices_fnames)):\n","        img = cv2.imread(filename, -1)\n","        img = resize(img)\n","        # img = (img / (2 ** 8)).astype(np.uint8)\n","        img = img.astype(np.float32) // 255\n","        z_slices.append(img)\n","    return np.stack(z_slices, axis=-1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["static_all_median = np.array([\n","    88., 88., 88., 86., 82., 78., 74., 71., 69., 69., 69., 71.\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["static_all_MAD = np.array([\n","    55., 56., 57., 58., 58., 57., 53., 47., 40., 33., 30., 26.\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["all_median = static_all_median\n","all_MAD = static_all_MAD"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["printed = True\n","\n","def extract_subvolume(location, volume):\n","    global printed\n","    # print(np.unique(volume, return_counts=True, return_index=True))\n","    x = location[0]\n","    y = location[1]\n","    subvolume = volume[x-BUFFER:x+BUFFER, y-BUFFER:y+BUFFER, :].astype(np.float32)\n","    # print(\"subvolume[:, :, 0]\", subvolume[:, :, 0])\n","    # median = np.full_like(subvolume, all_median).astype(np.float32)\n","    # MAD = np.full_like(subvolume, all_MAD).astype(np.float32)\n","    # mean = np.mean(subvolume, axis=2)\n","    # mean = np.stack([mean for i in range(Z_DIM)], axis=2) + exp\n","    # MAD = median_abs_deviation(subvolume, axis=2)\n","    # print(\"MAD\", MAD[0, 0, :])\n","    # print(\"mean\", mean)\n","    # print(\"median\", median[0, 0, :])\n","    \n","    # subvolume = (subvolume - median) / MAD\n","    # subvolume = subvolume / median\n","    \n","    if not printed:\n","        print(\"subvolume after taking care of median and MAD\", subvolume)\n","        printed = True\n","    \n","    return subvolume"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def TTA(x: torch.Tensor, model: nn.Module):\n","    # x.shape=(batch,c,h,w)\n","    shape = x.shape\n","    x = [x, *[torch.rot90(x, k=i, dims=(-2, -1)) for i in range(1, 4)]]\n","    x = torch.cat(x, dim=0)\n","    x = model(x)\n","    x = torch.sigmoid(x)\n","    x = x.reshape(4, shape[0], 1, *shape[2:])\n","    x = [torch.rot90(x[i], k=-i, dims=(-2, -1)) for i in range(4)]\n","    x = torch.stack(x, dim=0)\n","    return x.mean(0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.preprocessing import OneHotEncoder\n","\n","from albumentations.core.transforms_interface import ImageOnlyTransform\n","\n","class NormalizeTransform(ImageOnlyTransform):\n","    def __init__(self, always_apply=False, p=1.0):\n","        super(NormalizeTransform, self).__init__(always_apply, p)\n","\n","    def apply(self, img, **params):\n","        median = np.full_like(img, all_median).astype(np.float32)\n","        mad = np.full_like(img, all_MAD).astype(np.float32)\n","        img = (img - median) / mad\n","        img = img / median\n","        # img = img / 255\n","        # img = (img - 0.45)/0.225\n","        # img[img < 0] = 0\n","        return img\n","\n","\n","class SubvolumeDataset(Dataset):\n","    def __init__(self, locations, volume, labels, buffer, is_train: bool, return_location: bool = False):\n","        self.locations = locations\n","        self.volume = volume\n","        self.labels = labels        \n","        self.buffer = buffer\n","        self.is_train = is_train\n","        self.return_location = return_location\n","\n","    def __len__(self):\n","        return len(self.locations)\n","\n","    def __getitem__(self, idx):\n","        global possible_min_input, possible_max_input\n","        label = None\n","        location = np.array(self.locations[idx])\n","        y, x = location[0], location[1]\n","\n","        subvolume = extract_subvolume(location, self.volume)\n","        # print(\"subvolume\", subvolume)\n","        # print(\"labels\", labels)\n","        # subvolume = subvolume.numpy()\n","        subvolume = subvolume\n","        \n","        if self.labels is not None:\n","            label = self.labels[y - self.buffer:y + self.buffer, x - self.buffer:x + self.buffer]\n","            # print(\"label\", label)\n","            # n_category = 2\n","            # label = np.eye(n_category)[label]\n","            label = np.stack([label], axis=-1)\n","            # label = label.numpy()\n","            # print(\"label.shape\", label.shape\n","        \n","        if self.is_train and label is not None:            \n","            \n","            # print(\"label\", label.dtype)\n","            # print(\"subvolume in train dataset (before aug)\", subvolume, file=open(\"before-train-aug.log\", \"w\")) \n","            size = int(BUFFER * 2)\n","            performed = A.Compose([\n","                # A.ToFloat(max_value=possible_max_input - possible_min_input),\n","                # A.ToFloat(max_value=2**16-1),                       \n","                A.HorizontalFlip(p=0.5), # 水平方向に反転\n","                A.VerticalFlip(p=0.5), # 水平方向に反転\n","                A.RandomRotate90(p=0.5),\n","                # A.RandomBrightnessContrast(p=0.4),\n","                A.ShiftScaleRotate(p=0.5, border_mode=0), # シフト、スケーリング、回転\n","                # A.PadIfNeeded(min_height=size, min_width=size, always_apply=True, border_mode=0), # 必要に応じてパディングを追加\n","                A.RandomCrop(height=int(size / 1.25), width=int(size / 1.25), p=0.5), # ランダムにクロップ, Moduleの中で計算する際に次元がバッチ内で揃っている必要があるので最後にサイズは揃える\n","                # A.Perspective(p=0.5), # パースペクティブ変換                \n","                # A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n","                # A.CoarseDropout(max_holes=1, max_width=int(size * 0.3), max_height=int(size * 0.3), \n","                #                 mask_fill_value=0, p=0.2),\n","                A.OneOf([\n","                    # A.GaussNoise(var_limit=[1, 1]), # NG\n","                    A.GaussianBlur(blur_limit=(3, 5), p=1),\n","                    A.MotionBlur(blur_limit=5, p=1),\n","                ], p=0.4),\n","                A.Resize(BUFFER * 2, BUFFER * 2, always_apply=True),        \n","                NormalizeTransform(always_apply=True),        \n","                # A.Normalize(\n","                #     mean= [0] * Z_DIM,\n","                #     std= [1] * Z_DIM\n","                # ),\n","                # A.FromFloat(max_value=possible_max_input - possible_min_input),                \n","                ToTensorV2(transpose_mask=True),                \n","            ])(image=subvolume, mask=label)            \n","            subvolume = performed[\"image\"]\n","            label = performed[\"mask\"]\n","            # print(\"subvolume in train dataset (after aug)\", subvolume, file=open(\"after-train-aug.log\", \"w\"))\n","            # print(\"label\", label.dtype)\n","            # print(\"subvolume\", subvolume.dtype)\n","            # →C, H, W\n","            # subvolume = torch.from_numpy(subvolume.transpose(2, 0, 1).astype(np.float32))\n","            # print(performed)\n","            # print(subvolume.shape, label.shape)\n","            # H, W, C → C, H, W\n","            # label = torch.from_numpy(label.transpose(2, 0, 1).astype(np.uint8)) \n","        else:\n","            if label is None:\n","                performed = A.Compose([            \n","                    # A.ToFloat(max_value=possible_max_input - possible_min_input),\n","                    # A.ToFloat(max_value=2**16-1),\n","                    # A.Normalize(\n","                    #     mean= [0] * Z_DIM,\n","                    #     std= [1] * Z_DIM\n","                    # ),\n","                    # A.FromFloat(max_value=possible_max_input - possible_min_input),\n","                    NormalizeTransform(always_apply=True),\n","                    ToTensorV2(transpose_mask=True),\n","                ])(image=subvolume)\n","                subvolume = performed[\"image\"]\n","            else:\n","                # print(\"subvolume in val dataset (before aug)\", subvolume, file=open(\"before-val-aug.log\", \"w\")) \n","                performed = A.Compose([            \n","                    # A.ToFloat(max_value=possible_max_input - possible_min_input),\n","                    # A.ToFloat(max_value=2**16-1),\n","                    # A.Normalize(\n","                    #     mean= [0] * Z_DIM,\n","                    #     std= [1] * Z_DIM\n","                    # ),\n","                    # A.FromFloat(max_value=possible_max_input - possible_min_input),\n","                    NormalizeTransform(always_apply=True),\n","                    ToTensorV2(transpose_mask=True),\n","                ])(image=subvolume, mask=label)                \n","                label = performed[\"mask\"]                \n","                subvolume = performed[\"image\"]\n","                # print(\"subvolume in val dataset (after aug)\", subvolume, file=open(\"after-val-aug.log\", \"w\"))                \n","            # subvolume = torch.from_numpy(subvolume.transpose(2, 0, 1).astype(np.float32))\n","            # if label is not None:\n","                # label = torch.from_numpy(label.transpose(2, 0, 1).astype(np.uint8)) \n","        if self.return_location:\n","            return subvolume, location\n","        return subvolume, label        "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def dice_coef_torch(prob_preds, targets, beta=0.5, smooth=1e-5):\n","    # No need to binarize the predictions\n","    # prob_preds = torch.sigmoid(preds)\n","\n","    # flatten label and prediction tensors\n","    prob_preds = prob_preds.view(-1).float()\n","    targets = targets.view(-1).float()\n","\n","    intersection = (prob_preds * targets).sum()\n","\n","    dice = ((1 + beta**2) * intersection + smooth) / ((beta**2) * prob_preds.sum() + targets.sum() + smooth)\n","\n","    return dice\n","\n","\n","\n","class Model(pl.LightningModule):\n","    training_step_outputs = []\n","    validation_step_outputs = []\n","    test_step_outputs = [[], []]\n","\n","    def __init__(self, encoder_name, in_channels, out_classes, **kwargs):\n","        super().__init__()\n","\n","        self.model = smp.Unet(\n","            encoder_name=encoder_name,\n","            encoder_weights=\"imagenet\",\n","            # encoder_weights=None,\n","            encoder_depth=5,\n","            decoder_channels=[512, 256, 128, 64, 32],\n","            in_channels=in_channels,\n","            classes=out_classes,\n","            # aux_params={\n","            #     \"pooling\": \"max\",\n","            #     \"classes\": out_classes,\n","            #     \"dropout\": 0.2,\n","            #     \"activation\": None,\n","            # },\n","            **kwargs,\n","        )\n","\n","        # preprocessing parameteres for image\n","        # params = smp.encoders.get_preprocessing_params(encoder_name)\n","        # self.register_buffer(\"std\", torch.tensor(params[\"std\"]).view(1, 3, 1, 1))\n","        # self.register_buffer(\"mean\", torch.tensor(params[\"mean\"]).view(1, 3, 1, 1))\n","\n","        # for image segmentation dice loss could be the best first choice\n","        # self.segmentation_loss_fn = smp.losses.TverskyLoss(\n","        #     smp.losses.BINARY_MODE,\n","        #     log_loss=False,\n","        #     from_logits=True,\n","        #     smooth=1e-6,\n","        # )\n","        # smp.losses.FocalLoss()\n","        self.segmentation_loss_fn = smp.losses.DiceLoss(\n","            smp.losses.BINARY_MODE, log_loss=False, from_logits=True, smooth=1e-6\n","        )\n","        # self.segmentation_loss_fn = dice_coef_torch\n","        # self.classification_loss_fn = smp.losses.SoftCrossEntropyLoss()\n","\n","    def forward(self, image, stage):\n","        # normalize image here\n","        # image = (image - self.mean) / self.std\n","        if stage != \"train\":\n","            mask = TTA(image, self.model)\n","        else:\n","            mask = self.model(image)\n","        return mask\n","\n","    def shared_step(self, batch, stage):\n","        subvolumes, labels = batch\n","\n","        image, labels = subvolumes.float(), labels.float()\n","        # print(\"torch.unique(subvolumes)\", torch.unique(subvolumes), file=open(\"subvolumes_unique\", \"w\"))\n","\n","        # Shape of the image should be (batch_size, num_channels, height, width)\n","        # if you work with grayscale images, expand channels dim to have [batch_size, 1, height, width]\n","        assert image.ndim == 4\n","\n","        # Check that image dimensions are divisible by 32,\n","        # encoder and decoder connected by `skip connections` and usually encoder have 5 stages of\n","        # downsampling by factor 2 (2 ^ 5 = 32); e.g. if we have image with shape 65x65 we will have\n","        # following shapes of features in encoder and decoder: 84, 42, 21, 10, 5 -> 5, 10, 20, 40, 80\n","        # and we will get an error trying to concat these features\n","        h, w = image.shape[2:]\n","        assert h % 32 == 0 and w % 32 == 0\n","\n","        # Shape of the mask should be [batch_size, num_classes, height, width]\n","        # for binary segmentation num_classes = 1\n","        assert labels.ndim == 4\n","\n","        # Check that mask values in between 0 and 1, NOT 0 and 255 for binary segmentation\n","        assert labels.max() <= 1.0 and labels.min() >= 0\n","\n","        segmentation_out = self.forward(image, stage)\n","        # if len(segmentation_out.shape) == 3:\n","        #     segmentation_out = segmentation_out.unsqueeze(dim=1)\n","        # print(\"model out shape\", segmentation_out.shape)\n","        if stage == \"train\":\n","            segmentation_out = segmentation_out.sigmoid()\n","        # print(\"model out shape after sigmoid\", segmentation_out.shape)\n","        # print(\"label shape\", labels.shape)\n","\n","        # Predicted mask contains logits, and loss_fn param `from_logits` is set to True\n","        loss = self.segmentation_loss_fn(segmentation_out, labels)\n","\n","        # Lets compute metrics for some threshold\n","        # first convert mask values to probabilities, then\n","        # apply thresholding\n","        prob_mask = segmentation_out\n","        pred_mask = (prob_mask > threshold).float()\n","\n","        # We will compute IoU metric by two ways\n","        #   1. dataset-wise\n","        #   2. image-wise\n","        # but for now we just compute true positive, false positive, false negative and\n","        # true negative 'pixels' for each image and class\n","        # these values will be aggregated in the end of an epoch\n","        tp, fp, fn, tn = smp.metrics.get_stats(\n","            pred_mask.long(), labels.long(), mode=\"binary\"\n","        )\n","\n","        return {\n","            \"loss\": loss,\n","            \"tp\": tp,\n","            \"fp\": fp,\n","            \"fn\": fn,\n","            \"tn\": tn,\n","        }\n","\n","    def shared_epoch_end(self, outputs, stage):\n","        # aggregate step metics\n","        tp = torch.cat([x[\"tp\"] for x in outputs])\n","        fp = torch.cat([x[\"fp\"] for x in outputs])\n","        fn = torch.cat([x[\"fn\"] for x in outputs])\n","        tn = torch.cat([x[\"tn\"] for x in outputs])\n","        loss = torch.mean(torch.Tensor([x[\"loss\"] for x in outputs]))\n","\n","        # per image IoU means that we first calculate IoU score for each image\n","        # and then compute mean over these scores\n","        per_image_iou = smp.metrics.iou_score(\n","            tp, fp, fn, tn, reduction=\"micro-imagewise\"\n","        )\n","\n","        # dataset IoU means that we aggregate intersection and union over whole dataset\n","        # and then compute IoU score. The difference between dataset_iou and per_image_iou scores\n","        # in this particular case will not be much, however for dataset\n","        # with \"empty\" images (images without target class) a large gap could be observed.\n","        # Empty images influence a lot on per_image_iou and much less on dataset_iou.\n","        dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n","\n","        metrics = {\n","            f\"{stage}_per_image_iou\": per_image_iou,\n","            f\"{stage}_dataset_iou\": dataset_iou,\n","            f\"{stage}_loss\": loss,\n","            f\"{stage}_tp\": tp.sum().int().item(),\n","            f\"{stage}_fp\": fp.sum().int().item(),\n","            f\"{stage}_fn\": fn.sum().int().item(),\n","            f\"{stage}_tn\": tn.sum().int().item(),\n","        }\n","\n","        self.log_dict(metrics, prog_bar=True, sync_dist=True)\n","\n","    def training_step(self, batch, batch_idx):\n","        out = self.shared_step(batch, \"train\")\n","        self.training_step_outputs.append(out)\n","        return out\n","\n","    def on_train_epoch_end(self):\n","        out = self.shared_epoch_end(self.training_step_outputs, \"train\")\n","        self.training_step_outputs.clear()\n","        return out\n","\n","    def validation_step(self, batch, batch_idx):\n","        out = self.shared_step(batch, \"valid\")\n","        self.validation_step_outputs.append(out)\n","        return out\n","\n","    def on_validation_epoch_end(self):\n","        out = self.shared_epoch_end(self.validation_step_outputs, \"valid\")\n","        self.validation_step_outputs.clear()\n","        return out\n","\n","    def test_step(self, batch, batch_idx):\n","        global predictions_map, predictions_map_counts\n","\n","        patch_batch, loc_batch = batch\n","\n","        loc_batch = loc_batch.long()\n","        patch_batch = patch_batch.float()\n","        predictions: torch.Tensor = self.forward(patch_batch, \"test\")\n","        # print(\"predictions.shape\", predictions.shape)\n","        # print(\"predictions\", predictions)\n","        # predictions = predictions.sigmoid()\n","        # print(\"Softmaxed predictions where conf is gt threshold\", predictions[predictions.gt(threshold)])\n","        # print(\"predictions.shape after sigmoid\", predictions.shape)\n","        # →(BATCH, W, H, C)\n","        predictions = torch.permute(predictions, (0, 3, 2, 1)).squeeze(dim=-1)\n","        predictions = (\n","            predictions.cpu().numpy()\n","        )  # move predictions to cpu and convert to numpy\n","        loc_batch = loc_batch.cpu().numpy()\n","        # print(\"predictions_map\", predictions_map)\n","        # print(\"predictions_map_count\", predictions_map_counts)\n","        self.test_step_outputs[0].extend(loc_batch)\n","        self.test_step_outputs[1].extend(predictions)\n","        return loc_batch, predictions\n","\n","    def on_test_epoch_end(self):\n","        global predictions_map, predictions_map_counts\n","\n","        locs = np.array(self.test_step_outputs[0])\n","        preds = np.array(self.test_step_outputs[1])\n","        print(\"locs\", locs.shape)\n","        print(\"preds\", preds.shape)\n","\n","        new_predictions_map = np.zeros_like(predictions_map[:, :, 0])\n","        new_predictions_map_counts = np.zeros_like(predictions_map_counts[:, :, 0])\n","\n","        for (y, x), pred in zip(locs, preds):\n","            new_predictions_map[\n","                y - BUFFER : y + BUFFER, x - BUFFER : x + BUFFER\n","            ] += pred\n","            new_predictions_map_counts[\n","                y - BUFFER : y + BUFFER, x - BUFFER : x + BUFFER\n","            ] += 1\n","        # print(\"new_predictions_map\", new_predictions_map.shape)\n","        # print(\"predictions_map\", predictions_map.shape)\n","        new_predictions_map /= new_predictions_map_counts + exp\n","        new_predictions_map = new_predictions_map[:, :, np.newaxis]\n","        predictions_map = np.concatenate(\n","            [predictions_map, new_predictions_map], axis=-1\n","        )        \n","\n","    def configure_optimizers(self):\n","        optimizer = optim.Adam(self.parameters(), lr=lr)\n","        # Using a scheduler is optional but can be helpful.\n","        # The scheduler reduces the LR if the validation performance hasn't improved for the last N epochs\n","        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","            optimizer, mode=\"max\", factor=0.1, patience=8, min_lr=5e-5\n","        )\n","        return {\n","            \"optimizer\": optimizer,\n","            \"lr_scheduler\": {\"scheduler\": scheduler, \"monitor\": \"valid_tp\"},\n","        }\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class EnsembleModel:\n","    def __init__(self, test_loader, test_volume):\n","        super().__init__()\n","        self.test_loader = test_loader\n","        self.test_volume = test_volume\n","        self.list = []\n","        for fold in [1, 2, 3]:\n","            _model = Model.load_from_checkpoint(\n","                f\"weights/weights_fold-{fold}.ckpt\",\n","                # f\"/kaggle/input/first-ink-detection/weights_fold-{fold}.ckpt\",\n","                encoder_name=backbone,\n","                in_channels=Z_DIM,\n","                out_classes=1,\n","            )\n","            trainer = pl.Trainer(\n","                accelerator=\"gpu\",\n","                devices=\"1\",\n","                max_epochs=num_epochs,\n","            )\n","\n","            self.list.append((_model, trainer))\n","    \n","    def forward(self):        \n","        global predictions_map, predictions_map_counts, all_median\n","        predictions_map = np.empty_like(self.test_volume[:, :, 0])[:, :, np.newaxis].astype(np.float64)\n","        predictions_map_counts = np.empty_like(predictions_map).astype(np.uint8)\n","        for i, (model, trainer) in enumerate(self.list):\n","            all_median = static_all_median[i]\n","            model.test_step_outputs = [[], []]\n","            # shape: (X, Y, C)            \n","            model.eval()\n","            trainer.test(\n","                model=model,\n","                dataloaders=self.test_loader,\n","                verbose=True,\n","            )\n","        predictions_map = predictions_map[:, :, 1:].mean(axis=-1, keepdims=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","from tqdm import tqdm\n","from skimage.transform import resize as resize_ski\n","import pathlib\n","import gc\n","\n","predictions_map = None\n","predictions_map_counts = None\n","\n","def compute_predictions_map(split, index):\n","    global predictions_map\n","    global predictions_map_counts\n","    \n","    print(f\"Load data for {split}/{index}\")\n","\n","    test_volume = load_volume(split=split, index=index)\n","    test_mask = load_mask(split=split, index=index)\n","\n","    test_locations = []\n","    stride = BUFFER // 2\n","    for y in range(BUFFER, test_volume.shape[0] - BUFFER, stride):\n","        for x in range(BUFFER, test_volume.shape[1] - BUFFER, stride):\n","            test_locations.append((y, x))\n","\n","    print(f\"{len(test_locations)} test locations (before filtering by mask)\")\n","\n","    # filter locations inside the mask\n","    test_locations = [loc for loc in test_locations if is_in_masked_zone(loc, test_mask)]\n","    \n","    print(f\"{len(test_locations)} test locations (after filtering by mask)\")\n","\n","    test_ds = SubvolumeDataset(test_locations, test_volume, None, BUFFER, is_train=False, return_location=True)\n","    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, num_workers=num_workers)        \n","\n","    # # shape: (X, Y, C)\n","    predictions_map = np.zeros_like(test_volume[:, :, 0]).transpose((1, 0))[:, :, np.newaxis].astype(np.float64)\n","    predictions_map_counts = np.zeros_like(predictions_map).astype(np.uint8)\n","\n","    # print(\"test_volume.shape\", test_volume.shape)\n","    # print(\"predictions_map.shape\", predictions_map.shape)\n","\n","    # print(f\"Compute predictions\")\n","    \n","    model = EnsembleModel(test_loader, test_volume)\n","    model.forward()\n","    del model\n","    del test_locations\n","    del test_loader\n","    del test_ds\n","    del test_volume\n","    del test_mask\n","    gc.collect()\n","    \n","    # print(\"predictions_map\", predictions_map, file=open(\"predictions_map\", \"w\"))\n","    return predictions_map\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fast run length encoding, from https://www.kaggle.com/code/hackerpoet/even-faster-run-length-encoder/script\n","# (H, W)の形式\n","def rle (img: np.ndarray, threshold: float):\n","    flat_img = img.flatten()\n","    flat_img = np.where(flat_img > threshold, 1, 0).astype(np.uint8)\n","\n","    starts = np.array((flat_img[:-1] == 0) & (flat_img[1:] == 1))\n","    ends = np.array((flat_img[:-1] == 1) & (flat_img[1:] == 0))\n","    starts_ix = np.where(starts)[0] + 2\n","    ends_ix = np.where(ends)[0] + 2\n","    lengths = ends_ix - starts_ix\n","\n","    return \" \".join(map(str, sum(zip(starts_ix, lengths), ())))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def update_submission(predictions_map, index):\n","    rle_ = rle(predictions_map, threshold=threshold)\n","    print(f\"{index},\" + rle_, file=open('submission.csv', 'a'))\n","    # print(f\"{index},\" + rle_, file=open('/kaggle/working/submission.csv', 'a'))"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/home/fummicc1/codes/Kaggle/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.50/home/fummicc1/codes/Kaggle/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(folder\u001b[39m.\u001b[39miterdir()):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.50/home/fummicc1/codes/Kaggle/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     index \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39mstem\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.50/home/fummicc1/codes/Kaggle/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     predictions_map \u001b[39m=\u001b[39m compute_predictions_map(split\u001b[39m=\u001b[39;49mkind, index\u001b[39m=\u001b[39;49mindex)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.50/home/fummicc1/codes/Kaggle/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     original_size \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(DATA_DIR \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mkind\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mindex\u001b[39m}\u001b[39;00m\u001b[39m/mask.png\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mshape[:\u001b[39m2\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.50/home/fummicc1/codes/Kaggle/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     predictions_map \u001b[39m=\u001b[39m resize_ski(predictions_map, (original_size[\u001b[39m0\u001b[39m], original_size[\u001b[39m1\u001b[39m], \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39msqueeze(axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)    \n","\u001b[1;32m/home/fummicc1/codes/Kaggle/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb Cell 22\u001b[0m in \u001b[0;36mcompute_predictions_map\u001b[0;34m(split, index)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.50/home/fummicc1/codes/Kaggle/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m predictions_map_counts \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros_like(predictions_map)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39muint8)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.50/home/fummicc1/codes/Kaggle/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# print(\"test_volume.shape\", test_volume.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.50/home/fummicc1/codes/Kaggle/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# print(\"predictions_map.shape\", predictions_map.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.50/home/fummicc1/codes/Kaggle/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.50/home/fummicc1/codes/Kaggle/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m# print(f\"Compute predictions\")\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.50/home/fummicc1/codes/Kaggle/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m model \u001b[39m=\u001b[39m EnsembleModel(test_loader, test_volume)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.50/home/fummicc1/codes/Kaggle/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m model\u001b[39m.\u001b[39mforward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.50/home/fummicc1/codes/Kaggle/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mdel\u001b[39;00m model\n","\u001b[1;32m/home/fummicc1/codes/Kaggle/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb Cell 22\u001b[0m in \u001b[0;36mEnsembleModel.__init__\u001b[0;34m(self, test_loader, test_volume)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.50/home/fummicc1/codes/Kaggle/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlist \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.50/home/fummicc1/codes/Kaggle/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m fold \u001b[39min\u001b[39;00m [\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m]:\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.50/home/fummicc1/codes/Kaggle/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     _model \u001b[39m=\u001b[39m Model\u001b[39m.\u001b[39;49mload_from_checkpoint(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.50/home/fummicc1/codes/Kaggle/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m         \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mweights/weights_fold-\u001b[39;49m\u001b[39m{\u001b[39;49;00mfold\u001b[39m}\u001b[39;49;00m\u001b[39m.ckpt\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.50/home/fummicc1/codes/Kaggle/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m         \u001b[39m# f\"/kaggle/input/first-ink-detection/weights_fold-{fold}.ckpt\",\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.50/home/fummicc1/codes/Kaggle/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m         encoder_name\u001b[39m=\u001b[39;49mbackbone,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.50/home/fummicc1/codes/Kaggle/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m         in_channels\u001b[39m=\u001b[39;49mZ_DIM,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.50/home/fummicc1/codes/Kaggle/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m         out_classes\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.50/home/fummicc1/codes/Kaggle/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.50/home/fummicc1/codes/Kaggle/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.50/home/fummicc1/codes/Kaggle/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m         accelerator\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.50/home/fummicc1/codes/Kaggle/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m         devices\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.50/home/fummicc1/codes/Kaggle/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m         max_epochs\u001b[39m=\u001b[39mnum_epochs,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.50/home/fummicc1/codes/Kaggle/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B192.168.60.50/home/fummicc1/codes/Kaggle/kaggle-ink-detection/keras-starter-kit-unet-train-on-full-dataset_submission.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlist\u001b[39m.\u001b[39mappend((_model, trainer))\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/pytorch_lightning/core/module.py:1531\u001b[0m, in \u001b[0;36mLightningModule.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m   1452\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_from_checkpoint\u001b[39m(\n\u001b[1;32m   1453\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1458\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1459\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Self:\n\u001b[1;32m   1460\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1461\u001b[0m \u001b[39m    Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint\u001b[39;00m\n\u001b[1;32m   1462\u001b[0m \u001b[39m    it stores the arguments passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1529\u001b[0m \u001b[39m        y_hat = pretrained_model(x)\u001b[39;00m\n\u001b[1;32m   1530\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1531\u001b[0m     loaded \u001b[39m=\u001b[39m _load_from_checkpoint(\n\u001b[1;32m   1532\u001b[0m         \u001b[39mcls\u001b[39;49m,\n\u001b[1;32m   1533\u001b[0m         checkpoint_path,\n\u001b[1;32m   1534\u001b[0m         map_location,\n\u001b[1;32m   1535\u001b[0m         hparams_file,\n\u001b[1;32m   1536\u001b[0m         strict,\n\u001b[1;32m   1537\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1538\u001b[0m     )\n\u001b[1;32m   1539\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(Self, loaded)\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/pytorch_lightning/core/saving.py:60\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load_from_checkpoint\u001b[39m(\n\u001b[1;32m     52\u001b[0m     \u001b[39mcls\u001b[39m: Union[Type[\u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m], Type[\u001b[39m\"\u001b[39m\u001b[39mpl.LightningDataModule\u001b[39m\u001b[39m\"\u001b[39m]],\n\u001b[1;32m     53\u001b[0m     checkpoint_path: Union[_PATH, IO],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m     58\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[\u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mpl.LightningDataModule\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m     59\u001b[0m     \u001b[39mwith\u001b[39;00m pl_legacy_patch():\n\u001b[0;32m---> 60\u001b[0m         checkpoint \u001b[39m=\u001b[39m pl_load(checkpoint_path, map_location\u001b[39m=\u001b[39;49mmap_location)\n\u001b[1;32m     62\u001b[0m     \u001b[39m# convert legacy checkpoints to the new format\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     checkpoint \u001b[39m=\u001b[39m _pl_migrate_checkpoint(\n\u001b[1;32m     64\u001b[0m         checkpoint, checkpoint_path\u001b[39m=\u001b[39m(checkpoint_path \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(checkpoint_path, (\u001b[39mstr\u001b[39m, Path)) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     65\u001b[0m     )\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/lightning_fabric/utilities/cloud_io.py:51\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(path_or_url, map_location)\u001b[0m\n\u001b[1;32m     49\u001b[0m fs \u001b[39m=\u001b[39m get_filesystem(path_or_url)\n\u001b[1;32m     50\u001b[0m \u001b[39mwith\u001b[39;00m fs\u001b[39m.\u001b[39mopen(path_or_url, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(f, map_location\u001b[39m=\u001b[39;49mmap_location)\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/torch/serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    808\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 809\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    810\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[1;32m    811\u001b[0m     \u001b[39mtry\u001b[39;00m:\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/torch/serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1170\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1171\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1172\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1174\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1176\u001b[0m \u001b[39mreturn\u001b[39;00m result\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/pickle.py:1212\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mEOFError\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(key, bytes_types)\n\u001b[0;32m-> 1212\u001b[0m         dispatch[key[\u001b[39m0\u001b[39;49m]](\u001b[39mself\u001b[39;49m)\n\u001b[1;32m   1213\u001b[0m \u001b[39mexcept\u001b[39;00m _Stop \u001b[39mas\u001b[39;00m stopinst:\n\u001b[1;32m   1214\u001b[0m     \u001b[39mreturn\u001b[39;00m stopinst\u001b[39m.\u001b[39mvalue\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/pickle.py:1253\u001b[0m, in \u001b[0;36m_Unpickler.load_binpersid\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_binpersid\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1252\u001b[0m     pid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstack\u001b[39m.\u001b[39mpop()\n\u001b[0;32m-> 1253\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpersistent_load(pid))\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/torch/serialization.py:1142\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1141\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1142\u001b[0m     typed_storage \u001b[39m=\u001b[39m load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   1144\u001b[0m \u001b[39mreturn\u001b[39;00m typed_storage\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/torch/serialization.py:1112\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_tensor\u001b[39m(dtype, numel, key, location):\n\u001b[1;32m   1110\u001b[0m     name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata/\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m-> 1112\u001b[0m     storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39;49mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39;49mUntypedStorage)\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_untyped_storage\n\u001b[1;32m   1113\u001b[0m     \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m     \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m     typed_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[1;32m   1116\u001b[0m         wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1117\u001b[0m         dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m   1118\u001b[0m         _internal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["print(\"Id,Predicted\", file=open('submission.csv', 'w'))\n","kind = \"train\"\n","folder = pathlib.Path(DATA_DIR) / kind\n","for p in list(folder.iterdir()):\n","    index = p.stem\n","    predictions_map = compute_predictions_map(split=kind, index=index)\n","    original_size = cv2.imread(DATA_DIR + f\"/{kind}/{index}/mask.png\", 0).shape[:2]\n","    predictions_map = resize_ski(predictions_map, (original_size[0], original_size[1], 1)).squeeze(axis=-1)    \n","    print(\"original predictions_map size\", predictions_map.shape)    \n","    update_submission(predictions_map, index)\n","    predictions_map = np.where(predictions_map >= threshold, 1, 0)\n","    plt.imsave(f\"{index}_{str(threshold)}_{str(BUFFER)}.png\", predictions_map, cmap=\"gray\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat":4,"nbformat_minor":4}
