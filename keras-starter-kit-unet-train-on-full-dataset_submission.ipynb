{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Keras starter kit [full training set, UNet]"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# import sys\n","\n","# sys.path.append('/kaggle/input/pretrainedmodels/pretrainedmodels-0.7.4')\n","# sys.path.append('/kaggle/input/segmentation-models-pytorch/segmentation_models.pytorch-master')\n","# sys.path.append('/kaggle/input/efficientnet-pytorch/EfficientNet-PyTorch-master')\n","# sys.path.append('/kaggle/input/timm-pytorch-image-models/pytorch-image-models-master')\n","# sys.path.append(\"/kaggle/input/einops/einops-master\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Setup"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-05-10T15:03:39.741181Z","iopub.status.busy":"2023-05-10T15:03:39.740765Z","iopub.status.idle":"2023-05-10T15:03:39.750574Z","shell.execute_reply":"2023-05-10T15:03:39.749341Z","shell.execute_reply.started":"2023-05-10T15:03:39.741144Z"},"trusted":true},"outputs":[],"source":["\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import wandb\n","import torchvision\n","import datetime\n","# import cupy\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","import pytorch_lightning\n","import segmentation_models_pytorch as smp\n","import pytorch_lightning as pl\n","import pytorch_lightning.callbacks.model_checkpoint\n","import pytorch_lightning.plugins\n","from skimage.transform import resize as resize_ski\n","from pytorch_lightning.strategies.ddp import DDPStrategy\n","from pytorch_lightning.loggers import WandbLogger\n","from einops import rearrange, reduce, repeat\n","import torch.nn.functional as F\n","import segmentation_models_pytorch as smp\n","from segmentation_models_pytorch.decoders.unet.decoder import UnetDecoder, DecoderBlock\n","from timm.models.resnet import resnet10t, resnet34d, resnet50d, resnet14t, seresnext26d_32x4d\n","import os\n","from dataclasses import dataclass\n","\n","from scipy.ndimage import distance_transform_edt\n","\n","import ssl\n","ssl._create_default_https_context = ssl._create_unverified_context\n","\n","import glob\n","import time\n","import PIL.Image as Image\n","import matplotlib.pyplot as plt\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import Rectangle\n","import matplotlib.patches as patches\n","from sklearn.model_selection import KFold\n","from tqdm import tqdm\n","import cv2\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data\n","\n","\n","@dataclass\n","class CFG():\n","    # Data config\n","    # DATA_DIR = '/kaggle/input/vesuvius-challenge-ink-detection/'\n","    # DATA_DIR = '/home/fummicc1/codes/competitions/kaggle-ink-detection'\n","    DATA_DIR = '/home/fummicc1/codes/Kaggle/kaggle-ink-detection'\n","    BUFFER = 160 # Half-size of papyrus patches we'll use as model inputs\n","    STRIDE = 80\n","    # Z_LIST = list(range(0, 20, 5)) + list(range(22, 34))  # Offset of slices in the z direction\n","    Z_LIST = list(range(22, 38, 2))\n","    # Z_LIST = list(range(0, 24, 8)) + list(range(24, 36, 2)) + list(range(36, 64, 10))\n","    Z_DIM = len(Z_LIST)  # Number of slices in the z direction. Max value is 64 - Z_START\n","    SHARED_HEIGHT = 4000  # Max height to resize all papyrii\n","\n","    # Model config\n","    BATCH_SIZE = 24\n","\n","    device = torch.device(\"cuda\")\n","    threshold = 0.65\n","    num_workers = 8\n","    exp = 1e-7\n","    mask_padding = BUFFER\n","\n","    num_epochs = 15\n","    lr = 5e-4\n","    eta_min_lr = 1e-6\n","    WANDB_NOTE = \"画像サイズを大きくしてチャネルを減らした\"\n","    pretrained = True"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["<matplotlib.image.AxesImage at 0x14667e667a00>"]},"execution_count":3,"metadata":{},"output_type":"execute_result"},{"name":"stdout","output_type":"stream","text":["Error in callback <function flush_figures at 0x14667e6610d0> (for post_execute):\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/matplotlib_inline/backend_inline.py:126\u001b[0m, in \u001b[0;36mflush_figures\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39mif\u001b[39;00m InlineBackend\u001b[39m.\u001b[39minstance()\u001b[39m.\u001b[39mclose_figures:\n\u001b[1;32m    124\u001b[0m     \u001b[39m# ignore the tracking, just draw and close all figures\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m         \u001b[39mreturn\u001b[39;00m show(\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    127\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    128\u001b[0m         \u001b[39m# safely show traceback if in IPython, else raise\u001b[39;00m\n\u001b[1;32m    129\u001b[0m         ip \u001b[39m=\u001b[39m get_ipython()\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/matplotlib_inline/backend_inline.py:90\u001b[0m, in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     \u001b[39mfor\u001b[39;00m figure_manager \u001b[39min\u001b[39;00m Gcf\u001b[39m.\u001b[39mget_all_fig_managers():\n\u001b[0;32m---> 90\u001b[0m         display(\n\u001b[1;32m     91\u001b[0m             figure_manager\u001b[39m.\u001b[39;49mcanvas\u001b[39m.\u001b[39;49mfigure,\n\u001b[1;32m     92\u001b[0m             metadata\u001b[39m=\u001b[39;49m_fetch_figure_metadata(figure_manager\u001b[39m.\u001b[39;49mcanvas\u001b[39m.\u001b[39;49mfigure)\n\u001b[1;32m     93\u001b[0m         )\n\u001b[1;32m     94\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     show\u001b[39m.\u001b[39m_to_draw \u001b[39m=\u001b[39m []\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/IPython/core/display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m     publish_display_data(data\u001b[39m=\u001b[39mobj, metadata\u001b[39m=\u001b[39mmetadata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    297\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m     format_dict, md_dict \u001b[39m=\u001b[39m \u001b[39mformat\u001b[39;49m(obj, include\u001b[39m=\u001b[39;49minclude, exclude\u001b[39m=\u001b[39;49mexclude)\n\u001b[1;32m    299\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m format_dict:\n\u001b[1;32m    300\u001b[0m         \u001b[39m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/IPython/core/formatters.py:178\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    176\u001b[0m md \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     data \u001b[39m=\u001b[39m formatter(obj)\n\u001b[1;32m    179\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[39m# FIXME: log the exception\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[39mraise\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwsyntax:\n\u001b[1;32m    231\u001b[0m     args, kw \u001b[39m=\u001b[39m fix(args, kw, sig)\n\u001b[0;32m--> 232\u001b[0m \u001b[39mreturn\u001b[39;00m caller(func, \u001b[39m*\u001b[39;49m(extras \u001b[39m+\u001b[39;49m args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/IPython/core/formatters.py:222\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 222\u001b[0m     r \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    223\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    224\u001b[0m     \u001b[39m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[1;32m    225\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_return(\u001b[39mNone\u001b[39;00m, args[\u001b[39m0\u001b[39m])\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/IPython/core/formatters.py:339\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 339\u001b[0m     \u001b[39mreturn\u001b[39;00m printer(obj)\n\u001b[1;32m    340\u001b[0m \u001b[39m# Finally look for special method names\u001b[39;00m\n\u001b[1;32m    341\u001b[0m method \u001b[39m=\u001b[39m get_real_method(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_method)\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/IPython/core/pylabtools.py:151\u001b[0m, in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend_bases\u001b[39;00m \u001b[39mimport\u001b[39;00m FigureCanvasBase\n\u001b[1;32m    149\u001b[0m     FigureCanvasBase(fig)\n\u001b[0;32m--> 151\u001b[0m fig\u001b[39m.\u001b[39;49mcanvas\u001b[39m.\u001b[39;49mprint_figure(bytes_io, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    152\u001b[0m data \u001b[39m=\u001b[39m bytes_io\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m fmt \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39msvg\u001b[39m\u001b[39m'\u001b[39m:\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/matplotlib/backend_bases.py:2366\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2362\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   2363\u001b[0m     \u001b[39m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[1;32m   2364\u001b[0m     \u001b[39m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[1;32m   2365\u001b[0m     \u001b[39mwith\u001b[39;00m cbook\u001b[39m.\u001b[39m_setattr_cm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure, dpi\u001b[39m=\u001b[39mdpi):\n\u001b[0;32m-> 2366\u001b[0m         result \u001b[39m=\u001b[39m print_method(\n\u001b[1;32m   2367\u001b[0m             filename,\n\u001b[1;32m   2368\u001b[0m             facecolor\u001b[39m=\u001b[39;49mfacecolor,\n\u001b[1;32m   2369\u001b[0m             edgecolor\u001b[39m=\u001b[39;49medgecolor,\n\u001b[1;32m   2370\u001b[0m             orientation\u001b[39m=\u001b[39;49morientation,\n\u001b[1;32m   2371\u001b[0m             bbox_inches_restore\u001b[39m=\u001b[39;49m_bbox_inches_restore,\n\u001b[1;32m   2372\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2373\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   2374\u001b[0m     \u001b[39mif\u001b[39;00m bbox_inches \u001b[39mand\u001b[39;00m restore_bbox:\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/matplotlib/backend_bases.py:2232\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2228\u001b[0m     optional_kws \u001b[39m=\u001b[39m {  \u001b[39m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdpi\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfacecolor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39medgecolor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39morientation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   2230\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbbox_inches_restore\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m   2231\u001b[0m     skip \u001b[39m=\u001b[39m optional_kws \u001b[39m-\u001b[39m {\u001b[39m*\u001b[39minspect\u001b[39m.\u001b[39msignature(meth)\u001b[39m.\u001b[39mparameters}\n\u001b[0;32m-> 2232\u001b[0m     print_method \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mwraps(meth)(\u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: meth(\n\u001b[1;32m   2233\u001b[0m         \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{k: v \u001b[39mfor\u001b[39;49;00m k, v \u001b[39min\u001b[39;49;00m kwargs\u001b[39m.\u001b[39;49mitems() \u001b[39mif\u001b[39;49;00m k \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m skip}))\n\u001b[1;32m   2234\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     print_method \u001b[39m=\u001b[39m meth\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:509\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprint_png\u001b[39m(\u001b[39mself\u001b[39m, filename_or_obj, \u001b[39m*\u001b[39m, metadata\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, pil_kwargs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    463\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[39m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[39m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 509\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_print_pil(filename_or_obj, \u001b[39m\"\u001b[39;49m\u001b[39mpng\u001b[39;49m\u001b[39m\"\u001b[39;49m, pil_kwargs, metadata)\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:457\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_print_pil\u001b[39m(\u001b[39mself\u001b[39m, filename_or_obj, fmt, pil_kwargs, metadata\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    453\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[39m    Draw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[39m    *pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[1;32m    456\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 457\u001b[0m     FigureCanvasAgg\u001b[39m.\u001b[39;49mdraw(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    458\u001b[0m     mpl\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mimsave(\n\u001b[1;32m    459\u001b[0m         filename_or_obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer_rgba(), \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39mfmt, origin\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mupper\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    460\u001b[0m         dpi\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure\u001b[39m.\u001b[39mdpi, metadata\u001b[39m=\u001b[39mmetadata, pil_kwargs\u001b[39m=\u001b[39mpil_kwargs)\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:400\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[39m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[39mwith\u001b[39;00m RendererAgg\u001b[39m.\u001b[39mlock, \\\n\u001b[1;32m    398\u001b[0m      (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoolbar\u001b[39m.\u001b[39m_wait_cursor_for_draw_cm() \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoolbar\n\u001b[1;32m    399\u001b[0m       \u001b[39melse\u001b[39;00m nullcontext()):\n\u001b[0;32m--> 400\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49mdraw(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrenderer)\n\u001b[1;32m    401\u001b[0m     \u001b[39m# A GUI class may be need to update a window using this draw, so\u001b[39;00m\n\u001b[1;32m    402\u001b[0m     \u001b[39m# don't forget to call the superclass.\u001b[39;00m\n\u001b[1;32m    403\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mdraw()\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/matplotlib/artist.py:95\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39m@wraps\u001b[39m(draw)\n\u001b[1;32m     94\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdraw_wrapper\u001b[39m(artist, renderer, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 95\u001b[0m     result \u001b[39m=\u001b[39m draw(artist, renderer, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     96\u001b[0m     \u001b[39mif\u001b[39;00m renderer\u001b[39m.\u001b[39m_rasterizing:\n\u001b[1;32m     97\u001b[0m         renderer\u001b[39m.\u001b[39mstop_rasterizing()\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[1;32m     73\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/matplotlib/figure.py:3140\u001b[0m, in \u001b[0;36mFigure.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3137\u001b[0m         \u001b[39m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[1;32m   3139\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch\u001b[39m.\u001b[39mdraw(renderer)\n\u001b[0;32m-> 3140\u001b[0m mimage\u001b[39m.\u001b[39;49m_draw_list_compositing_images(\n\u001b[1;32m   3141\u001b[0m     renderer, \u001b[39mself\u001b[39;49m, artists, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msuppressComposite)\n\u001b[1;32m   3143\u001b[0m \u001b[39mfor\u001b[39;00m sfig \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubfigs:\n\u001b[1;32m   3144\u001b[0m     sfig\u001b[39m.\u001b[39mdraw(renderer)\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/matplotlib/image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mif\u001b[39;00m not_composite \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m has_images:\n\u001b[1;32m    130\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m artists:\n\u001b[0;32m--> 131\u001b[0m         a\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[1;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[39m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     image_group \u001b[39m=\u001b[39m []\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[1;32m     73\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/matplotlib/axes/_base.py:3064\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   3061\u001b[0m \u001b[39mif\u001b[39;00m artists_rasterized:\n\u001b[1;32m   3062\u001b[0m     _draw_rasterized(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure, artists_rasterized, renderer)\n\u001b[0;32m-> 3064\u001b[0m mimage\u001b[39m.\u001b[39;49m_draw_list_compositing_images(\n\u001b[1;32m   3065\u001b[0m     renderer, \u001b[39mself\u001b[39;49m, artists, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49msuppressComposite)\n\u001b[1;32m   3067\u001b[0m renderer\u001b[39m.\u001b[39mclose_group(\u001b[39m'\u001b[39m\u001b[39maxes\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m   3068\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstale \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/matplotlib/image.py:131\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39mif\u001b[39;00m not_composite \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m has_images:\n\u001b[1;32m    130\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m artists:\n\u001b[0;32m--> 131\u001b[0m         a\u001b[39m.\u001b[39;49mdraw(renderer)\n\u001b[1;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[39m# Composite any adjacent images together\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     image_group \u001b[39m=\u001b[39m []\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/matplotlib/artist.py:72\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[0;34m(artist, renderer)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         renderer\u001b[39m.\u001b[39mstart_filter()\n\u001b[0;32m---> 72\u001b[0m     \u001b[39mreturn\u001b[39;00m draw(artist, renderer)\n\u001b[1;32m     73\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m     \u001b[39mif\u001b[39;00m artist\u001b[39m.\u001b[39mget_agg_filter() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/matplotlib/image.py:641\u001b[0m, in \u001b[0;36m_ImageBase.draw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    639\u001b[0m         renderer\u001b[39m.\u001b[39mdraw_image(gc, l, b, im, trans)\n\u001b[1;32m    640\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 641\u001b[0m     im, l, b, trans \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_image(\n\u001b[1;32m    642\u001b[0m         renderer, renderer\u001b[39m.\u001b[39;49mget_image_magnification())\n\u001b[1;32m    643\u001b[0m     \u001b[39mif\u001b[39;00m im \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    644\u001b[0m         renderer\u001b[39m.\u001b[39mdraw_image(gc, l, b, im)\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/matplotlib/image.py:949\u001b[0m, in \u001b[0;36mAxesImage.make_image\u001b[0;34m(self, renderer, magnification, unsampled)\u001b[0m\n\u001b[1;32m    946\u001b[0m transformed_bbox \u001b[39m=\u001b[39m TransformedBbox(bbox, trans)\n\u001b[1;32m    947\u001b[0m clip \u001b[39m=\u001b[39m ((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_clip_box() \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39mbbox) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_clip_on()\n\u001b[1;32m    948\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure\u001b[39m.\u001b[39mbbox)\n\u001b[0;32m--> 949\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_image(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_A, bbox, transformed_bbox, clip,\n\u001b[1;32m    950\u001b[0m                         magnification, unsampled\u001b[39m=\u001b[39;49munsampled)\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/matplotlib/image.py:496\u001b[0m, in \u001b[0;36m_ImageBase._make_image\u001b[0;34m(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border)\u001b[0m\n\u001b[1;32m    494\u001b[0m vrange \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m offset\n\u001b[1;32m    495\u001b[0m \u001b[39m# resample the input data to the correct resolution and shape\u001b[39;00m\n\u001b[0;32m--> 496\u001b[0m A_resampled \u001b[39m=\u001b[39m _resample(\u001b[39mself\u001b[39;49m, A_scaled, out_shape, t)\n\u001b[1;32m    497\u001b[0m \u001b[39mdel\u001b[39;00m A_scaled  \u001b[39m# Make sure we don't use A_scaled anymore!\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[39m# Un-scale the resampled data to approximately the original\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[39m# range. Things that interpolated to outside the original range\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[39m# will still be outside, but possibly clipped in the case of\u001b[39;00m\n\u001b[1;32m    501\u001b[0m \u001b[39m# higher order interpolation + drastically changing data.\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/matplotlib/image.py:207\u001b[0m, in \u001b[0;36m_resample\u001b[0;34m(image_obj, data, out_shape, transform, resample, alpha)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mif\u001b[39;00m resample \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     resample \u001b[39m=\u001b[39m image_obj\u001b[39m.\u001b[39mget_resample()\n\u001b[0;32m--> 207\u001b[0m _image\u001b[39m.\u001b[39;49mresample(data, out, transform,\n\u001b[1;32m    208\u001b[0m                 _interpd_[interpolation],\n\u001b[1;32m    209\u001b[0m                 resample,\n\u001b[1;32m    210\u001b[0m                 alpha,\n\u001b[1;32m    211\u001b[0m                 image_obj\u001b[39m.\u001b[39;49mget_filternorm(),\n\u001b[1;32m    212\u001b[0m                 image_obj\u001b[39m.\u001b[39;49mget_filterrad())\n\u001b[1;32m    213\u001b[0m \u001b[39mreturn\u001b[39;00m out\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["plt.imshow(Image.open(CFG.DATA_DIR + \"/train/1/ir.png\"), cmap=\"gray\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import cupy as cp\n","xp = cp\n","\n","delta_lookup = {\n","    \"xx\": xp.array([[1, -2, 1]], dtype=float),\n","    \"yy\": xp.array([[1], [-2], [1]], dtype=float),\n","    \"xy\": xp.array([[1, -1], [-1, 1]], dtype=float),\n","}\n","\n","def operate_derivative(img_shape, pair):\n","    assert len(img_shape) == 2\n","    delta = delta_lookup[pair]\n","    fft = xp.fft.fftn(delta, img_shape)\n","    return fft * xp.conj(fft)\n","\n","def soft_threshold(vector, threshold):\n","    return xp.sign(vector) * xp.maximum(xp.abs(vector) - threshold, 0)\n","\n","def back_diff(input_image, dim):\n","    assert dim in (0, 1)\n","    r, n = xp.shape(input_image)\n","    size = xp.array((r, n))\n","    position = xp.zeros(2, dtype=int)\n","    temp1 = xp.zeros((r+1, n+1), dtype=float)\n","    temp2 = xp.zeros((r+1, n+1), dtype=float)\n","    \n","    temp1[position[0]:size[0], position[1]:size[1]] = input_image\n","    temp2[position[0]:size[0], position[1]:size[1]] = input_image\n","    \n","    size[dim] += 1\n","    position[dim] += 1\n","    temp2[position[0]:size[0], position[1]:size[1]] = input_image\n","    temp1 -= temp2\n","    size[dim] -= 1\n","    return temp1[0:size[0], 0:size[1]]\n","\n","\n","def forward_diff(input_image, dim):\n","    assert dim in (0, 1)\n","    r, n = xp.shape(input_image)\n","    size = xp.array((r, n))\n","    position = xp.zeros(2, dtype=int)\n","    temp1 = xp.zeros((r+1, n+1), dtype=float)\n","    temp2 = xp.zeros((r+1, n+1), dtype=float)\n","        \n","    size[dim] += 1\n","    position[dim] += 1\n","\n","    temp1[position[0]:size[0], position[1]:size[1]] = input_image\n","    temp2[position[0]:size[0], position[1]:size[1]] = input_image\n","    \n","    size[dim] -= 1\n","    temp2[0:size[0], 0:size[1]] = input_image\n","    temp1 -= temp2\n","    size[dim] += 1\n","    return -temp1[position[0]:size[0], position[1]:size[1]]\n","\n","def iter_deriv(input_image, b, scale, mu, dim1, dim2):\n","    g = back_diff(forward_diff(input_image, dim1), dim2)\n","    d = soft_threshold(g + b, 1 / mu)\n","    b = b + (g - d)\n","    L = scale * back_diff(forward_diff(d - b, dim2), dim1)\n","    return L, b\n","\n","def iter_xx(*args):\n","    return iter_deriv(*args, dim1=1, dim2=1)\n","\n","def iter_yy(*args):\n","    return iter_deriv(*args, dim1=0, dim2=0)\n","\n","def iter_xy(*args):\n","    return iter_deriv(*args, dim1=0, dim2=1)\n","\n","def iter_sparse(input_image, bsparse, scale, mu):\n","    d = soft_threshold(input_image + bsparse, 1 / mu)\n","    bsparse = bsparse + (input_image - d)\n","    Lsparse = scale * (d - bsparse)\n","    return Lsparse, bsparse\n","\n","def denoise_image(input_image, iter_num=100, fidelity=150, sparsity_scale=10, continuity_scale=0.5, mu=1):\n","    image_size = xp.shape(input_image)\n","    #print(\"Initialize denoising\")\n","    norm_array = (\n","        operate_derivative(image_size, \"xx\") + \n","        operate_derivative(image_size, \"yy\") + \n","        2 * operate_derivative(image_size, \"xy\")\n","    )\n","    norm_array += (fidelity / mu) + sparsity_scale ** 2\n","    b_arrays = {\n","        \"xx\": xp.zeros(image_size, dtype=float),\n","        \"yy\": xp.zeros(image_size, dtype=float),\n","        \"xy\": xp.zeros(image_size, dtype=float),\n","        \"L1\": xp.zeros(image_size, dtype=float),\n","    }\n","    g_update = xp.multiply(fidelity / mu, input_image)\n","    for i in tqdm(range(iter_num), total=iter_num):\n","        #print(f\"Starting iteration {i+1}\")\n","        g_update = xp.fft.fftn(g_update)\n","        if i == 0:\n","            g = xp.fft.ifftn(g_update / (fidelity / mu)).real\n","        else:\n","            g = xp.fft.ifftn(xp.divide(g_update, norm_array)).real\n","        g_update = xp.multiply((fidelity / mu), input_image)\n","        \n","        #print(\"XX update\")\n","        L, b_arrays[\"xx\"] = iter_xx(g, b_arrays[\"xx\"], continuity_scale, mu)\n","        g_update += L\n","        \n","        #print(\"YY update\")\n","        L, b_arrays[\"yy\"] = iter_yy(g, b_arrays[\"yy\"], continuity_scale, mu)\n","        g_update += L\n","        \n","        #print(\"XY update\")\n","        L, b_arrays[\"xy\"] = iter_xy(g, b_arrays[\"xy\"], 2 * continuity_scale, mu)\n","        g_update += L\n","        \n","        #print(\"L1 update\")\n","        L, b_arrays[\"L1\"] = iter_sparse(g, b_arrays[\"L1\"], sparsity_scale, mu)\n","        g_update += L\n","        \n","    g_update = xp.fft.fftn(g_update)\n","    g = xp.fft.ifftn(xp.divide(g_update, norm_array)).real\n","    \n","    g[g < 0] = 0\n","    g -= g.min()\n","    g /= g.max()\n","    return g"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def is_in_masked_zone(location, mask):\n","    return mask[location[0], location[1]] > 0\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def resize(img):\n","    current_height, current_width = img.shape    \n","    aspect_ratio = current_width / current_height\n","    if CFG.SHARED_HEIGHT is None:\n","        return img\n","    if CFG.SHARED_HEIGHT > current_height:\n","        # 解像度が既に小さい場合は行わない\n","        return img\n","    new_height = CFG.SHARED_HEIGHT\n","    new_width = int(CFG.SHARED_HEIGHT * aspect_ratio)\n","    new_size = (new_width, new_height)\n","    # (W, H)の順で渡すが結果は(H, W)になっている\n","    img = cv2.resize(img, new_size)\n","    return img\n","\n","def load_mask(split, index): \n","    img = cv2.imread(f\"{CFG.DATA_DIR}/{split}/{index}/mask.png\", 0) // 255    \n","    img = resize(img)    \n","    return img\n","\n","\n","def load_labels(split, index):    \n","    img = cv2.imread(f\"{CFG.DATA_DIR}/{split}/{index}/inklabels.png\", 0) // 255        \n","    img = resize(img)\n","    return img"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def extract_subvolume(location, volume):\n","    global printed\n","    y = location[0]\n","    x = location[1]\n","    subvolume = volume[y-CFG.BUFFER:y+CFG.BUFFER, x-CFG.BUFFER:x+CFG.BUFFER, :].astype(np.float32)\n","    \n","    return subvolume"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def load_volume(split, index):\n","    # Load the 3d x-ray scan, one slice at a time\n","    all = sorted(glob.glob(f\"{CFG.DATA_DIR}/{split}/{index}/surface_volume/*.tif\"))\n","    z_slices_fnames = [all[i] for i in range(len(all)) if i in CFG.Z_LIST]\n","    assert len(z_slices_fnames) == CFG.Z_DIM\n","    z_slices = []\n","    for z, filename in  tqdm(enumerate(z_slices_fnames)):\n","        img = cv2.imread(filename, -1)        \n","        img = resize(img)\n","        # img = (img / (2 ** 8)).astype(np.uint8)\n","        img = img.astype(np.float32) // 255\n","        z_slices.append(img)\n","    return np.stack(z_slices, axis=-1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def generate_locations_ds(volume, mask, label=None, skip_zero=False):\n","    is_in_mask_train = lambda x: is_in_masked_zone(x, mask)\n","\n","    # Create a list to store train locations\n","    locations = []\n","\n","    # Generate train locations\n","    volume_height, volume_width = volume.shape[:-1]\n","\n","    for y in range(CFG.BUFFER, volume_height - CFG.BUFFER, CFG.STRIDE):\n","        for x in range(CFG.BUFFER, volume_width - CFG.BUFFER, CFG.STRIDE):\n","            if skip_zero and label is not None and np.all(label[y - CFG.STRIDE : y + CFG.STRIDE, x - CFG.STRIDE : x + CFG.STRIDE] == 0):\n","                print(f\"skip location at (y: {y}, x: {x})\")\n","                continue\n","            if is_in_mask_train((y, x)):\n","                locations.append((y, x))\n","\n","    # Convert the list of train locations to a PyTorch tensor\n","    locations_ds = np.stack(locations, axis=0)\n","    return locations_ds"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tc = torch\n","def TTA(x:tc.Tensor,model:nn.Module):\n","    #x.shape=(batch,c,h,w)\n","    shape=x.shape\n","    x=[x,*[tc.rot90(x,k=i,dims=(-2,-1)) for i in range(1,4)]]\n","    x=tc.cat(x,dim=0)\n","    x, _ = model(x)\n","    x=x.reshape(4,shape[0], 1 ,*shape[2:])\n","    x=[tc.rot90(x[i],k=-i,dims=(-2,-1)) for i in range(4)]\n","    x=tc.stack(x,dim=0)\n","    return x.mean(0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.preprocessing import OneHotEncoder\n","\n","from albumentations.core.transforms_interface import ImageOnlyTransform\n","\n","class SubvolumeDataset(Dataset):\n","    def __init__(self, locations, volume, labels, buffer, is_train: bool, return_location: bool = False):\n","        self.locations = locations\n","        self.volume = volume\n","        self.labels = labels        \n","        self.buffer = buffer\n","        self.is_train = is_train\n","        self.return_location = return_location\n","\n","    def __len__(self):\n","        return len(self.locations)\n","\n","    def __getitem__(self, idx):\n","        label = None\n","        location = np.array(self.locations[idx])\n","        y, x = location[0], location[1]\n","\n","        subvolume = extract_subvolume(location, self.volume)\n","        \n","        if self.labels is not None:\n","            label = self.labels[y - self.buffer:y + self.buffer, x - self.buffer:x + self.buffer]            \n","            label = np.stack([label], axis=-1)            \n","            \n","        # 段々meanは小さくなる\n","        mean = np.array([0.45 - i / 100 for i in range(0, CFG.Z_DIM)]).reshape(-1, 1, 1)\n","        # 段々stdは小さくなる\n","        std = np.array([0.22 - i / 300 for i in range(0, CFG.Z_DIM)]).reshape(-1, 1, 1)\n","        \n","        if self.is_train and label is not None:    \n","            transformed = A.Compose([\n","                A.HorizontalFlip(p=0.3),\n","                A.VerticalFlip(p=0.3),\n","                A.RandomScale(p=0.4, scale_limit=0.2),\n","                A.Transpose(p=0.4),\n","                A.RandomRotate90(p=0.3),\n","                A.ShiftScaleRotate(p=0.7, scale_limit=0.4, shift_limit=0.2),                \n","                A.OneOf([\n","                    A.GaussNoise(var_limit=[5, 20]),\n","                    A.GaussianBlur(blur_limit=(3, 5)),\n","                    A.MotionBlur(blur_limit=(3, 5)),\n","                ], p=0.4),\n","                # A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.5),\n","                # A.CoarseDropout(\n","                #     max_holes=1, \n","                #     max_width=int(CFG.BUFFER * 2 * 0.3),\n","                #     max_height=int(CFG.BUFFER * 2 * 0.3), \n","                #     mask_fill_value=0, \n","                #     p=0.5\n","                # ),\n","                A.GridDistortion(p=0.5),\n","                A.PixelDropout(mask_drop_value=0,  p=0.2),\n","                # A.CoarseDropout(\n","                #     max_holes=1,\n","                #     max_width=int(self.buffer * 0.3),\n","                #     max_height=int(self.buffer * 0.3),\n","                #     mask_fill_value=0,\n","                #     p=0.5,\n","                # ),\n","                # A.GridDropout(p=0.15),\n","                A.PadIfNeeded(min_height=self.buffer * 2, min_width=self.buffer * 2),\n","                A.Resize(height=self.buffer * 2, width=self.buffer * 2),\n","            ])(image=subvolume, mask=label)\n","            subvolume = transformed[\"image\"]\n","            label = transformed[\"mask\"]\n","            subvolume = np.transpose(subvolume, (2, 0, 1))\n","            label = np.transpose(label, (2, 0, 1))\n","            subvolume /= 255.\n","            subvolume = (subvolume - mean) / std       \n","        else:\n","            if label is None:\n","                subvolume = np.transpose(subvolume, (2, 0, 1))\n","                subvolume /= 255.\n","                subvolume = (subvolume - mean) / std\n","            else:\n","                # print(\"subvolume in val dataset (before aug)\", subvolume, file=open(\"before-val-aug.log\", \"w\")) \n","                subvolume = np.transpose(subvolume, (2, 0, 1))\n","                label = np.transpose(label, (2, 0, 1))\n","                subvolume /= 255.\n","                subvolume = (subvolume - mean) / std\n","        # print(\"subvolume\", subvolume)\n","        if self.return_location:\n","            return subvolume, location\n","        return subvolume, label        "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class SmpUnetDecoder(nn.Module):\n","\tdef __init__(\n","\t\tself,\n","\t\tin_channel,\n","\t\tskip_channel,\n","\t\tout_channel,\n","\t):\n","\t\tsuper().__init__()\n","\t\tself.center = nn.Identity()\n","\n","\t\ti_channel = [\n","\t\t\tin_channel,\n","\t\t] + out_channel[:-1]\n","\t\ts_channel = skip_channel\n","\t\to_channel = out_channel\n","\t\tblock = [\n","\t\t\tDecoderBlock(i, s, o, use_batchnorm=True, attention_type=None)\n","\t\t\tfor i, s, o in zip(i_channel, s_channel, o_channel)\n","\t\t]\n","\t\tself.block = nn.ModuleList(block)\n","\n","\tdef forward(self, feature, skip):\n","\t\td = self.center(feature)\n","\t\tdecode = []\n","\t\tfor i, block in enumerate(self.block):\n","\t\t\ts = skip[i]\n","\t\t\td = block(d, s)\n","\t\t\tdecode.append(d)\n","\n","\t\tlast = d\n","\t\treturn last, decode\n","\n","\n","class Net(nn.Module):\n","\tdef __init__(\n","\t\tself,\n","\t):\n","\t\tsuper().__init__()\n","\t\tself.output_type = [\"inference\", \"loss\"]\n","\n","\t\tconv_dim = 64\n","\t\tencoder1_dim = [\n","\t\t\tconv_dim,\n","\t\t\t256,\n","\t\t\t512,\n","\t\t\t1024,\n","\t\t\t2048,\n","\t\t]\n","\t\tdecoder1_dim = [\n","\t\t\t256,\n","\t\t\t128,\n","\t\t\t64,\n","\t\t\t64,\n","\t\t]\n","\n","\t\tself.encoder1 = seresnext26d_32x4d(pretrained=CFG.pretrained, in_chans=CFG.Z_DIM - 4)\n","\n","\t\tself.decoder1 = SmpUnetDecoder(\n","\t\t\tin_channel=encoder1_dim[-1],\n","\t\t\tskip_channel=encoder1_dim[:-1][::-1],\n","\t\t\tout_channel=decoder1_dim,\n","\t\t)\n","\t\t# -- pool attention weight  \n","\t\tself.weight1 = nn.ModuleList(\n","\t\t\t[\n","\t\t\t\tnn.Sequential(\n","\t\t\t\t\tnn.Conv2d(dim, dim, kernel_size=3, padding=1),\n","\t\t\t\t\tnn.ReLU(inplace=True),\n","\t\t\t\t)\n","\t\t\t\tfor dim in encoder1_dim\n","\t\t\t]\n","\t\t)\n","\t\tself.logit1 = nn.Conv2d(decoder1_dim[-1], 1, kernel_size=1)\n","\n","\t\t# --------------------------------\n","\t\t#\n","\t\tencoder2_dim = [64, 128, 256, 512]  #\n","\t\tdecoder2_dim = [\n","\t\t\t128,\n","\t\t\t64,\n","\t\t\t32,\n","\t\t]\n","\t\tself.encoder2 = resnet10t(pretrained=CFG.pretrained, in_chans=decoder1_dim[-1])\n","\n","\t\tself.decoder2 = SmpUnetDecoder(\n","\t\t\tin_channel=encoder2_dim[-1],\n","\t\t\tskip_channel=encoder2_dim[:-1][::-1],\n","\t\t\tout_channel=decoder2_dim,\n","\t\t)\n","\t\tself.logit2 = nn.Conv2d(decoder2_dim[-1], 1, kernel_size=1)\n","\n","\tdef forward(self, batch):\n","\t\tv = batch\n","\t\tB, C, H, W = v.shape\n","\t\tvv = [\n","\t\t\tv[:, i : i + CFG.Z_DIM - 4]\n","\t\t\tfor i in [0, 2, 4]\n","\t\t]\n","\t\tK = len(vv)\n","\t\tx = torch.cat(vv, 0)\n","\t\t# x = v\n","\n","\t\t# ----------------------\n","\t\tencoder = []\n","\t\te = self.encoder1\n","\t\tx = e.conv1(x)\n","\t\tx = e.bn1(x)\n","\t\tx = e.act1(x)\n","\t\tencoder.append(x)\n","\t\tx = F.avg_pool2d(x, kernel_size=2, stride=2)\n","\t\tx = e.layer1(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer2(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer3(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer4(x)\n","\t\tencoder.append(x)\n","\t\t# print('encoder', [f.shape for f in encoder])\n","\n","\t\tfor i in range(len(encoder)):\n","\t\t\te = encoder[i]\n","\t\t\tf = self.weight1[i](e)\n","\t\t\t_, c, h, w = e.shape\n","\t\t\tf = rearrange(f, \"(K B) c h w -> B K c h w\", K=K, B=B, h=h, w=w)  #\n","\t\t\te = rearrange(e, \"(K B) c h w -> B K c h w\", K=K, B=B, h=h, w=w)  #\n","\t\t\tw = F.softmax(f, 1)\n","\t\t\te = (w * e).sum(1)\n","\t\t\tencoder[i] = e\n","\n","\t\tfeature = encoder[-1]\n","\t\tskip = encoder[:-1][::-1]\n","\t\tlast, decoder = self.decoder1(feature, skip)\n","\t\tlogit1 = self.logit1(last)\n","  \n","\t\tlogit1 = F.interpolate(\n","\t\t\tlogit1, size=(H, W), mode=\"bilinear\", align_corners=False, antialias=True\n","\t\t)\n","\n","\t\t# ----------------------\n","\t\tx = last  # .detach()\n","\t\t# x = F.avg_pool2d(x,kernel_size=2,stride=2)\n","\t\tencoder = []\n","\t\te = self.encoder2\n","\t\tx = e.layer1(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer2(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer3(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer4(x)\n","\t\tencoder.append(x)\n","\n","\t\tfeature = encoder[-1]\n","\t\tskip = encoder[:-1][::-1]\n","\t\tlast, decoder = self.decoder2(feature, skip)\n","\t\tlogit2 = self.logit2(last)\n","\t\tlogit2 = F.interpolate(\n","\t\t\tlogit2, size=(H, W), mode=\"bilinear\", align_corners=False, antialias=True\n","\t\t)\n","\t\treturn logit1, logit2\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ref - https://www.kaggle.com/competitions/vesuvius-challenge-ink-detection/discussion/397288\n","def fbeta_score(preds, targets, threshold, beta=0.5, smooth=1e-5):\n","    preds_t = torch.where(preds > threshold, 1.0, 0.0).float()\n","    y_true_count = targets.sum()\n","    \n","    ctp = preds_t[targets==1].sum()\n","    cfp = preds_t[targets==0].sum()\n","    beta_squared = beta * beta\n","\n","    c_precision = ctp / (ctp + cfp + smooth)\n","    c_recall = ctp / (y_true_count + smooth)\n","    dice = (1 + beta_squared) * (c_precision * c_recall) / (beta_squared * c_precision + c_recall + smooth)\n","\n","    return dice"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def dice_coef_torch(prob_preds, targets, beta=0.5, smooth=1e-5):\n","    # No need to binarize the predictions\n","    # prob_preds = torch.sigmoid(preds)\n","\n","    # flatten label and prediction tensors\n","    prob_preds = prob_preds.view(-1).float()\n","    targets = targets.view(-1).float()\n","\n","    intersection = (prob_preds * targets).sum()\n","\n","    dice = ((1 + beta**2) * intersection + smooth) / ((beta**2) * prob_preds.sum() + targets.sum() + smooth)\n","\n","    return dice\n","\n","\n","\n","class Model(pl.LightningModule):\n","    training_step_outputs = []\n","    validation_step_outputs = []\n","    test_step_outputs = [[], []]\n","\n","    def __init__(self, **kwargs):\n","        super().__init__()\n","\n","        self.model = Net()        \n","\n","        # self.loss1 = nn.BCEWithLogitsLoss(\n","        #     pos_weight=torch.tensor([0.7])\n","        # )\n","        # self.loss2 = nn.BCEWithLogitsLoss(\n","        #     pos_weight=torch.tensor([0.7])\n","        # )\n","        self.loss1 = smp.losses.TverskyLoss(\n","            smp.losses.BINARY_MODE,\n","            log_loss=False,\n","            from_logits=True, \n","            smooth=1e-6,\n","            alpha=0.6,\n","            beta=0.4,\n","        )\n","        self.loss2 = smp.losses.TverskyLoss(\n","            smp.losses.BINARY_MODE,\n","            log_loss=False,\n","            from_logits=True, \n","            smooth=1e-6,\n","            alpha=0.6,\n","            beta=0.4,\n","        )\n","\n","    def forward(self, image, stage):\n","        mask = TTA(image, self.model)\n","        return mask\n","\n","    def shared_step(self, batch, stage):\n","        subvolumes, labels = batch\n","\n","        image, labels = subvolumes.float(), labels.float()        \n","        assert image.ndim == 4\n","        \n","        h, w = image.shape[2:]\n","        assert h % 32 == 0 and w % 32 == 0\n","        \n","        # print(\"labels\", labels.max(), labels.min())\n","\n","        assert labels.max() <= 1.0 and labels.min() >= 0\n","\n","        logit1, logit2 = self.forward(image, stage)\n","        \n","        # print(\"segmentation_out\", segmentation_out.shape)\n","\n","        loss = 0.3 * self.loss1(logit1, labels) + 0.7 * self.loss2(logit2, labels)\n","        \n","        prob2 = torch.sigmoid(logit2)\n","\n","        pred_mask = (prob2 > CFG.threshold).float()\n","        \n","        # print(\"pred_mask\", pred_mask)\n","        \n","        score = fbeta_score(pred_mask, labels, threshold=CFG.threshold)\n","\n","        tp, fp, fn, tn = smp.metrics.get_stats(\n","            pred_mask.long(), labels.long(), mode=\"binary\"\n","        )\n","\n","        return {\n","            \"loss\": loss,\n","            \"tp\": tp,\n","            \"fp\": fp,\n","            \"fn\": fn,\n","            \"tn\": tn,\n","            \"score\": score,\n","        }\n","\n","    def shared_epoch_end(self, outputs, stage):\n","        # aggregate step metics\n","        tp = torch.cat([x[\"tp\"] for x in outputs])\n","        fp = torch.cat([x[\"fp\"] for x in outputs])\n","        fn = torch.cat([x[\"fn\"] for x in outputs])\n","        tn = torch.cat([x[\"tn\"] for x in outputs])\n","        loss = torch.mean(torch.Tensor([x[\"loss\"] for x in outputs]))\n","        fbeta_score = torch.mean(torch.Tensor([x[\"score\"] for x in outputs]))\n","\n","        per_image_iou = smp.metrics.iou_score(\n","            tp, fp, fn, tn, reduction=\"micro-imagewise\"\n","        )\n","\n","        dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n","\n","        metrics = {\n","            f\"{stage}_per_image_iou\": per_image_iou,\n","            f\"{stage}_dataset_iou\": dataset_iou,\n","            f\"{stage}_loss\": loss.item(),\n","            f\"{stage}_tp\": tp.sum().int().item(),\n","            f\"{stage}_fp\": fp.sum().int().item(),\n","            f\"{stage}_fn\": fn.sum().int().item(),\n","            f\"{stage}_tn\": tn.sum().int().item(),\n","            f\"{stage}_score\": fbeta_score.item(),\n","        }\n","\n","        self.log_dict(metrics, prog_bar=True, sync_dist=True)\n","\n","    def training_step(self, batch, batch_idx):\n","        out = self.shared_step(batch, \"train\")\n","        self.training_step_outputs.append(out)\n","        return out\n","\n","    def on_train_epoch_end(self):\n","        out = self.shared_epoch_end(self.training_step_outputs, \"train\")\n","        self.training_step_outputs.clear()\n","        return out\n","\n","    def validation_step(self, batch, batch_idx):\n","        out = self.shared_step(batch, \"valid\")\n","        self.validation_step_outputs.append(out)\n","        return out\n","\n","    def on_validation_epoch_end(self):\n","        out = self.shared_epoch_end(self.validation_step_outputs, \"valid\")\n","        self.validation_step_outputs.clear()\n","        return out\n","\n","    def test_step(self, batch, batch_idx):\n","        global predictions_map, predictions_map_counts\n","\n","        patch_batch, loc_batch = batch\n","\n","        loc_batch = loc_batch.long()\n","        patch_batch = patch_batch.float()\n","        predictions = self.forward(patch_batch, \"test\")\n","        predictions = predictions.sigmoid()\n"," \n","        predictions = torch.permute(predictions, (0, 2, 3, 1)).squeeze(dim=-1)\n","        predictions = (\n","            predictions.cpu().numpy()\n","        )\n","        loc_batch = loc_batch.cpu().numpy()\n","        \n","        self.test_step_outputs[0].extend(loc_batch)\n","        self.test_step_outputs[1].extend(predictions)        \n","        return loc_batch, predictions\n","\n","\n","    def on_test_epoch_end(self):\n","        global predictions_map, predictions_map_counts\n","\n","        locs = np.array(self.test_step_outputs[0])\n","        preds = np.array(self.test_step_outputs[1])\n","        print(\"locs\", locs.shape)\n","        print(\"preds\", preds.shape)\n","\n","        new_predictions_map = np.zeros_like(predictions_map[:, :, 0])\n","        new_predictions_map_counts = np.zeros_like(predictions_map_counts[:, :, 0])\n","\n","        for (y, x), pred in zip(locs, preds):\n","            new_predictions_map[\n","                y - CFG.BUFFER : y + CFG.BUFFER, x - CFG.BUFFER : x + CFG.BUFFER\n","            ] += pred\n","            new_predictions_map_counts[\n","                y - CFG.BUFFER : y + CFG.BUFFER, x - CFG.BUFFER : x + CFG.BUFFER\n","            ] += 1\n","        \n","        new_predictions_map /= new_predictions_map_counts + CFG.exp\n","        new_predictions_map = new_predictions_map[:, :, np.newaxis]\n","        predictions_map = np.concatenate(\n","            [predictions_map, new_predictions_map], axis=-1\n","        )\n","\n","    def configure_optimizers(self):\n","        optimizer = optim.AdamW(self.parameters(), lr=CFG.lr)\n","        \n","        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","            optimizer, mode=\"min\", factor=0.8, patience=2,\n","        )\n","        return {\n","            \"optimizer\": optimizer,\n","            \"lr_scheduler\": {\"scheduler\": scheduler, \"monitor\": \"valid_loss\"},\n","        }\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class EnsembleModel:\n","    def __init__(self, test_loader, test_volume):\n","        super().__init__()\n","        self.test_loader = test_loader\n","        self.test_volume = test_volume\n","        self.list = []\n","        for fold in [1, 2, 3, 4]:\n","            _model = Model.load_from_checkpoint(\n","                f\"weights/weights_fold-{fold}.ckpt\",               \n","            )\n","            trainer = pl.Trainer(\n","                accelerator=\"gpu\",\n","                devices=\"1\",\n","                enable_checkpointing=False,\n","            )\n","\n","            self.list.append((_model, trainer))\n","    \n","    def forward(self):\n","        global predictions_map, predictions_map_counts\n","        predictions_map = np.empty_like(self.test_volume[:, :, 0])[:, :, np.newaxis].astype(np.float64)\n","        predictions_map_counts = np.empty_like(predictions_map).astype(np.uint8)\n","        for i, (model, trainer) in enumerate(self.list):\n","            model: Model = model\n","            model.test_step_outputs = [[], []]\n","            model.eval()\n","            trainer.test(\n","                model=model,\n","                dataloaders=self.test_loader,\n","                verbose=True,\n","            )\n","            self.list[i] = None\n","            gc.collect()\n","        predictions_map = predictions_map[:, :, 1:].mean(axis=-1, keepdims=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","from tqdm import tqdm\n","from skimage.transform import resize as resize_ski\n","import pathlib\n","import gc\n","\n","def compute_predictions_map(split, index):\n","    global predictions_map\n","    global predictions_map_counts\n","    \n","    print(f\"Load data for {split}/{index}\")\n","\n","    test_volume = load_volume(split=split, index=index)\n","    test_mask = load_mask(split=split, index=index)    \n","    \n","    #get coord\n","    stride = CFG.BUFFER\n","    H,W,D  = test_volume.shape\n","\n","    ##pad #assume H,W >size\n","    px, py = W % stride, H % stride\n","    if (px != 0) or (py != 0):\n","        px = stride - px\n","        py = stride - py\n","        test_volume = np.pad(test_volume, [(0, py), (0, px), (0, 0)], constant_values=0)\n","        test_mask = np.pad(test_mask, [(0, py), (0, px)], constant_values=0)  \n","        \n","    test_locations = generate_locations_ds(test_volume, test_mask)  \n","    \n","    print(f\"{len(test_locations)} test locations (before filtering by mask)\")\n","\n","    # filter locations inside the mask\n","    test_locations = [loc for loc in test_locations if is_in_masked_zone(loc, test_mask)]\n","    \n","    print(f\"{len(test_locations)} test locations (after filtering by mask)\")\n","\n","    test_ds = SubvolumeDataset(test_locations, test_volume, None, CFG.BUFFER, is_train=False, return_location=True)\n","    test_loader = DataLoader(test_ds, batch_size=CFG.BATCH_SIZE, num_workers=CFG.num_workers, pin_memory=True)        \n","\n","    # # shape: (Y, X, C)\n","    predictions_map = np.zeros_like(test_volume[:, :, 0])[:, :, np.newaxis].astype(np.float64)\n","    predictions_map_counts = np.zeros_like(predictions_map).astype(np.uint8)\n","\n","    print(\"test_volume.shape\", test_volume.shape)\n","    print(\"predictions_map.shape\", predictions_map.shape)\n","\n","    print(f\"Compute predictions\")\n","    \n","    model = EnsembleModel(test_loader, test_volume)\n","    model.forward()\n","    del model\n","    del test_locations\n","    del test_loader\n","    del test_ds\n","    del test_volume\n","    del test_mask\n","    gc.collect()\n","    \n","    # print(\"predictions_map\", predictions_map, file=open(\"predictions_map\", \"w\"))\n","    return predictions_map[:H, :W, :]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Fast run length encoding, from https://www.kaggle.com/code/hackerpoet/even-faster-run-length-encoder/script\n","# (H, W)の形式\n","def rle (img: np.ndarray, threshold: float):\n","    flat_img = img.flatten()\n","    flat_img = np.where(flat_img > threshold, 1, 0).astype(np.uint8)\n","\n","    starts = np.array((flat_img[:-1] == 0) & (flat_img[1:] == 1))\n","    ends = np.array((flat_img[:-1] == 1) & (flat_img[1:] == 0))\n","    starts_ix = np.where(starts)[0] + 2\n","    ends_ix = np.where(ends)[0] + 2\n","    lengths = ends_ix - starts_ix\n","\n","    return \" \".join(map(str, sum(zip(starts_ix, lengths), ())))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def update_submission(predictions_map, index):\n","    rle_ = rle(predictions_map, threshold=CFG.threshold)\n","    print(f\"{index},\" + rle_, file=open('submission.csv', 'a'))\n","    # print(f\"{index},\" + rle_, file=open('/kaggle/working/submission.csv', 'a'))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Load data for test/a\n"]},{"name":"stderr","output_type":"stream","text":["8it [00:05,  1.60it/s]\n"]},{"name":"stdout","output_type":"stream","text":["1648 test locations (before filtering by mask)\n","1648 test locations (after filtering by mask)\n","test_volume.shape (2880, 6400, 8)\n","predictions_map.shape (2880, 6400, 1)\n","Compute predictions\n"]},{"name":"stderr","output_type":"stream","text":["/home/fummicc1/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/lightning_fabric/plugins/environments/slurm.py:166: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/fummicc1/anaconda3/envs/ink-detection-3_8/lib/ ...\n","  rank_zero_warn(\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","/home/fummicc1/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n","  warning_cache.warn(\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","/home/fummicc1/anaconda3/envs/ink-detection-3_8/lib/python3.8/site-packages/lightning_fabric/plugins/environments/slurm.py:166: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/fummicc1/anaconda3/envs/ink-detection-3_8/lib/ ...\n","  rank_zero_warn(\n","You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ecb41aef6a9f4365a49553c736c08166","version_major":2,"version_minor":0},"text/plain":["Testing: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["locs (1648, 2)\n","preds (1648, 320, 320)\n"]},{"name":"stderr","output_type":"stream","text":["You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f3e933f8ea8644fcb318dde2d0f471fc","version_major":2,"version_minor":0},"text/plain":["Testing: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["locs (1648, 2)\n","preds (1648, 320, 320)\n"]},{"name":"stderr","output_type":"stream","text":["You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"708e1bcfda8343759d86fb85f6717696","version_major":2,"version_minor":0},"text/plain":["Testing: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["locs (1648, 2)\n","preds (1648, 320, 320)\n"]},{"name":"stderr","output_type":"stream","text":["You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"94b4957070ed415fb16e2b41a2624390","version_major":2,"version_minor":0},"text/plain":["Testing: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["locs (1648, 2)\n","preds (1648, 320, 320)\n","original size (2727, 6330)\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 250/250 [00:34<00:00,  7.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Load data for test/b\n"]},{"name":"stderr","output_type":"stream","text":["8it [00:05,  1.47it/s]\n"]},{"name":"stdout","output_type":"stream","text":["1486 test locations (before filtering by mask)\n","1486 test locations (after filtering by mask)\n","test_volume.shape (4160, 4800, 8)\n","predictions_map.shape (4160, 4800, 1)\n","Compute predictions\n"]},{"name":"stderr","output_type":"stream","text":["GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ccdff578395c424eba1c5889094681b6","version_major":2,"version_minor":0},"text/plain":["Testing: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["locs (1486, 2)\n","preds (1486, 320, 320)\n"]},{"name":"stderr","output_type":"stream","text":["You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c2c86bd4fd2b4ecfad942f5ed9b525c6","version_major":2,"version_minor":0},"text/plain":["Testing: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["locs (1486, 2)\n","preds (1486, 320, 320)\n"]},{"name":"stderr","output_type":"stream","text":["You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"309915fc47624f98aedc72bffd672933","version_major":2,"version_minor":0},"text/plain":["Testing: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["locs (1486, 2)\n","preds (1486, 320, 320)\n"]},{"name":"stderr","output_type":"stream","text":["You are using a CUDA device ('NVIDIA RTX A5000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0fa8ceef59ec4d189b7aaaf1abe1f9ef","version_major":2,"version_minor":0},"text/plain":["Testing: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["locs (1486, 2)\n","preds (1486, 320, 320)\n","original size (5454, 6330)\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 250/250 [01:11<00:00,  3.48it/s]\n"]}],"source":["predictions_map = None\n","predictions_map_counts = None\n","print(\"Id,Predicted\", file=open('submission.csv', 'w'))\n","kind = \"test\"\n","folder = pathlib.Path(CFG.DATA_DIR) / kind\n","for p in list(folder.iterdir()):\n","    index = p.stem\n","    predictions_map = compute_predictions_map(split=kind, index=index)\n","    original_size = cv2.imread(CFG.DATA_DIR + f\"/{kind}/{index}/mask.png\", 0).shape[:2]\n","    resized_predictions_map = resize_ski(predictions_map, (original_size[0], original_size[1], 1), anti_aliasing=True).squeeze(axis=-1)\n","    print(\"original size\", original_size)\n","    resized_predictions_map=xp.array(resized_predictions_map)\n","    resized_predictions_map=denoise_image(resized_predictions_map, iter_num=250)\n","    resized_predictions_map=resized_predictions_map.get()\n","    update_submission(resized_predictions_map, index)\n","    resized_predictions_map = np.where(resized_predictions_map >= CFG.threshold, 255, 0)\n","    plt.imsave(f\"{index}_{str(CFG.threshold)}_{str(CFG.BUFFER)}.png\", resized_predictions_map, cmap=\"gray\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat":4,"nbformat_minor":4}
