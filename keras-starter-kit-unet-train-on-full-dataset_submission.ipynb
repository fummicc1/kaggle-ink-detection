{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Keras starter kit [full training set, UNet]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Setup"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T09:04:23.702624Z","iopub.status.busy":"2023-05-08T09:04:23.702304Z","iopub.status.idle":"2023-05-08T09:04:32.404721Z","shell.execute_reply":"2023-05-08T09:04:32.403572Z","shell.execute_reply.started":"2023-05-08T09:04:23.702594Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","import glob\n","import time\n","import PIL.Image as Image\n","import matplotlib.pyplot as plt\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import Rectangle\n","import matplotlib.patches as patches\n","from tqdm import tqdm\n","\n","# Data config\n","# DATA_DIR = '/kaggle/input/vesuvius-challenge-ink-detection/'\n","DATA_DIR = \".\"\n","BUFFER = 128  # Half-size of papyrus patches we'll use as model inputs\n","Z_DIM = 4   # Number of slices in the z direction. Max value is 64 - Z_START\n","Z_START = 60  # Offset of slices in the z direction\n","SHARED_HEIGHT = 1600  # Height to resize all papyrii\n","\n","# (x, y)\n","val_location = (200, 800)\n","val_zone_size = (BUFFER * 2, BUFFER * 2)\n","\n","# Model config\n","BATCH_SIZE = 32\n","USE_MIXED_PRECISION = False\n","USE_JIT_COMPILE = False"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["\n","class UNet(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(UNet, self).__init__()\n","\n","        def conv_block(in_channels, out_channels):\n","            return nn.Sequential(\n","                nn.ReLU(),\n","                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(out_channels),\n","            )\n","\n","        def transpose_conv_block(in_channels, out_channels):\n","            return nn.Sequential(\n","                nn.ReLU(),\n","                nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(out_channels),\n","            )\n","\n","        self.encoder = nn.ModuleList([\n","            nn.Sequential(\n","                nn.Conv2d(in_channels if i == 1 else 64 * 2**(i - 1), 64 * 2**i, kernel_size=3, stride=2, padding=1),\n","                nn.BatchNorm2d(64 * 2**i),\n","                nn.ReLU(),\n","                nn.Conv2d(64 * 2**i, 64 * 2**i, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(64 * 2**i),\n","                nn.ReLU(),\n","            )\n","            for i in range(1, 3)\n","        ])\n","\n","\n","        self.middle = nn.Sequential(\n","            conv_block(256, 256),\n","            conv_block(256, 256),\n","        )\n","        \n","        self.decoder = nn.ModuleList([\n","            nn.Sequential(\n","                transpose_conv_block(2 ** (i + 7), 2 ** (i + 6)),\n","                transpose_conv_block(2 ** (i + 6), 2 ** (i + 5)),\n","                nn.Upsample(scale_factor=2, mode=\"nearest\"),\n","            )\n","            for i in range(2, 0, -1)\n","        ])\n","        self.final_decoder = nn.Sequential(\n","            nn.Conv2d(64, out_channels, kernel_size=3, padding=1),\n","        )\n","        self.activation = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        skip_connections = []\n","        for layer in self.encoder:\n","            x = layer(x)\n","            skip_connections.append(x)\n","\n","        x = self.middle(x)\n","        \n","        # print(\"encoder ok\", x.shape)\n","        for i, layer in enumerate(self.decoder):            \n","            # print(f\"decoder will {i}: ok\", x.shape)\n","            x = torch.cat([x, skip_connections[-i-1]], dim=1)  # Concatenate along channel dimension\n","            # print(f\"decoder with skip connection {i}: ok\", x.shape)            \n","            x = layer(x)            \n","            # print(f\"decoder {i}: ok\", x.shape)\n","        # print(\"decoder ok\")\n","        x = self.final_decoder(x)\n","        x = self.activation(x)\n","        return x"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["device = torch.device(\"cuda\")"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["model = UNet(Z_DIM, 1)\n","model = nn.DataParallel(model)\n","# model.load_state_dict(torch.load(f\"{DATA_DIR}/model.pt\"))\n","model = model.to(device)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Load up the training data"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T09:04:32.407446Z","iopub.status.busy":"2023-05-08T09:04:32.406726Z","iopub.status.idle":"2023-05-08T09:04:32.414915Z","shell.execute_reply":"2023-05-08T09:04:32.413627Z","shell.execute_reply.started":"2023-05-08T09:04:32.407412Z"},"trusted":true},"outputs":[],"source":["def resize(img):\n","    current_width, current_height = img.size\n","    aspect_ratio = current_width / current_height\n","    new_width = int(SHARED_HEIGHT * aspect_ratio)\n","    new_size = (new_width, SHARED_HEIGHT)\n","    img = img.resize(new_size)\n","    return img\n","\n","def load_mask(split, index):\n","    img = Image.open(f\"{DATA_DIR}/{split}/{index}/mask.png\").convert(\"1\")\n","    img = resize(img)\n","    return torch.tensor(np.array(img), dtype=torch.bool)\n","\n","\n","def load_labels(split, index):\n","    img = Image.open(f\"{DATA_DIR}/{split}/{index}/inklabels.png\").convert(\"1\")\n","    img = resize(img)\n","    return torch.tensor(np.array(img), dtype=torch.bool)\n","\n","def load_volume(split, index):\n","    # Load the 3d x-ray scan, one slice at a time\n","    z_slices_fnames = sorted(glob.glob(f\"{DATA_DIR}/{split}/{index}/surface_volume/*.tif\"))[Z_START:Z_START + Z_DIM]\n","    z_slices = []\n","    for z, filename in  tqdm(enumerate(z_slices_fnames)):\n","        img = Image.open(filename).convert(\"1\")\n","        img = resize(img)\n","        z_slice = np.array(img, dtype=\"float32\")\n","        z_slices.append(z_slice)\n","    return np.stack(z_slices, axis=-1)"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T09:04:32.417450Z","iopub.status.busy":"2023-05-08T09:04:32.416496Z","iopub.status.idle":"2023-05-08T09:04:35.551992Z","shell.execute_reply":"2023-05-08T09:04:35.550830Z","shell.execute_reply.started":"2023-05-08T09:04:32.417412Z"},"trusted":true},"outputs":[],"source":["def extract_subvolume(location, volume):\n","    x = location[0]\n","    y = location[1]\n","    subvolume = volume[x-BUFFER:x+BUFFER, y-BUFFER:y+BUFFER, :]\n","    subvolume = torch.from_numpy(subvolume).float() / 65535.\n","    return subvolume\n","\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","from tqdm import tqdm\n","import pathlib\n","\n","def is_in_masked_zone(location, mask):\n","    return mask[location[0], location[1]]\n","\n","def compute_predictions_map(split, index):\n","    print(f\"Load data for {split}/{index}\")\n","\n","    test_volume = load_volume(split=split, index=index)\n","    test_mask = load_mask(split=split, index=index)\n","\n","    test_locations = []\n","    stride = BUFFER // 2\n","    for x in range(BUFFER, test_volume.shape[0] - BUFFER, stride):\n","        for y in range(BUFFER, test_volume.shape[1] - BUFFER, stride):\n","            test_locations.append((x, y))\n","\n","    print(f\"{len(test_locations)} test locations (before filtering by mask)\")\n","\n","    # filter locations inside the mask\n","    test_locations = [loc for loc in test_locations if is_in_masked_zone(loc, test_mask)]\n","\n","    class TestDataset(Dataset):\n","        def __init__(self, test_locations, test_volume):\n","            self.test_locations = test_locations\n","            self.test_volume = test_volume\n","\n","        def __len__(self):\n","            return len(self.test_locations)\n","\n","        def __getitem__(self, idx):\n","            location = torch.tensor(self.test_locations[idx])        \n","            subvolume = extract_subvolume(location, self.test_volume)\n","            subvolume = torch.permute(subvolume, (2, 1, 0))\n","            return location, subvolume\n","\n","    test_ds = TestDataset(test_locations, test_volume)\n","    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n","\n","    predictions_map = np.zeros(test_volume.shape[:2] + (1,), dtype=\"float16\")\n","    predictions_map_counts = np.zeros(test_volume.shape[:2] + (1,), dtype=\"int8\")\n","\n","    print(f\"Compute predictions\")\n","\n","    model.eval()  # set model to evaluation mode\n","    with torch.no_grad():\n","        for loc_batch, patch_batch in tqdm(test_loader):\n","            loc_batch = loc_batch.to(device)\n","            patch_batch = patch_batch.to(device)\n","            predictions = model(patch_batch)\n","            predictions = torch.permute(predictions, (0, 3, 2, 1))\n","            predictions = predictions.cpu().numpy()  # move predictions to cpu and convert to numpy\n","            for (x, y), pred in zip(loc_batch, predictions):\n","                predictions_map[x - BUFFER : x + BUFFER, y - BUFFER : y + BUFFER, :] += pred\n","                predictions_map_counts[x - BUFFER : x + BUFFER, y - BUFFER : y + BUFFER, :] += 1\n","    predictions_map /= (predictions_map_counts + 1e-7)\n","    return predictions_map\n"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["from skimage.transform import resize as resize_ski"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["def rle(predictions_map, threshold):\n","    flat_img = predictions_map.flatten()\n","    flat_img = np.where(flat_img > threshold, 1, 0).astype(np.uint8)\n","\n","    starts = np.array((flat_img[:-1] == 0) & (flat_img[1:] == 1))\n","    ends = np.array((flat_img[:-1] == 1) & (flat_img[1:] == 0))\n","    starts_ix = np.where(starts)[0] + 2\n","    ends_ix = np.where(ends)[0] + 2\n","    lengths = ends_ix - starts_ix\n","    return \" \".join(map(str, sum(zip(starts_ix, lengths), ())))"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["threshold = 0.01\n","\n","# print(\"Id,Predicted\\n\", file=open('/kaggle/working/submission.csv', 'w'))\n","print(\"Id,Predicted\\n\", file=open('submission.csv', 'w'))\n","\n","def update_submission(predictions_map, index):\n","    rle_ = rle(predictions_map, threshold=threshold)\n","    # print(f\"{index},\" + rle_ + \"\\n\", file=open('/kaggle/working/submission.csv', 'a'))\n","    print(f\"{index},\" + rle_ + \"\\n\", file=open('submission.csv', 'a'))"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Load data for test/a\n"]},{"name":"stderr","output_type":"stream","text":["4it [00:00, 15.77it/s]\n"]},{"name":"stdout","output_type":"stream","text":["1155 test locations (before filtering by mask)\n","Compute predictions\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 25/25 [00:05<00:00,  4.63it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Load data for test/b\n"]},{"name":"stderr","output_type":"stream","text":["4it [00:00,  9.15it/s]\n"]},{"name":"stdout","output_type":"stream","text":["525 test locations (before filtering by mask)\n","Compute predictions\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 11/11 [00:00<00:00, 21.20it/s]\n"]}],"source":["folder = pathlib.Path(DATA_DIR) / \"test\"\n","for p in folder.iterdir():\n","    index = p.stem\n","    predictions_map = compute_predictions_map(split=\"test\", index=index)\n","    original_size = Image.open(DATA_DIR + f\"/test/{index}/mask.png\").size\n","    predictions_map = resize_ski(predictions_map, (original_size[1], original_size[0])).squeeze()\n","    update_submission(predictions_map, index)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":4}
