{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Keras starter kit [full training set, UNet]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-10T15:03:39.741181Z","iopub.status.busy":"2023-05-10T15:03:39.740765Z","iopub.status.idle":"2023-05-10T15:03:39.750574Z","shell.execute_reply":"2023-05-10T15:03:39.749341Z","shell.execute_reply.started":"2023-05-10T15:03:39.741144Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","import glob\n","import time\n","import PIL.Image as Image\n","import matplotlib.pyplot as plt\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import Rectangle\n","import matplotlib.patches as patches\n","from tqdm import tqdm\n","import cv2\n","\n","# Data config\n","# DATA_DIR = '/kaggle/input/vesuvius-challenge-ink-detection/'\n","DATA_DIR = '/home/fummicc1/codes/competitions/kaggle-ink-detection'\n","BUFFER = 128  # Half-size of papyrus patches we'll use as model inputs\n","Z_LIST = list(range(0, 65, 3))  # Offset of slices in the z direction\n","Z_DIM = len(Z_LIST)  # Number of slices in the z direction. Max value is 64 - Z_START\n","SHARED_HEIGHT = 4000  # Height to resize all papyrii\n","\n","# (y, x)\n","val_location = (600, 500)\n","val_zone_size = (1000, 2000)\n","\n","# Model config\n","BATCH_SIZE = 128\n","USE_MIXED_PRECISION = False\n","USE_JIT_COMPILE = False\n","\n","device = torch.device(\"cuda\")\n","threshold = 0.35\n","num_workers = 4\n","exp = 1e-7"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from scipy.stats import median_abs_deviation\n","\n","def calculate_MAD(volume):\n","    all_MAD = median_abs_deviation(volume, axis=[0, 1])\n","    return all_MAD\n","    \n","def calculate_median(volume):\n","    all_median = np.median(volume, axis=[0, 1])\n","    return all_median"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["all_median = np.array([19581., 19608., 19635., 19654., 19710., 19863., 20186., 20833.,\n","        21908., 22622., 21038., 18247., 17848., 19475., 21128., 22109.,\n","        22629., 22924., 23099., 23198., 23261., 23297.])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["all_MAD = np.array([12424., 12524., 12643., 12756., 12864., 12935., 13006., 13152.,\n","        13550., 14332., 14870., 12094.,  7500.,  5350.,  4056.,  3408.,\n","         3067.,  2859.,  2725.,  2643.,  2588.,  2557.])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["possible_max_input = ((2 ** 16 - 1) / all_median.min())\n","possible_max_input"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def resize(img):\n","    current_height, current_width = img.shape    \n","    aspect_ratio = current_width / current_height\n","    new_width = int(SHARED_HEIGHT * aspect_ratio)\n","    new_size = (new_width, SHARED_HEIGHT)\n","    # (W, H)の順で渡すが結果は(H, W)になっている\n","    img = cv2.resize(img, new_size)\n","    return img\n","\n","def load_mask(split, index):\n","    img = cv2.imread(f\"{DATA_DIR}/{split}/{index}/mask.png\", 0) // 255\n","    img = resize(img)    \n","    return img\n","\n","\n","def load_labels(split, index):\n","    img = cv2.imread(f\"{DATA_DIR}/{split}/{index}/inklabels.png\", 0) // 255\n","    img = resize(img)\n","    return img\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def load_volume(split, index):\n","    # Load the 3d x-ray scan, one slice at a time\n","    all = sorted(glob.glob(f\"{DATA_DIR}/{split}/{index}/surface_volume/*.tif\"))\n","    z_slices_fnames = [all[i] for i in range(len(all)) if i in Z_LIST]\n","    assert len(z_slices_fnames) == Z_DIM\n","    z_slices = []\n","    for z, filename in  tqdm(enumerate(z_slices_fnames)):\n","        img = cv2.imread(filename, -1)\n","        img = resize(img)\n","        z_slices.append(img)\n","    return np.stack(z_slices, axis=-1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def sample_random_location(shape):\n","    random_train_x = np.random.randint(low=BUFFER, high=shape[1] - BUFFER - 1, size=())\n","    random_train_y = np.random.randint(low=BUFFER, high=shape[0] - BUFFER - 1, size=())\n","    random_train_location = np.stack([random_train_y, random_train_x], axis=-1)\n","    return random_train_location\n","\n","\n","def is_in_masked_zone(location, mask):\n","    return mask[location[0], location[1]] > 0\n","\n","def is_in_val_zone(location, val_location, val_zone_size):\n","    x = location[1]\n","    y = location[0]\n","    x_match = val_location[1] - BUFFER <= x <= val_location[1] + val_zone_size[1] + BUFFER\n","    y_match = val_location[0] - BUFFER <= y <= val_location[0] + val_zone_size[0] + BUFFER\n","    return x_match and y_match\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["printed = False\n","\n","def extract_subvolume(location, volume):\n","    global printed\n","    # print(np.unique(volume, return_counts=True, return_index=True))\n","    x = location[0]\n","    y = location[1]\n","    subvolume = volume[x-BUFFER:x+BUFFER, y-BUFFER:y+BUFFER, :].astype(np.float32)\n","    # print(\"subvolume[:, :, 0]\", subvolume[:, :, 0])\n","    median = np.full_like(subvolume, all_median).astype(np.float32)\n","    MAD = np.full_like(subvolume, all_MAD).astype(np.float32)\n","    # mean = np.mean(subvolume, axis=2)\n","    # mean = np.stack([mean for i in range(Z_DIM)], axis=2) + exp\n","    # MAD = median_abs_deviation(subvolume, axis=2)\n","    # print(\"MAD\", MAD[0, 0, :])\n","    # print(\"mean\", mean)\n","    # print(\"median\", median[0, 0, :])\n","    \n","    subvolume = (subvolume / median)\n","    \n","    if not printed:\n","        print(\"subvolume after taking care of median and MAD\", subvolume)\n","        printed = True\n","    \n","    return subvolume"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.preprocessing import OneHotEncoder\n","\n","class SubvolumeDataset(Dataset):\n","    def __init__(self, locations, volume, labels, buffer, is_train: bool, return_location: bool = False):\n","        self.locations = locations\n","        self.volume = volume\n","        self.labels = labels        \n","        self.buffer = buffer\n","        self.is_train = is_train\n","        self.return_location = return_location\n","\n","    def __len__(self):\n","        return len(self.locations)\n","\n","    def __getitem__(self, idx):\n","        label = None\n","        location = np.array(self.locations[idx])\n","        y, x = location[0], location[1]\n","\n","        subvolume = extract_subvolume(location, self.volume)        \n","        # print(\"subvolume\", subvolume)\n","        # print(\"labels\", labels)\n","        # subvolume = subvolume.numpy()\n","        subvolume = subvolume\n","        \n","        if self.labels is not None:\n","            label = self.labels[y - self.buffer:y + self.buffer, x - self.buffer:x + self.buffer]\n","            # print(\"label\", label)\n","            # n_category = 2\n","            # label = np.eye(n_category)[label]\n","            label = np.stack([label], axis=-1)\n","            # label = label.numpy()\n","            # print(\"label.shape\", label.shape\n","        \n","        if self.is_train and label is not None:            \n","            \n","            # print(\"label\", label.dtype)\n","            # print(\"subvolume in dataset (before aug)\", subvolume)    \n","            size = int(BUFFER * 1.5)\n","            performed = A.Compose([            \n","                A.ToFloat(max_value=possible_max_input),\n","                A.HorizontalFlip(p=0.5), # 水平方向に反転\n","                A.VerticalFlip(p=0.5), # 水平方向に反転\n","                A.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=1, border_mode=0), # シフト、スケーリング、回転\n","                A.PadIfNeeded(min_height=size, min_width=size, always_apply=True, border_mode=0), # 必要に応じてパディングを追加\n","                A.RandomCrop(height=size, width=size, always_apply=True), # ランダムにクロップ, Moduleの中で計算する際に次元がバッチ内で揃っている必要があるので最後にサイズは揃える\n","                # A.GaussNoise(p=0.2), # ガウスノイズを追加　Note: これは背景とinkの境界を曖昧にしてしまうため不適切かもしれない\n","                A.Perspective(p=0.5), # パースペクティブ変換                   \n","                # A.OneOf(\n","                #     [\n","                #         A.Sharpen(p=1),\n","                #         A.Blur(blur_limit=3, p=1),\n","                #         A.MotionBlur(blur_limit=3, p=1),\n","                #     ],\n","                #     p=0.9,\n","                # ),\n","                A.Resize(BUFFER * 2, BUFFER * 2, always_apply=True),\n","                A.FromFloat(max_value=possible_max_input),\n","            ])(image=subvolume, mask=label)            \n","            subvolume = performed[\"image\"]            \n","            label = performed[\"mask\"]\n","            # print(\"subvolume in dataset (after aug)\", subvolume)\n","            # print(\"label\", label.dtype)\n","            # print(\"subvolume\", subvolume.dtype)\n","            # →C, H, W\n","            subvolume = torch.from_numpy(subvolume.transpose(2, 0, 1).astype(np.float64))\n","            # print(performed)\n","            # print(subvolume.shape, label.shape)\n","            # H, W, C → C, H, W\n","            label = torch.from_numpy(label.transpose(2, 0, 1).astype(np.uint8)) \n","        else:\n","            performed = A.Compose([  \n","                A.ToFloat(max_value=possible_max_input),                \n","                # A.Normalize(\n","                #     mean=[mean],\n","                #     std=[std],\n","                # ),\n","                A.FromFloat(max_value=possible_max_input),\n","            ])(image=subvolume)\n","            subvolume = performed[\"image\"]\n","            subvolume = torch.from_numpy(subvolume.transpose(2, 0, 1).astype(np.float64))\n","            if label is not None:\n","                label = torch.from_numpy(label.transpose(2, 0, 1).astype(np.uint8)) \n","        if self.return_location:\n","            return subvolume, location\n","        return subvolume, label        "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class DiceLoss(nn.Module):\n","    def __init__(self, smooth=1e-5):\n","        super(DiceLoss, self).__init__()\n","        self.smooth = smooth\n","\n","    def forward(self, preds, targets):\n","        preds = preds.view(-1)  # Flatten the tensor\n","        targets = targets.view(-1)  # Flatten the tensor\n","\n","        intersection = (preds * targets).sum()\n","        union = preds.sum() + targets.sum()\n","\n","        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n","\n","        return 1 - dice"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class CombinedLoss(nn.Module):\n","    def __init__(self, alpha=0.5, beta=0.5, smooth=1e-5):\n","        super(CombinedLoss, self).__init__()\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.dice_loss = DiceLoss(smooth)\n","        self.bce_loss = nn.BCELoss()\n","\n","    def forward(self, preds, targets):\n","        dice_loss = self.dice_loss(preds, targets)\n","        bce_loss = self.bce_loss(preds, targets)\n","        return self.alpha * dice_loss + self.beta * bce_loss\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-10T15:03:39.754332Z","iopub.status.busy":"2023-05-10T15:03:39.753960Z","iopub.status.idle":"2023-05-10T15:03:39.773134Z","shell.execute_reply":"2023-05-10T15:03:39.772207Z","shell.execute_reply.started":"2023-05-10T15:03:39.754292Z"},"trusted":true},"outputs":[],"source":["\n","class UNet(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(UNet, self).__init__()\n","\n","        def conv_block(in_channels, out_channels):\n","            return nn.Sequential(                \n","                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(out_channels),\n","                nn.ReLU(),\n","            )\n","\n","        def transpose_conv_block(in_channels, out_channels):\n","            return nn.Sequential(                \n","                nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, padding=1),\n","                nn.BatchNorm2d(out_channels),\n","                nn.ReLU(),\n","            )\n","            \n","        self.first_conv = nn.Sequential(\n","            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","        )\n","\n","        self.encoder = nn.ModuleList([\n","            nn.Sequential(\n","                nn.Conv2d(2**(i + 5), 2**(i + 6), kernel_size=3, stride=2, padding=1),\n","                nn.BatchNorm2d(2**(i + 6)),\n","                nn.ReLU(),\n","                nn.Conv2d(2**(i + 6), 2**(i + 6), kernel_size=3, padding=1),\n","                nn.BatchNorm2d(2**(i + 6)),\n","                nn.ReLU(),\n","            )\n","            for i in range(1, 4)\n","        ])\n","\n","\n","        self.middle = nn.Sequential(\n","            conv_block(512, 1024),\n","            conv_block(1024, 512),\n","        )\n","        \n","        self.decoder = nn.ModuleList([\n","            nn.Sequential(\n","                transpose_conv_block(2 ** (i + 7), 2 ** (i + 6)),\n","                transpose_conv_block(2 ** (i + 6), 2 ** (i + 5)),\n","                nn.Upsample(scale_factor=2, mode=\"nearest\"),\n","            )\n","            for i in range(3, 0, -1)\n","        ])\n","        self.final_decoder = nn.Sequential(\n","            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n","            # nn.BatchNorm2d(64),\n","            # nn.ReLU(),\n","            nn.Conv2d(64, out_channels, kernel_size=3, padding=1),\n","            # nn.BatchNorm2d(out_channels),\n","        )\n","        self.activation = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        # print(\"input:\", x)\n","        skip_connections = []\n","        x = self.first_conv(x)\n","        skip_connections.append(x)\n","        for layer in self.encoder:\n","            x = layer(x)\n","            skip_connections.append(x)\n","        # print(\"encoder ok\", x.shape)\n","\n","        x = self.middle(x)\n","        \n","        # print(\"middle ok\", x.shape)\n","        for i, layer in enumerate(self.decoder):                        \n","            skips = skip_connections.pop()\n","            # print(f\"decoder before skip connection: {i}: ok\", x.shape, skips.shape)\n","            x = torch.cat([x, skips], dim=1)  # Concatenate along channel dimension\n","            # print(f\"decoder with skip connection {i}: ok\", x.shape)            \n","            x = layer(x)\n","            # print(f\"decoder {i}: ok\", x.shape)\n","        # print(\"decoder ok\")\n","        x = torch.cat([x, skip_connections[0]], dim=1)\n","        x = self.final_decoder(x)\n","        x = self.activation(x)\n","        # print(\"final out\", x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-10T15:03:39.775244Z","iopub.status.busy":"2023-05-10T15:03:39.774877Z","iopub.status.idle":"2023-05-10T15:03:39.789220Z","shell.execute_reply":"2023-05-10T15:03:39.788194Z","shell.execute_reply.started":"2023-05-10T15:03:39.775205Z"},"trusted":true},"outputs":[],"source":["device = torch.device(\"cuda\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-10T15:03:39.792507Z","iopub.status.busy":"2023-05-10T15:03:39.791249Z","iopub.status.idle":"2023-05-10T15:03:39.864099Z","shell.execute_reply":"2023-05-10T15:03:39.862787Z","shell.execute_reply.started":"2023-05-10T15:03:39.792466Z"},"trusted":true},"outputs":[],"source":["model = UNet(Z_DIM, 1)\n","model = nn.DataParallel(model)\n","# model.load_state_dict(torch.load(f\"/kaggle/input/ink-detection/model.pt\"))\n","model.load_state_dict(torch.load(\"model.pt\"))\n","model = model.to(device)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Load up the training data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-10T15:03:39.881907Z","iopub.status.busy":"2023-05-10T15:03:39.881465Z","iopub.status.idle":"2023-05-10T15:03:39.904353Z","shell.execute_reply":"2023-05-10T15:03:39.903210Z","shell.execute_reply.started":"2023-05-10T15:03:39.881867Z"},"trusted":true},"outputs":[],"source":["\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","from tqdm import tqdm\n","\n","def compute_predictions_map(split, index):\n","    \n","    print(f\"Load data for {split}/{index}\")\n","\n","    test_volume = load_volume(split=split, index=index)\n","    test_mask = load_mask(split=split, index=index)    \n","\n","    test_locations = []\n","    stride = BUFFER // 2\n","    for y in range(BUFFER, test_volume.shape[0] - BUFFER, stride):\n","        for x in range(BUFFER, test_volume.shape[1] - BUFFER, stride):\n","            test_locations.append((y, x))\n","\n","    print(f\"{len(test_locations)} test locations (before filtering by mask)\")\n","\n","    # filter locations inside the mask\n","    test_locations = [loc for loc in test_locations if is_in_masked_zone(loc, test_mask)]\n","    \n","    print(f\"{len(test_locations)} test locations (after filtering by mask)\")\n","\n","    test_ds = SubvolumeDataset(test_locations, test_volume, None, BUFFER, is_train=False, return_location=True)\n","    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, num_workers=num_workers)\n","\n","    # shape: (X, Y, C)\n","    predictions_map = np.zeros_like(test_volume[:, :, 0]).transpose((1, 0))[:, :, np.newaxis].astype(np.float64)\n","    \n","    print(\"test_volume.shape\", test_volume.shape)\n","    print(\"predictions_map.shape\", predictions_map.shape)\n","\n","    print(f\"Compute predictions\")\n","\n","    model.eval()  # set model to evaluation mode\n","    with torch.no_grad():    \n","        for patch_batch, loc_batch in tqdm(test_loader):\n","            loc_batch = loc_batch.to(device).long()\n","            patch_batch = patch_batch.to(device).float()\n","            predictions = model(patch_batch)\n","            # print(\"predictions\", predictions)\n","            # print(\"predictions\", predictions[:, :, 0, 0])\n","            # print(\"Softmaxed predictions where conf is gt threshold\", predictions[predictions.gt(threshold)])\n","            # →(BATCH, W, H, C)\n","            predictions = torch.permute(predictions, (0, 3, 2, 1))\n","            predictions = predictions.cpu().numpy()  # move predictions to cpu and convert to numpy\n","            for (y, x), pred in zip(loc_batch, predictions):\n","                # print(\"index: \", index ,\"x, y, pred\", x.item(), y.item(), pred[BUFFER, BUFFER, :].item(), file=open('log.out', 'a'))\n","                predictions_map[\n","                    x - BUFFER : x + BUFFER, y - BUFFER : y + BUFFER, :\n","                ][pred > threshold] = 1\n","    print(\"predictions_map\", predictions_map, file=open(\"predictions_map\", \"w\"))\n","    return predictions_map\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-10T15:03:39.906477Z","iopub.status.busy":"2023-05-10T15:03:39.905840Z","iopub.status.idle":"2023-05-10T15:03:39.916814Z","shell.execute_reply":"2023-05-10T15:03:39.915804Z","shell.execute_reply.started":"2023-05-10T15:03:39.906254Z"},"trusted":true},"outputs":[],"source":["from skimage.transform import resize as resize_ski\n","import pathlib"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-10T15:03:39.918543Z","iopub.status.busy":"2023-05-10T15:03:39.918095Z","iopub.status.idle":"2023-05-10T15:03:39.929658Z","shell.execute_reply":"2023-05-10T15:03:39.928498Z","shell.execute_reply.started":"2023-05-10T15:03:39.918498Z"},"trusted":true},"outputs":[],"source":["def rle(predictions_map, threshold):\n","    flat_img = (np.where(predictions_map.flatten() >= threshold, 1, 0)).astype(np.uint8)\n","    \n","    # Add padding at the beginning and end\n","    flat_img = np.pad(flat_img, pad_width=1, mode='constant', constant_values=0)\n","\n","    starts = np.where((flat_img[:-1] == 0) & (flat_img[1:] == 1))[0]\n","    ends = np.where((flat_img[:-1] == 1) & (flat_img[1:] == 0))[0]\n","\n","    lengths = ends - starts\n","\n","    return \" \".join(map(str, np.c_[starts, lengths].flatten()))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-10T15:03:39.933503Z","iopub.status.busy":"2023-05-10T15:03:39.930806Z","iopub.status.idle":"2023-05-10T15:03:39.945903Z","shell.execute_reply":"2023-05-10T15:03:39.944714Z","shell.execute_reply.started":"2023-05-10T15:03:39.933458Z"},"trusted":true},"outputs":[],"source":["def update_submission(predictions_map, index):\n","    rle_ = rle(predictions_map, threshold=threshold)\n","    print(f\"{index},\" + rle_, file=open('submission.csv', 'a'))\n","    # print(f\"{index},\" + rle_, file=open('/kaggle/working/submission.csv', 'a'))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-10T15:03:39.948169Z","iopub.status.busy":"2023-05-10T15:03:39.947490Z"},"trusted":true},"outputs":[],"source":["print(\"Id,Predicted\", file=open('submission.csv', 'w'))\n","kind = \"test\"\n","folder = pathlib.Path(DATA_DIR) / kind\n","for p in list(folder.iterdir()):\n","    index = p.stem\n","    predictions_map = compute_predictions_map(split=kind, index=index)\n","    original_size = cv2.imread(DATA_DIR + f\"/{kind}/{index}/mask.png\", 0).shape[:2]\n","    # W, H, C → H, W, C\n","    predictions_map = predictions_map.transpose((1, 0, 2))    \n","    predictions_map = resize_ski(predictions_map, (original_size[0], original_size[1], 1)).squeeze(axis=-1)    \n","    print(\"original predictions_map size\", predictions_map.shape)    \n","    # H, W → W, H\n","    update_submission(predictions_map, index)\n","    plt.imsave(f\"{index}.png\", predictions_map, cmap=\"gray\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":4}
