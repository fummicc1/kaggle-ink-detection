{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#!/usr/bin/env python\n","# coding: utf-8"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["for dicsussion, refer to https://www.kaggle.com/competitions/vesuvius-challenge-ink-detection/discussion/407972#2272189"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In[19]:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import sys\n","\n","# sys.path.append('/kaggle/input/ink-00/my_lib')\n","# sys.path.append('/kaggle/input/pretrainedmodels/pretrainedmodels-0.7.4')\n","# sys.path.append('/kaggle/input/efficientnet-pytorch/EfficientNet-PyTorch-master')\n","# sys.path.append('/kaggle/input/timm-pytorch-image-models/pytorch-image-models-master')\n","# sys.path.append('/kaggle/input/segmentation-models-pytorch/segmentation_models.pytorch-master')\n","# sys.path.append('/kaggle/input/einops/einops-master')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In[20]:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import hashlib\n","import numpy as np\n","import pandas as pd\n","from dotdict import dotdict\n","from time import time"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from collections import defaultdict\n","from glob import glob\n","import PIL.Image as Image\n","Image.MAX_IMAGE_PIXELS = 10000000000  # Ignore PIL warnings about large images"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import cv2"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from einops import rearrange, reduce, repeat\n","import segmentation_models_pytorch as smp\n","from segmentation_models_pytorch.decoders.unet.decoder import UnetDecoder, DecoderBlock\n","from timm.models.resnet import resnet10t, resnet34d"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torchvision\n","import datetime\n","# import cupy\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","import pytorch_lightning\n","import segmentation_models_pytorch as smp\n","import pytorch_lightning as pl\n","import pytorch_lightning.plugins\n","from skimage.transform import resize as resize_ski\n","from pytorch_lightning.strategies.ddp import DDPStrategy\n","from pytorch_lightning.loggers import WandbLogger\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from scipy.ndimage import distance_transform_edt"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import glob\n","import time\n","import PIL.Image as Image\n","import matplotlib.pyplot as plt\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import Rectangle\n","import matplotlib.patches as patches\n","from sklearn.model_selection import KFold\n","from tqdm import tqdm\n","import cv2\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib\n","import matplotlib.pyplot as plt"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In[21]:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def time_to_str(time):\n","\th = time // 3600\n","\tm = (time % 3600) // 60\n","\ts = time % 60\n","\treturn f\"{h:02f}, {m:02f}, {s:02f}\""]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In[22]:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Config(object):\n","\tmode = [\n","\t\t'train', #\n","\t\t# 'test', 'skip_fake_test',\n","\t]\n","\tcrop_fade  = 56\n","\tcrop_size  = 384\n","\tcrop_depth = 5\n","\tinfer_fragment_z = [28, 37]\n","\tthreshold = 0.5\n","\tlr = 1e-4\n","\tbatch_size = 32\n","\tnum_workers = 8\n","\tepochs = 20"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["CFG = Config()\n","CFG.is_tta = True #True"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if 'train' in CFG.mode:\n","\tCFG.stride = CFG.crop_size//2\n","if 'test' in CFG.mode:\n","\tCFG.stride = CFG.crop_size//2"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def cfg_to_text():\n","\td = Config.__dict__\n","\ttext = [f'\\t{k} : {v}' for k,v in d.items() if not (k.startswith('__') and k.endswith('__'))]\n","\td = CFG.__dict__\n","\ttext += [f'\\t{k} : {v}' for k,v in d.items() if not (k.startswith('__') and k.endswith('__'))]\n","\treturn 'CFG\\n'+'\\n'.join(text)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In[ ]:"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":[" dataset ##"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["--"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def do_binarise(m, threshold=0.5):\n","\tm = m-m.min()\n","\tm = m/(m.max()+1e-7)\n","\tm = (m>threshold).astype(np.float32)\n","\treturn m"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def read_data(fragment_id, z0=CFG.infer_fragment_z[0], z1=CFG.infer_fragment_z[1]):\n","\tvolume = []\n","\tstart_timer = time.time()\n","\tfor i in range(z0,z1):\n","\t\tv = np.array(Image.open(f'{data_dir}/{fragment_id}/surface_volume/{i:02d}.tif'), dtype=np.uint16)\n","\t\tv = (v >> 8).astype(np.uint8)\n","\t\t#v = (v / 65535.0 * 255).astype(np.uint8)\n","\t\tvolume.append(v)\n","\t\tprint(f'\\r @ read_data(): volume{fragment_id}  {time_to_str(time.time() - start_timer)}', end='', flush=True)\n","\t#print('')\n","\tvolume = np.stack(volume, -1)\n","\theight, width, depth = volume.shape\n","\t#print(f'fragment_id={fragment_id} volume: {volume.shape}')\n","\n","\t#---\n","\tmask = cv2.imread(f'{data_dir}/{fragment_id}/mask.png',cv2.IMREAD_GRAYSCALE)\n","\tmask = do_binarise(mask)\n","\n","\tif 'train' in CFG.mode:\n","\t\tir    = cv2.imread(f'{data_dir}/{fragment_id}/ir.png',cv2.IMREAD_GRAYSCALE)\n","\t\tlabel = cv2.imread(f'{data_dir}/{fragment_id}/inklabels.png',cv2.IMREAD_GRAYSCALE)\n","\t\tir    = ir/255\n","\t\tlabel = do_binarise(label)\n","\n","\tif 'test' in CFG.mode:\n","\t\tir = None\n","\t\tlabel = None\n","\n","\td = dotdict(\n","\t\tfragment_id = fragment_id,\n","\t\tvolume = volume,\n","\t\tir     = ir,\n","\t\tlabel  = label,\n","\t\tmask   = mask,\n","\t)\n","\treturn d"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def read_data1(fragment_id):\n","\tif fragment_id=='2a':\n","\t\ty = 9456\n","\t\td = read_data('2') \n","\t\td = dotdict(\n","\t\t\tfragment_id='2a',\n","\t\t\tvolume  = d.volume[:y],\n","\t\t\tir      = d.ir[:y],\n","\t\t\tlabel   = d.label[:y],\n","\t\t\tmask    = d.mask[:y],\n","\t\t)\n","\telif  fragment_id=='2b':\n","\t\ty = 9456\n","\t\td = read_data('2') \n","\t\td = dotdict(\n","\t\t\tfragment_id='2b',\n","\t\t\tvolume  = d.volume[y:],\n","\t\t\tir      = d.ir[y:],\n","\t\t\tlabel   = d.label[y:],\n","\t\t\tmask    = d.mask[y:],\n","\t\t)\n","\telse:\n","\t\td = read_data(fragment_id)\n","\treturn d"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","def load_mask(split, index):\n","\timg = cv2.imread(f\"{data_dir}/{split}/{index}/mask.png\", 0) // 255\n","\treturn img\n","\n","\n","def load_labels(split, index):\n","\timg = cv2.imread(f\"{data_dir}/{split}/{index}/inklabels.png\", 0) // 255\n","\treturn img\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ref - https://www.kaggle.com/competitions/vesuvius-challenge-ink-detection/discussion/397288\n","def fbeta_score(preds, targets, threshold, beta=0.5, smooth=1e-5):\n","\tpreds_t = torch.where(preds > threshold, 1.0, 0.0).float()\n","\ty_true_count = targets.sum()\n","\n","\tctp = preds_t[targets == 1].sum()\n","\tcfp = preds_t[targets == 0].sum()\n","\tbeta_squared = beta * beta\n","\n","\tc_precision = ctp / (ctp + cfp + smooth)\n","\tc_recall = ctp / (y_true_count + smooth)\n","\tdice = (\n","\t\t(1 + beta_squared)\n","\t\t* (c_precision * c_recall)\n","\t\t/ (beta_squared * c_precision + c_recall + smooth)\n","\t)\n","\n","\treturn dice\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def run_check_data():\n","\td=read_data1(valid_id[0])#valid_id[0]\n","\tprint('')\n","\tprint('fragment_id:', d.fragment_id)\n","\tprint('volume:', d.volume.shape, d.volume.min(), d.volume.max())\n","\tprint('mask  :', d.mask.shape, d.mask.min(), d.mask.max())\n","\tif 'train' in CFG.mode:\n","\t\tprint('ir    :', d.ir.shape, d.ir.min(), d.ir.max())\n","\t\tprint('label :', d.label.shape, d.label.min(), d.label.max())"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["un_check_data()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["ref - https://www.kaggle.com/competitions/vesuvius-challenge-ink-detection/discussion/397288"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In[ ]:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def extract(location, volume):\n","\tglobal printed\n","\tx = location[0]\n","\ty = location[1]\n","\tsubvolume = volume[y: y + CFG.crop_size, x : x + CFG.crop_size, :].astype(\n","\t\tnp.float32\n","\t)\n","\treturn subvolume"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In[ ]:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.preprocessing import OneHotEncoder"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from albumentations.core.transforms_interface import ImageOnlyTransform"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def run_check_net():\n","\theight, width = CFG.crop_size, CFG.crop_size\n","\tdepth = CFG.infer_fragment_z[1] - CFG.infer_fragment_z[0]\n","\tbatch_size = 3"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# x = np.arange(0,3)\n","# y = np.arange(0,4)\n","# x,y = np.meshgrid(x,y)\n","# xy  = np.stack([x,y],-1).reshape(-1,2)\n","# x, y, xy"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.preprocessing import OneHotEncoder\n","\n","from albumentations.core.transforms_interface import ImageOnlyTransform\n","\n","\n","class SubvolumeDataset(Dataset):\n","\tdef __init__(\n","\t\tself,\n","\t\tlocations,\n","\t\tvolume,\n","\t\tlabels,\n","\t\tbuffer,\n","\t\tis_train: bool,\n","\t\treturn_location: bool = False,\n","\t):\n","\t\tself.locations = locations\n","\t\tself.volume = volume\n","\t\tself.labels = labels\n","\t\tself.buffer = buffer\n","\t\tself.is_train = is_train\n","\t\tself.return_location = return_location\n","\n","\tdef __len__(self):\n","\t\treturn len(self.locations)\n","\n","\tdef __getitem__(self, idx):\n","\t\tlabel = None\n","\t\tlocation = np.array(self.locations[idx])\n","\t\tx, y = location[0], location[1]\n","\n","\t\tsubvolume = extract(location, self.volume)\n","\n","\t\tif self.labels is not None:\n","\t\t\tlabel = self.labels[\n","\t\t\t\ty : y + self.buffer * 2, x: x + self.buffer * 2\n","\t\t\t]\n","\t\t\tlabel = np.stack([label], axis=-1)\n","\n","\t\tif self.is_train and label is not None:\n","\t\t\ttransformed = A.Compose(\n","\t\t\t\t[\n","\t\t\t\t\tA.HorizontalFlip(p=0.5),\n","\t\t\t\t\tA.VerticalFlip(p=0.5),\n","\t\t\t\t\tA.Transpose(p=0.5),\n","\t\t\t\t\tA.RandomScale(p=0.5),\n","\t\t\t\t\tA.RandomRotate90(p=0.5),\n","\t\t\t\t\tA.ShiftScaleRotate(p=0.5),\n","\t\t\t\t\tA.Resize(height=self.buffer * 2, width=self.buffer * 2),\n","\t\t\t\t]\n","\t\t\t)(image=subvolume, mask=label)\n","\t\t\tsubvolume = transformed[\"image\"]\n","\t\t\tlabel = transformed[\"mask\"]\n","\t\t\tsubvolume = np.transpose(subvolume, (2, 0, 1))\n","\t\t\tlabel = np.transpose(label, (2, 0, 1))\n","\t\t\tsubvolume /= 255.0\n","\t\t\tsubvolume = (subvolume - 0.45) / 0.225\n","\t\telse:\n","\t\t\t# print(\"subvolume in val dataset (before aug)\", subvolume, file=open(\"before-val-aug.log\", \"w\"))\n","\t\t\tsubvolume = np.transpose(subvolume, (2, 0, 1))\n","\t\t\tlabel = np.transpose(label, (2, 0, 1))\n","\t\t\tsubvolume /= 255.0\n","\t\t\tsubvolume = (subvolume - 0.45) / 0.225\n","\t\td = {\n","\t\t\t\"volume\": subvolume,\n","   \t\t\t\"label\": label,\n","\t\t}\n","\t\tif len(subvolume.shape) < 3:\n","\t\t\tprint(\"shhape 2 location:\", location)\n","\t\telif subvolume.shape[1] != CFG.crop_size or subvolume.shape[2] != CFG.crop_size:\n","\t\t\tprint(\"location()\", location)\n","\t\treturn d\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def collate_fn(batch):\n","    keys = batch[0].keys()\n","\n","    collated_batch = {}\n","    for key in keys:\n","        if batch[0][key] is not None:\n","            collated_batch[key] = torch.stack([torch.from_numpy(sample[key]) for sample in batch])\n","        else:\n","            collated_batch[key] = None\n","\n","    return collated_batch\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class SmpUnetDecoder(nn.Module):\n","\tdef __init__(\n","\t\tself,\n","\t\tin_channel,\n","\t\tskip_channel,\n","\t\tout_channel,\n","\t):\n","\t\tsuper().__init__()\n","\t\tself.center = nn.Identity()\n","\n","\t\ti_channel = [\n","\t\t\tin_channel,\n","\t\t] + out_channel[:-1]\n","\t\ts_channel = skip_channel\n","\t\to_channel = out_channel\n","\t\tblock = [\n","\t\t\tDecoderBlock(i, s, o, use_batchnorm=True, attention_type=None)\n","\t\t\tfor i, s, o in zip(i_channel, s_channel, o_channel)\n","\t\t]\n","\t\tself.block = nn.ModuleList(block)\n","\n","\tdef forward(self, feature, skip):\n","\t\td = self.center(feature)\n","\t\tdecode = []\n","\t\tfor i, block in enumerate(self.block):\n","\t\t\ts = skip[i]\n","\t\t\td = block(d, s)\n","\t\t\tdecode.append(d)\n","\n","\t\tlast = d\n","\t\treturn last, decode\n","\n","\n","class Net(nn.Module):\n","\tdef __init__(\n","\t\tself,\n","\t):\n","\t\tsuper().__init__()\n","\t\tself.output_type = [\"inference\", \"loss\"]\n","\n","\t\tconv_dim = 64\n","\t\tencoder1_dim = [\n","\t\t\tconv_dim,\n","\t\t\t64,\n","\t\t\t128,\n","\t\t\t256,\n","\t\t\t512,\n","\t\t]\n","\t\tdecoder1_dim = [\n","\t\t\t256,\n","\t\t\t128,\n","\t\t\t64,\n","\t\t\t64,\n","\t\t]\n","\n","\t\tself.encoder1 = resnet34d(pretrained=False, in_chans=CFG.crop_depth)\n","\n","\t\tself.decoder1 = SmpUnetDecoder(\n","\t\t\tin_channel=encoder1_dim[-1],\n","\t\t\tskip_channel=encoder1_dim[:-1][::-1],\n","\t\t\tout_channel=decoder1_dim,\n","\t\t)\n","\t\t# -- pool attention weight\n","\t\tself.weight1 = nn.ModuleList(\n","\t\t\t[\n","\t\t\t\tnn.Sequential(\n","\t\t\t\t\tnn.Conv2d(dim, dim, kernel_size=3, padding=1),\n","\t\t\t\t\tnn.ReLU(inplace=True),\n","\t\t\t\t)\n","\t\t\t\tfor dim in encoder1_dim\n","\t\t\t]\n","\t\t)\n","\t\tself.logit1 = nn.Conv2d(decoder1_dim[-1], 1, kernel_size=1)\n","\n","\t\t# --------------------------------\n","\t\t#\n","\t\tencoder2_dim = [64, 128, 256, 512]  #\n","\t\tdecoder2_dim = [\n","\t\t\t128,\n","\t\t\t64,\n","\t\t\t32,\n","\t\t]\n","\t\tself.encoder2 = resnet10t(pretrained=False, in_chans=decoder1_dim[-1])\n","\n","\t\tself.decoder2 = SmpUnetDecoder(\n","\t\t\tin_channel=encoder2_dim[-1],\n","\t\t\tskip_channel=encoder2_dim[:-1][::-1],\n","\t\t\tout_channel=decoder2_dim,\n","\t\t)\n","\t\tself.logit2 = nn.Conv2d(decoder2_dim[-1], 1, kernel_size=1)\n","\n","\tdef forward(self, batch):\n","\t\tv = batch\n","\t\tB, C, H, W = v.shape\n","\t\tvv = [\n","\t\t\tv[:, i : i + CFG.crop_depth]\n","\t\t\tfor i in [\n","\t\t\t\t0,\n","\t\t\t\t2,\n","\t\t\t\t4,\n","\t\t\t]\n","\t\t]\n","\t\tK = len(vv)\n","\t\tx = torch.cat(vv, 0)\n","\t\t# x = v\n","\n","\t\t# ----------------------\n","\t\tencoder = []\n","\t\te = self.encoder1\n","\t\tx = e.conv1(x)\n","\t\tx = e.bn1(x)\n","\t\tx = e.act1(x)\n","\t\tencoder.append(x)\n","\t\tx = F.avg_pool2d(x, kernel_size=2, stride=2)\n","\t\tx = e.layer1(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer2(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer3(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer4(x)\n","\t\tencoder.append(x)\n","\t\t# print('encoder', [f.shape for f in encoder])\n","\n","\t\tfor i in range(len(encoder)):\n","\t\t\te = encoder[i]\n","\t\t\tf = self.weight1[i](e)\n","\t\t\t_, c, h, w = e.shape\n","\t\t\tf = rearrange(f, \"(K B) c h w -> B K c h w\", K=K, B=B, h=h, w=w)  #\n","\t\t\te = rearrange(e, \"(K B) c h w -> B K c h w\", K=K, B=B, h=h, w=w)  #\n","\t\t\tw = F.softmax(f, 1)\n","\t\t\te = (w * e).sum(1)\n","\t\t\tencoder[i] = e\n","\n","\t\tfeature = encoder[-1]\n","\t\tskip = encoder[:-1][::-1]\n","\t\tlast, decoder = self.decoder1(feature, skip)\n","\t\tlogit1 = self.logit1(last)\n","\n","\t\t# ----------------------\n","\t\tx = last  # .detach()\n","\t\t# x = F.avg_pool2d(x,kernel_size=2,stride=2)\n","\t\tencoder = []\n","\t\te = self.encoder2\n","\t\tx = e.layer1(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer2(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer3(x)\n","\t\tencoder.append(x)\n","\t\tx = e.layer4(x)\n","\t\tencoder.append(x)\n","\n","\t\tfeature = encoder[-1]\n","\t\tskip = encoder[:-1][::-1]\n","\t\tlast, decoder = self.decoder2(feature, skip)\n","\t\tlogit2 = self.logit2(last)\n","\t\tlogit2 = F.interpolate(\n","\t\t\tlogit2, size=(H, W), mode=\"bilinear\", align_corners=False, antialias=True\n","\t\t)\n","  \n","\t\tlogit1 = F.interpolate(\n","\t\t\tlogit1, size=(H, W), mode=\"bilinear\", align_corners=False, antialias=True\n","\t\t)\n","\n","\t\toutput = {\n","\t\t\t\"logit1\": logit1,\n","\t\t\t\"logit2\": logit2,\n","\t\t}\n","\t\treturn output\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In[ ]:"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### infer here !!!!<br>\n","https://gist.github.com/janpaul123/ca3477c1db6de4346affca37e0e3d5b0"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def mask_to_rle(mask):\n","\tm = mask.reshape(-1)\n","\t# m = np.where(mask > threshold, 1, 0).astype(np.uint8)\n","\n","\ts = np.array((m[:-1] == 0) & (m[1:] == 1))\n","\te = np.array((m[:-1] == 1) & (m[1:] == 0))\n","\n","\ts_index = np.where(s)[0] + 2\n","\te_index = np.where(e)[0] + 2\n","\tlength = e_index - s_index\n","\trle = \" \".join(map(str, sum(zip(s_index, length), ())))\n","\treturn rle\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In[ ]:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","def metric_to_text(ink, label, mask):\n","\ttext = []\n","\n","\tp = ink.reshape(-1)\n","\tt = label.reshape(-1)\n","\tpos = np.log(np.clip(p, 1e-7, 1))\n","\tneg = np.log(np.clip(1 - p, 1e-7, 1))\n","\tbce = -(t * pos + (1 - t) * neg).mean()\n","\ttext.append(f\"bce={bce:0.5f}\")\n","\n","\tmask_sum = mask.sum()\n","\t# print(f'{threshold:0.1f}, {precision:0.3f}, {recall:0.3f}, {fpr:0.3f},  {dice:0.3f},  {score:0.3f}')\n","\ttext.append(\"p_sum  th   prec   recall   fpr   dice   score\")\n","\ttext.append(\"-----------------------------------------------\")\n","\tfor threshold in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n","\t\tp = ink.reshape(-1)\n","\t\tt = label.reshape(-1)\n","\t\tp = (p > threshold).astype(np.float32)\n","\t\tt = (t > 0.5).astype(np.float32)\n","\n","\t\ttp = p * t\n","\t\tprecision = tp.sum() / (p.sum() + 0.0001)\n","\t\trecall = tp.sum() / t.sum()\n","\n","\t\tfp = p * (1 - t)\n","\t\tfpr = fp.sum() / (1 - t).sum()\n","\n","\t\tbeta = 0.5\n","\t\t#  0.2*1/recall + 0.8*1/prec\n","\t\tscore = (\n","\t\t\tbeta * beta / (1 + beta * beta) * 1 / recall\n","\t\t\t+ 1 / (1 + beta * beta) * 1 / precision\n","\t\t)\n","\t\tscore = 1 / score\n","\n","\t\tdice = 2 * tp.sum() / (p.sum() + t.sum())\n","\t\tp_sum = p.sum() / mask_sum\n","\n","\t\t# print(fold, threshold, precision, recall, fpr,  score)\n","\t\ttext.append(\n","\t\t\tf\"{p_sum:0.2f}, {threshold:0.2f}, {precision:0.3f}, {recall:0.3f}, {fpr:0.3f},  {dice:0.3f},  {score:0.3f}\"\n","\t\t)\n","\ttext = \"\\n\".join(text)\n","\treturn text"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In[ ]:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def make_infer_mask():\n","\ts = CFG.crop_size\n","\tf = CFG.crop_fade\n","\tx = np.linspace(-1, 1, s)\n","\ty = np.linspace(-1, 1, s)\n","\txx, yy = np.meshgrid(x, y)\n","\td = 1 - np.maximum(np.abs(xx), np.abs(yy))\n","\td1 = np.clip(d, 0, f / s * 2)\n","\td1 = d1 / d1.max()\n","\tinfer_mask = d1\n","\treturn infer_mask"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In[ ]:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","class Model(pl.LightningModule):\n","\ttraining_step_outputs = []\n","\tvalidation_step_outputs = []\n","\ttest_step_outputs = [[], []]\n","\n","\tdef __init__(self):\n","\t\tsuper().__init__()\n","\n","\t\tself.model = Net()\n","\n","\t\tself.loss1 = nn.BCEWithLogitsLoss(pos_weight=0.5)\n","\t\tself.loss2 = nn.BCEWithLogitsLoss(pos_weight=0.5)\n","\t\t\n","\n","\tdef forward(self, image, stage):\n","\t\tmask = self.model(image)\n","\t\treturn mask\n","\n","\tdef shared_step(self, batch, stage):\n","\t\tsubvolumes, labels = batch[\"volume\"], batch[\"label\"]\n","\n","\t\timage, labels = subvolumes.float(), labels.float()\n","\t\tassert image.ndim == 4\n","\n","\t\th, w = image.shape[2:]\n","\t\tassert h % 32 == 0 and w % 32 == 0\n","\n","\t\t# print(\"labels\", labels.max(), labels.min())\n","\n","\t\tassert labels.max() <= 1.0 and labels.min() >= 0\n","\n","\t\tsegmentation_out = self.forward(image, stage)\n","\n","\t\tloss = self.loss1(segmentation_out[\"logit1\"], labels) + self.loss2(segmentation_out[\"logit2\"], labels)\n","  \n","\t\tprob = segmentation_out[\"logit2\"].sigmoid()\n","\t\t\n","\t\tscore = fbeta_score(prob, labels, CFG.threshold)\n","\n","\t\tpred_mask = (prob > CFG.threshold).float()\n","\n","\t\ttp, fp, fn, tn = smp.metrics.get_stats(\n","\t\t\tpred_mask.long(), labels.long(), mode=\"binary\"\n","\t\t)\n","\n","\t\tm = {\n","\t\t\t\"loss\": loss,        \n","\t\t\t\"tp\": tp,\n","\t\t\t\"fp\": fp,\n","\t\t\t\"fn\": fn,\n","\t\t\t\"tn\": tn,    \n","\t\t\t\"score\": score,\n","\t\t}\n","\t\treturn m\n","\n","\tdef shared_epoch_end(self, outputs, stage):\n","\t\t# aggregate step metics\n","\t\ttp = torch.cat([x[\"tp\"] for x in outputs])\n","\t\tfp = torch.cat([x[\"fp\"] for x in outputs])\n","\t\tfn = torch.cat([x[\"fn\"] for x in outputs])\n","\t\ttn = torch.cat([x[\"tn\"] for x in outputs])\n","\t\tloss = torch.mean(torch.Tensor([x[\"loss\"] for x in outputs]))\n","\t\tloss = torch.mean(torch.Tensor([x[\"loss\"] for x in outputs]))\n","\t\tfbeta_score = torch.mean(torch.Tensor([x[\"score\"] for x in outputs]))\n","\t\t\n","\t\tper_image_iou = smp.metrics.iou_score(\n","\t\t\ttp, fp, fn, tn, reduction=\"micro-imagewise\"\n","\t\t)\n","\n","\t\tdataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n","\t\t\n","\t\tmetrics = {\n","\t\t\tf\"{stage}_per_image_iou\": per_image_iou,\n","\t\t\tf\"{stage}_dataset_iou\": dataset_iou,\n","\t\t\tf\"{stage}_loss\": loss.item(),\n","\t\t\tf\"{stage}_tp\": tp.sum().int().item(),\n","\t\t\tf\"{stage}_fp\": fp.sum().int().item(),\n","\t\t\tf\"{stage}_fn\": fn.sum().int().item(),\n","\t\t\tf\"{stage}_tn\": tn.sum().int().item(),\n","\t\t\tf\"{stage}_score\": fbeta_score.item(),\n","\t\t\tf\"{stage}_score\": fbeta_score.item(),\n","\t\t}\n","\n","\t\tself.log_dict(metrics, prog_bar=True, sync_dist=True)\n","\n","\tdef training_step(self, batch, batch_idx):\n","\t\tout = self.shared_step(batch, \"train\")\n","\t\tself.training_step_outputs.append(out)\n","\t\treturn out\n","\n","\tdef on_train_epoch_end(self):\n","\t\tout = self.shared_epoch_end(self.training_step_outputs, \"train\")\n","\t\tself.training_step_outputs.clear()\n","\t\treturn out\n","\n","\tdef validation_step(self, batch, batch_idx):\n","\t\tout = self.shared_step(batch, \"valid\")\n","\t\tself.validation_step_outputs.append(out)\n","\t\treturn out\n","\n","\tdef on_validation_epoch_end(self):\n","\t\tout = self.shared_epoch_end(self.validation_step_outputs, \"valid\")\n","\t\tself.validation_step_outputs.clear()\n","\t\treturn out\n","\n","\tdef test_step(self, batch, batch_idx):\n","\t\tpass\n","\n","\tdef on_test_epoch_end(self):\n","\t\tpass\n","\n","\tdef configure_optimizers(self):\n","\t\toptimizer = optim.AdamW(self.parameters(), lr=CFG.lr)\n","\n","\t\tscheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","\t\t\toptimizer,\n","\t\t\tmode=\"min\",\n","\t\t\tfactor=0.05,\n","\t\t\tpatience=5,\n","\t\t)\n","\t\treturn {\n","\t\t\t\"optimizer\": optimizer,\n","\t\t\t\"lr_scheduler\": {\"scheduler\": scheduler, \"monitor\": \"valid_loss\"},\n","\t\t}\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In[ ]:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","def train_one(d, val_d):\n","\tnet = Model()\n","\n","\t#get coord\n","\tcrop_size  = CFG.crop_size\n","\tstride = CFG.stride\n","\tH,W,D  = d.volume.shape\n","\n","\t##pad #assume H,W >size\n","\tpx, py = W % stride, H % stride\n","\tif (px != 0) or (py != 0):\n","\t\tpx = stride - px\n","\t\tpy = stride - py\n","\t\tpad_volume = np.pad(d.volume, [(0, py), (0, px), (0, 0)], constant_values=0)\n","\t\tpad_label = np.pad(d.label, [(0, py), (0, px)], constant_values=0)\n","\telse:\n","\t\tpad_volume = d.volume\n","\t\tpad_label = d.label\n","\n","\tpH, pW, _  = pad_volume.shape\n","\tx = np.arange(0, pW-crop_size + 1,stride)\n","\ty = np.arange(0, pH-crop_size + 1,stride)\n","\tx,y = np.meshgrid(x,y)\n","\txy  = np.stack([x,y],-1).reshape(-1,2)\n","\tprint('H,W,pH,pW,len(xy)',H,W,pH,pW,len(xy))\n"," \n","\tval_H,val_W,val_D  = val_d.volume.shape\n","\n","\t##pad #assume H,W >size\n","\tval_px, val_py = val_W % stride, val_H % stride\n","\tif (val_px != 0) or (val_py != 0):\n","\t\tval_px = stride - val_px\n","\t\tval_py = stride - val_py\n","\t\tpad_val_volume = np.pad(val_d.volume, [(0, val_py), (0, val_px), (0, 0)], constant_values=0)\n","\t\tpad_val_label = np.pad(val_d.label, [(0, val_py), (0, val_px)], constant_values=0)\n","\telse:\n","\t\tpad_val_volume = val_d.volume\n","\t\tpad_val_label = val_d.label\n","\n","\tval_pH, val_pW, _  = pad_val_volume.shape\n","\tval_x = np.arange(0, val_pW - crop_size + 1,stride)\n","\tval_y = np.arange(0, val_pH - crop_size + 1,stride)\n","\tval_x,val_y = np.meshgrid(val_x,val_y)\n","\tval_xy  = np.stack([val_x,val_y],-1).reshape(-1,2)\n"," \n","\tprint('val_H,val_W,val_pH,val_pW,len(val_xy)',val_H,val_W,val_pH,val_pW,len(val_xy))\n","\n","\ttrain_ds = SubvolumeDataset(\n","\t\tlocations=xy,\n","\t\tvolume=pad_volume,\n","\t\tlabels=pad_label,\n","\t\tbuffer=crop_size // 2,\n","\t\tis_train=True,\n","\t)\n","\tval_ds = SubvolumeDataset(\n","\t\tval_xy,\n","\t\tpad_val_volume,\n","\t\tpad_val_label,\n","\t\tcrop_size // 2,\n","\t\tis_train=False,\n","\t)\n","\n","\t# Define data loaders for training and testing data in this fold\n","\ttrain_loader = torch.utils.data.DataLoader(\n","\t\ttrain_ds,\n","\t\tbatch_size=CFG.batch_size,\n","\t\tnum_workers=CFG.num_workers,\n","\t\tshuffle=True,\n","\t\tcollate_fn=collate_fn,\n","\t)\n","\tval_loader = torch.utils.data.DataLoader(\n","\t\tval_ds,\n","\t\tbatch_size=CFG.batch_size,\n","\t\tnum_workers=CFG.num_workers,\n","\t\tshuffle=False,\n","\t\tcollate_fn=collate_fn,\n","\t)\n","\ttrainer = pl.Trainer(\n","     \tmax_epochs=CFG.epochs,\n","\t\taccelerator=\"gpu\",\n","\t\tdevices=\"0,1,2,3\",\n","\t\tlogger=WandbLogger(name=f\"2.5d-stack-unet-{datetime.datetime.now()}\"),\n","\t\t# strategy='ddp_find_unused_parameters_true',\n","\t)\n","\ttrainer.fit(\n","\t\tnet,\n","\t\ttrain_loader,\n","\t\tval_loader,\n","\t)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(cfg_to_text())\n","\n","if 'train' in CFG.mode:\n","\tdata_dir = '/home/fummicc1/codes/competitions/kaggle-ink-detection/train'\n","\tvalid_id =[\n","\t\t'1','2b',\n","\t]\n","\ttrain_id = [\n","\t\t\"2a\", \"3\"\n","\t]\n","if 'test' in CFG.mode: \n","\tdata_dir = '/home/fummicc1/codes/competitions/kaggle-ink-detection/test'\n","\tvalid_id = glob(f'{data_dir}/*')\n","\tvalid_id = sorted(valid_id)\n","\tvalid_id = [f.split('/')[-1] for f in valid_id]\n","\n","\t# https://www.kaggle.com/competitions/vesuvius-challenge-ink-detection/discussion/410985\n","\ta_file = f'{data_dir}/a/mask.png'\n","\twith open(a_file,'rb') as f:\n","\t\thash_md5 = hashlib.md5(f.read()).hexdigest()\n","\tis_skip_test = hash_md5 == '0b0fffdc0e88be226673846a143bb3e0'\n","\tprint('is_skip_test:',is_skip_test)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4eb54d13a4b44c8fae45e9ad655222b5","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"edeb792bcd5447409e07cd064aaaa6d7","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["for t, fragment_id in enumerate(train_id):\n","    d = read_data1(fragment_id)\n","    val_d = read_data1(valid_id[0])\n","    train_one(d, val_d)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat":4,"nbformat_minor":2}
